{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class NiftiLabelsMasker in module nilearn.input_data.nifti_labels_masker:\n",
      "\n",
      "class NiftiLabelsMasker(nilearn.input_data.base_masker.BaseMasker, nilearn._utils.cache_mixin.CacheMixin)\n",
      " |  NiftiLabelsMasker(labels_img, labels=None, background_label=0, mask_img=None, smoothing_fwhm=None, standardize=False, standardize_confounds=True, high_variance_confounds=False, detrend=False, low_pass=None, high_pass=None, t_r=None, dtype=None, resampling_target='data', memory=Memory(location=None), memory_level=1, verbose=0, strategy='mean', reports=True)\n",
      " |  \n",
      " |  Class for masking of Niimg-like objects.\n",
      " |  \n",
      " |  NiftiLabelsMasker is useful when data from non-overlapping volumes should\n",
      " |  be extracted (contrarily to NiftiMapsMasker). Use case: Summarize brain\n",
      " |  signals from clusters that were obtained by prior K-means or Ward\n",
      " |  clustering.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  labels_img : Niimg-like object\n",
      " |      See http://nilearn.github.io/manipulating_images/input_output.html\n",
      " |      Region definitions, as one image of labels.\n",
      " |  \n",
      " |  labels : list of str, optional\n",
      " |      Full labels corresponding to the labels image. This is used\n",
      " |      to improve reporting quality if provided.\n",
      " |      Warning: The labels must be consistent with the label\n",
      " |      values provided through `labels_img`.\n",
      " |  \n",
      " |  background_label : number, optional\n",
      " |      Label used in labels_img to represent background.\n",
      " |      Warning: This value must be consisent with label values and\n",
      " |      image provided.\n",
      " |      Default=0.\n",
      " |  \n",
      " |  mask_img : Niimg-like object, optional\n",
      " |      See http://nilearn.github.io/manipulating_images/input_output.html\n",
      " |      Mask to apply to regions before extracting signals.\n",
      " |  \n",
      " |  smoothing_fwhm : float, optional\n",
      " |      If smoothing_fwhm is not None, it gives the full-width half maximum in\n",
      " |      millimeters of the spatial smoothing to apply to the signal.\n",
      " |  \n",
      " |  standardize : {False, True, 'zscore', 'psc'}, optional\n",
      " |      Strategy to standardize the signal.\n",
      " |      'zscore': the signal is z-scored. Timeseries are shifted\n",
      " |      to zero mean and scaled to unit variance.\n",
      " |      'psc':  Timeseries are shifted to zero mean value and scaled\n",
      " |      to percent signal change (as compared to original mean signal).\n",
      " |      True : the signal is z-scored. Timeseries are shifted\n",
      " |      to zero mean and scaled to unit variance.\n",
      " |      False : Do not standardize the data.\n",
      " |      Default=False.\n",
      " |  \n",
      " |  standardize_confounds : boolean, optional\n",
      " |      If standardize_confounds is True, the confounds are z-scored:\n",
      " |      their mean is put to 0 and their variance to 1 in the time dimension.\n",
      " |      Default=True.\n",
      " |  \n",
      " |  high_variance_confounds : boolean, optional\n",
      " |      If True, high variance confounds are computed on provided image with\n",
      " |      :func:`nilearn.image.high_variance_confounds` and default parameters\n",
      " |      and regressed out. Default=False.\n",
      " |  \n",
      " |  detrend : boolean, optional\n",
      " |      This parameter is passed to signal.clean. Please see the related\n",
      " |      documentation for details. Default=False.\n",
      " |  \n",
      " |  low_pass : None or float, optional\n",
      " |      This parameter is passed to signal.clean. Please see the related\n",
      " |      documentation for details\n",
      " |  \n",
      " |  high_pass : None or float, optional\n",
      " |      This parameter is passed to signal.clean. Please see the related\n",
      " |      documentation for details\n",
      " |  \n",
      " |  t_r : float, optional\n",
      " |      This parameter is passed to signal.clean. Please see the related\n",
      " |      documentation for details\n",
      " |  \n",
      " |  dtype : {dtype, \"auto\"}, optional\n",
      " |      Data type toward which the data should be converted. If \"auto\", the\n",
      " |      data will be converted to int32 if dtype is discrete and float32 if it\n",
      " |      is continuous.\n",
      " |  \n",
      " |  resampling_target : {\"data\", \"labels\", None}, optional\n",
      " |      Gives which image gives the final shape/size. For example, if\n",
      " |      `resampling_target` is \"data\", the atlas is resampled to the\n",
      " |      shape of the data if needed. If it is \"labels\" then mask_img\n",
      " |      and images provided to fit() are resampled to the shape and\n",
      " |      affine of maps_img. \"None\" means no resampling: if shapes and\n",
      " |      affines do not match, a ValueError is raised. Default=\"data\".\n",
      " |  \n",
      " |  memory : joblib.Memory or str, optional\n",
      " |      Used to cache the region extraction process.\n",
      " |      By default, no caching is done. If a string is given, it is the\n",
      " |      path to the caching directory.\n",
      " |  \n",
      " |  memory_level : int, optional\n",
      " |      Aggressiveness of memory caching. The higher the number, the higher\n",
      " |      the number of functions that will be cached. Zero means no caching.\n",
      " |      Default=1.\n",
      " |  \n",
      " |  verbose : integer, optional\n",
      " |      Indicate the level of verbosity. By default, nothing is printed\n",
      " |      Default=0.\n",
      " |  \n",
      " |  strategy : str, optional\n",
      " |      The name of a valid function to reduce the region with.\n",
      " |      Must be one of: sum, mean, median, mininum, maximum, variance,\n",
      " |      standard_deviation. Default='mean'.\n",
      " |  \n",
      " |  reports : boolean, optional\n",
      " |       If set to True, data is saved in order to produce a report.\n",
      " |       Default=True.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  nilearn.input_data.NiftiMasker\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      NiftiLabelsMasker\n",
      " |      nilearn.input_data.base_masker.BaseMasker\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      nilearn._utils.cache_mixin.CacheMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, labels_img, labels=None, background_label=0, mask_img=None, smoothing_fwhm=None, standardize=False, standardize_confounds=True, high_variance_confounds=False, detrend=False, low_pass=None, high_pass=None, t_r=None, dtype=None, resampling_target='data', memory=Memory(location=None), memory_level=1, verbose=0, strategy='mean', reports=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, imgs=None, y=None)\n",
      " |      Prepare signal extraction from regions.\n",
      " |      \n",
      " |      All parameters are unused, they are for scikit-learn compatibility.\n",
      " |  \n",
      " |  fit_transform(self, imgs, confounds=None, sample_mask=None)\n",
      " |      Prepare and perform signal extraction from regions.\n",
      " |  \n",
      " |  generate_report(self)\n",
      " |  \n",
      " |  inverse_transform(self, signals)\n",
      " |      Compute voxel signals from region signals\n",
      " |      \n",
      " |      Any mask given at initialization is taken into account.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      signals : (2D numpy.ndarray)\n",
      " |          Signal for each region.\n",
      " |          shape: (number of scans, number of regions)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      voxel_signals : (Nifti1Image)\n",
      " |          Signal for each voxel\n",
      " |          shape: (number of scans, number of voxels)\n",
      " |  \n",
      " |  transform_single_imgs(self, imgs, confounds=None, sample_mask=None)\n",
      " |      Extract signals from a single 4D niimg.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      imgs : 3D/4D Niimg-like object\n",
      " |          See http://nilearn.github.io/manipulating_images/input_output.html\n",
      " |          Images to process. It must boil down to a 4D image with scans\n",
      " |          number as last dimension.\n",
      " |      \n",
      " |      confounds : CSV file or array-like or pandas DataFrame, optional\n",
      " |          This parameter is passed to signal.clean. Please see the related\n",
      " |          documentation for details.\n",
      " |          shape: (number of scans, number of confounds)\n",
      " |      \n",
      " |      sample_mask : Any type compatible with numpy-array indexing, optional\n",
      " |          shape: (number of scans - number of volumes removed, )\n",
      " |          Masks the niimgs along time/fourth dimension to perform scrubbing\n",
      " |          (remove volumes with high motion) and/or non-steady-state volumes.\n",
      " |          This parameter is passed to signal.clean.\n",
      " |      \n",
      " |              .. versionadded:: 0.8.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      region_signals : 2D numpy.ndarray\n",
      " |          Signal for each label.\n",
      " |          shape: (number of scans, number of labels)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nilearn.input_data.base_masker.BaseMasker:\n",
      " |  \n",
      " |  transform(self, imgs, confounds=None, sample_mask=None)\n",
      " |      Apply mask, spatial and temporal preprocessing\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      imgs : 3D/4D Niimg-like object\n",
      " |          See http://nilearn.github.io/manipulating_images/input_output.html\n",
      " |          Images to process. It must boil down to a 4D image with scans\n",
      " |          number as last dimension.\n",
      " |      \n",
      " |      confounds : CSV file or array-like, optional\n",
      " |          This parameter is passed to signal.clean. Please see the related\n",
      " |          documentation for details.\n",
      " |          shape: (number of scans, number of confounds)\n",
      " |      \n",
      " |      sample_mask : Any type compatible with numpy-array indexing, optional\n",
      " |          shape: (number of scans - number of volumes removed, )\n",
      " |          Masks the niimgs along time/fourth dimension to perform scrubbing\n",
      " |          (remove volumes with high motion) and/or non-steady-state volumes.\n",
      " |          This parameter is passed to signal.clean.\n",
      " |      \n",
      " |              .. versionadded:: 0.8.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      region_signals : 2D numpy.ndarray\n",
      " |          Signal for each element.\n",
      " |          shape: (number of scans, number of elements)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nilearn\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "help(NiftiLabelsMasker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Decoding of a dataset after GLM fit for signal extraction\n",
    "\n",
    "Full step-by-step example of fitting a GLM to perform a decoding experiment.\n",
    "We use the data from one subject of the Haxby dataset.\n",
    "\n",
    "More specifically:\n",
    "\n",
    "1. Download the Haxby dataset.\n",
    "2. Extract the information to generate a glm representing the blocks of stimuli.\n",
    "3. Analyze the decoding performance using a classifier.\n",
    "\n",
    "To run this example, you must launch IPython via ``ipython\n",
    "--matplotlib`` in a terminal, or use the Jupyter notebook.\n",
    "    :depth: 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch example Haxby dataset\n",
    "We download the Haxby dataset\n",
    "This is a study of visual object category representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/datasets/__init__.py:89: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# By default 2nd subject will be fetched\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pandas import DataFrame as df\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nilearn import datasets\n",
    "import sklearn\n",
    "from sklearn.utils import Bunch as bunch\n",
    "from tqdm import tqdm\n",
    "from os.path import expanduser as xpu\n",
    "import loadutils as lu\n",
    "haxby_dataset = datasets.fetch_haxby(data_dir=xpu('~/../../data/cisl/DATA/nilearn_data/haxy'),\n",
    "                                     subjects=list(range(1,6)),\n",
    "                                     fetch_stimuli=True,\n",
    "                                     resume=True,\n",
    "                                     verbose=1)\n",
    "\n",
    "# repetition has to be known\n",
    "TR = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nilearn.plotting.displays.OrthoSlicer at 0x7efade846f10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAADJCAYAAAAHFcoVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAM+klEQVR4nO3dXYiUZRsH8Hv9JMXcwE9K1EgMotQITwwkEiz7wDISQkpIEsITpS9PUgkyKMSIis2k8ESDoELY6iCSDjopDC3FNWkTtfUjc3UrRI37Peh10ffdrdndmX2uZ+b3gxty2Xn2ntlrnqvrPzPPNqWUcgIAwhlS9AYAgJ5p0gAQlCYNAEFp0gAQlCYNAEFp0gAQlCYNAEFp0gAQlCYNAEFp0gAQlCYNAEFp0gAQlCYNAEFp0gAQlCbdizVr1qQ1a9YUvQ34P2qTeqOmezes6A1ENXPmzKK3AD1Sm9QbNd07kzQABFVIk25vb0933313ET+61Nrb29Off/6Zurq6UkdHR3rvvffS6NGji94WEJjzbf9EOd+apEvmgQceSGPGjEmzZ89Oc+bMSWvXri16SwB1KcL5VpMuqRMnTqTPP/88zZ49u+itANS1Is+3mnRJXX/99enee+9Nhw4dKnorAHWtyPOtJl0yH3/8cTp37lw6evRoOnnyZFq3bl3RWwKoSxHOt5p0ySxevDhde+21af78+enmm29O48aNK3pLAHUpwvlWky6pr776Kr3//vvptddeK3orAHWtyPNtYRczGT58eBo5cmT3vy9dupT++uuvorZTSps3b04///xzuu2229LevXuL3g4QlPPtwBV1vi1skv7000/T+fPnu9f69euL2kpp/frrr2nbtm3pxRdfLHorQGDOtwNX1Pm2kEl6+vTpRfzY0uvpcXv66acL2AlQFs63/RPlfOs1aQAIyh/YAKBmWltb//V7mpubK/7ef7Jo0aIB3T4ikzQABKVJA0BQ4m4ABmygUXW191Av0bdJGgCC0qQBIChxNwD9EiHi7k1PeytjBG6SBoCgNGkACErcDcC/ihxtV6qM7/42SQNAUJo0AATVlFLKRW+imqoVyVy+lmxnZ2dVjnelssQsDFwtIkK1yWAZrIi7ljXdFxHr3yQNAEFp0gAQVKnj7lpGMYMVv0SMVxiYWkeEokFqoch3b0ep6StFqW+TNAAEpUkDQFClu5hJPXyg/kr1cn3ZRlFv9TcQvT0W6rc81HN8JmkACEqTBoCgQr+727sNryZGLEa0SDBibfZGzcYTrZ5Til/TRdaxSRoAgtKkASCoEHG3+KX/xIm1EbEmLytLbVZKDQ+uiLVdlpouolZN0gAQlCYNAEGV7mImXO3K6Eps2HcRo79Go4YHh1ovJ5M0AASlSQNAUIXG3eIXiqDu4nI98OpS69VVxEszJmkACEqTBoCgBuViJmWMXMry4freiAevVsYa7E3Za3Og1HblylL3Za/pWtakSRoAgtKkASComr27uywxS71ygQjqldommlrWpEkaAILSpAEgqKrH3WLueC7/ThohGlR/jUX0/f88B+qLSRoAgtKkASAoTRoAgtKkASAoTRoAgir0T1UyuLwTlnqmvqlHJmkACMokDdSdRpuqfTa6fpmkASAoTRoAghJ3U3qiPiCKal+G2SQNAEFp0gAQlLibUhJxU6l6/itwngf1zyQNAEFp0gAQlCYNAEFp0gAQlCYNAEFp0gAQlCYNAEFp0gAQVFUuZuID9QBQfSZpAAhKkwaAoFy7m1Lx0grQSEzSABBUVSbp5ubmahwmlGHD/n5o6vG+dXZ2DsrPaWlpqfox6/H30Vf1XJu11N96bGtrS5s2barybqAy4m6AEvGST2OpSpMerMlsMF2eUurxvg2WlStXVv2YTlBqs79qUY9Qa16TBoCgxN1AXVu0aFHRW4B+M0kDQFCaNAAEJe4G6o6Im3phkgaAoDRpAAhK3A2UViPG2vV4pbl6vIre119/nVL6+3oGA6lTkzQABGWSBiiRerzSnKvo9U6TBkqlESNuGpe4GwCC0qQBIChxNxCeiJtGZZIGgKA0aQAIStxNqVyOPVtbWwveCbUm4gaTNACEpUkDQFBVibuvjKXEkHHVU3yo5upHPdUlVJtJGgCC0qQBICjv7gZqRpQNA2OSBoCgqj5J+xxrPKYZak2NQW2YpAEgKE0aAIKq2RvHfI61WI0UP/Z2X9Vd9TVSXUEEJmkACEqTBoCgBuVz0uLI6hM7AsRVrXO0SRoAgtKkASCoQi8L2lMcIAKnWnzCgHqkrhuLSRoAgtKkASAofwWrRLyju/8qeexEh71Te1CZaj9XTNIAEJQmDQBBibuBHom44/OngeufSRoAgtKkASAocXdAYkaKovbKyQVOilXL541JGgCC0qQBIKhwcXcjxzaiRoqg7iAukzQABKVJA0BQ4eLu/vKhfgaqkV5qEXHXr0aq4yIN1nPIJA0AQWnSABBU6Li7HiK5ergPjUhkSD1Qx9VVxPncJA0AQWnSABBU6Li7PyLEOyLu+tLb77Ns8aG6hP4p8rljkgaAoDRpAAiq7uJuGCx9jcCKiMdF3NA/UZ47JmkACEqTBoCgmlJKuehNDIa+Ro3Nzc0ppZQ6Ozsr+v4o0QjxDTT27kttqkv+V8RPJfT1fFsrEZ8vJmkACEqTBoCgGibuvlIlcU+l8UvEeITy6E/02FNtqkP6I0r0XWTcHf25Y5IGgKA0aQAIqiHj7kq0tLSklFJauXJlwTuBq6lNam2wY/DBirujR9s9MUkDQFCaNAAE5drdAFyl0lg4yrvDe1LGaLsnJmkACMokDUC/VGtabWlpSbfeeuug/bwyMUkDQFCaNAAEJe4GoHDff/+9z/73wCQNAEFp0gAQlCYNAEFp0gAQlCYNAEFp0gAQlCYNAEFp0gAQVOgmvWrVqvTTTz+ls2fPpm+++SbNmzev6C1RgBEjRqStW7ems2fPpo6OjrR69eqitwSlN3z48LR///505MiRorfCPwjbpOfOnZteeeWV9Mgjj6SxY8emrVu3po8++igNGRJ2y9TI+vXr04wZM9LUqVPTXXfdlZ577rm0cOHCorcFpfbss8+mU6dOFb0N/kVVOt4zzzyTPvzww6u+9vrrr6fNmzf3+5jTpk1L+/btS7t3704ppbRt27Y0fvz4NGHChIFstWJtbW2pra1tUH5WPbvxxhvT6dOn05w5c1JKKU2ePDmdPHkyzZ8/v+JjPPHEE+mll15KnZ2d6cCBA2nLli1p+fLlNdpxfGqzsT366KOpq6ure50/fz59+eWXfTrGtGnT0rJly9LGjRtrtMu+UdP/LA90TZo0Kf/+++957NixOaWUhw4dmk+cOJFvv/32/Oabb+YzZ870uPbs2dPrMceMGZO//fbbPHfu3DxkyJC8atWqvHv37gHv1Rr8tWLFirxv3758zTXX5M8++yy/+uqrOaVUUW00NzfnnHOeMGFC9/GWLFmS9+7dW/j9sqyi15gxY/L+/fvzU089lZ9//vlen09nzpy56nY7d+7MixcvzvPnz89Hjhwp/H5Y/7iqc6DW1ta8YsWKnFLK9913X963b9+Aj7l27dp84cKFfPHixXzq1Kl8xx13FP1gWf1cn3zySd67d2/es2dPHjFiRMW3u+GGG3LOOY8cObL7awsWLMjt7e2F3yfLKnI1NTXlnTt35rfeeqtPt1u8eHFubW3NKSVNuhyrOgdaunRp3rVrV04p5e3bt+cXXnih4tveeeeduaurK3d1deUffvghp5Tyk08+mQ8ePJhnzJiRm5qa8sKFC/Px48fz5MmTi37ArH6s+++/P+ecu/9HrtJ1eZIeP35899cefvhhk7TV8Ovll1/Ou3btysOGDav4NqNGjcoHDx7MN910U05Jky7Jqs6BRo4cmX/77bd8yy235K6urjxlypScUspvv/12dwP+33W5Ife03njjjbxp06arvvbdd9/lJUuWFP2AWX1co0ePzocOHcpbtmzJR48ezdddd12fauPYsWN5wYIF3f/esGFD3r59e+H3y7KKWkuXLs3t7e153Lhx3V9bu3Ztr8+nrq6unFLKs2bNyhcuXMgdHR25o6Mjnz59Ol+6dCl3dHTkqVOnFn6/rB5X9Q72zjvv5D179uQvvvhiwMd6/PHHc1tbW54+fXpO6e+I848//sgzZ84s+gGz+rjefffdvGPHjpxSyi0tLfmDDz7o0+03btyYd+3alZubm/PMmTPzL7/8khcuXFj4/bKsItbs2bPzyZMn86xZs/p826FDh+aJEyd2r4ceeigfO3YsT5w4MQ8ZMqTw+2b1uKp3sHnz5uWcc16+fHlVjrdhw4Z8+PDhfO7cubx///68bNmyoh8sq4/rwQcfvGp6Hj16dP7xxx/zY489VvExRowYkbdu3ZrPnj2bjx8/nlevXl34/bKsota6devyxYsXr5qSL7/G3Ncl7o6/mv77H1UxZcqUdODAgTRp0qTU1dVVrcMCQEOq2pVBmpqa0po1a9KOHTs0aACogmHVOMioUaPSiRMn0uHDh9M999xTjUMCQMOratwNAFSPC2EDQFCaNAAEpUkDQFCaNAAEpUkDQFCaNAAE9R8T97NBnE+fuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 475.2x187.2 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nilearn.plotting import plot_epi, plot_img, plot_anat\n",
    "plot_anat(haxby_dataset.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# haxby_description=Path(haxby_dataset.description).read_text().decode()\n",
    "# haxby_description\n",
    "# from io import StringIO\n",
    "# pd.read_csv(StringIO())\n",
    "# haxby_dataset.description.decode().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the behavioral data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load target information as string and give a numerical identifier to each\n",
    "behavioral = pd.read_csv(haxby_dataset.session_target[0], sep=' ')\n",
    "conditions = behavioral['labels'].values\n",
    "\n",
    "# Record these as an array of sessions\n",
    "sessions = behavioral['chunks'].values\n",
    "unique_sessions = behavioral['chunks'].unique()\n",
    "\n",
    "# fMRI data: a unique file for each session\n",
    "func_filename = haxby_dataset.func[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rest</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rest</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rest</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rest</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rest</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>rest</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>rest</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>rest</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>rest</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>rest</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1452 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     labels  chunks\n",
       "0      rest       0\n",
       "1      rest       0\n",
       "2      rest       0\n",
       "3      rest       0\n",
       "4      rest       0\n",
       "...     ...     ...\n",
       "1447   rest      11\n",
       "1448   rest      11\n",
       "1449   rest      11\n",
       "1450   rest      11\n",
       "1451   rest      11\n",
       "\n",
       "[1452 rows x 2 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conditions,conditions.shape,sessions,sessions.shape\n",
    "behavioral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "# nib.load(func_filename).shape,\n",
    "unique_sessions\n",
    "# behavioral.shape\n",
    "# nib.load(subject_data.mar_scans.func[1][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 12)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# behavioral.labels.unique()\n",
    "unique_sessions\n",
    "behavioral = pd.read_csv(haxby_dataset.session_target[0], sep=' ')\n",
    "behavioral.labels.unique().__len__(),behavioral.chunks.unique().__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a proper event structure for each session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0:      onset trial_type  duration\n",
       " 6     15.0   scissors       2.5\n",
       " 7     17.5   scissors       2.5\n",
       " 8     20.0   scissors       2.5\n",
       " 9     22.5   scissors       2.5\n",
       " 10    25.0   scissors       2.5\n",
       " ..     ...        ...       ...\n",
       " 110  275.0      chair       2.5\n",
       " 111  277.5      chair       2.5\n",
       " 112  280.0      chair       2.5\n",
       " 113  282.5      chair       2.5\n",
       " 114  285.0      chair       2.5\n",
       " \n",
       " [72 rows x 3 columns],\n",
       " 1:      onset    trial_type  duration\n",
       " 6     15.0          face       2.5\n",
       " 7     17.5          face       2.5\n",
       " 8     20.0          face       2.5\n",
       " 9     22.5          face       2.5\n",
       " 10    25.0          face       2.5\n",
       " ..     ...           ...       ...\n",
       " 110  275.0  scrambledpix       2.5\n",
       " 111  277.5  scrambledpix       2.5\n",
       " 112  280.0  scrambledpix       2.5\n",
       " 113  282.5  scrambledpix       2.5\n",
       " 114  285.0  scrambledpix       2.5\n",
       " \n",
       " [72 rows x 3 columns],\n",
       " 2:      onset trial_type  duration\n",
       " 6     15.0        cat       2.5\n",
       " 7     17.5        cat       2.5\n",
       " 8     20.0        cat       2.5\n",
       " 9     22.5        cat       2.5\n",
       " 10    25.0        cat       2.5\n",
       " ..     ...        ...       ...\n",
       " 110  275.0      house       2.5\n",
       " 111  277.5      house       2.5\n",
       " 112  280.0      house       2.5\n",
       " 113  282.5      house       2.5\n",
       " 114  285.0      house       2.5\n",
       " \n",
       " [72 rows x 3 columns],\n",
       " 3:      onset trial_type  duration\n",
       " 6     15.0       shoe       2.5\n",
       " 7     17.5       shoe       2.5\n",
       " 8     20.0       shoe       2.5\n",
       " 9     22.5       shoe       2.5\n",
       " 10    25.0       shoe       2.5\n",
       " ..     ...        ...       ...\n",
       " 110  275.0   scissors       2.5\n",
       " 111  277.5   scissors       2.5\n",
       " 112  280.0   scissors       2.5\n",
       " 113  282.5   scissors       2.5\n",
       " 114  285.0   scissors       2.5\n",
       " \n",
       " [72 rows x 3 columns],\n",
       " 4:      onset    trial_type  duration\n",
       " 6     15.0         house       2.5\n",
       " 7     17.5         house       2.5\n",
       " 8     20.0         house       2.5\n",
       " 9     22.5         house       2.5\n",
       " 10    25.0         house       2.5\n",
       " ..     ...           ...       ...\n",
       " 110  275.0  scrambledpix       2.5\n",
       " 111  277.5  scrambledpix       2.5\n",
       " 112  280.0  scrambledpix       2.5\n",
       " 113  282.5  scrambledpix       2.5\n",
       " 114  285.0  scrambledpix       2.5\n",
       " \n",
       " [72 rows x 3 columns],\n",
       " 5:      onset trial_type  duration\n",
       " 6     15.0      house       2.5\n",
       " 7     17.5      house       2.5\n",
       " 8     20.0      house       2.5\n",
       " 9     22.5      house       2.5\n",
       " 10    25.0      house       2.5\n",
       " ..     ...        ...       ...\n",
       " 110  275.0   scissors       2.5\n",
       " 111  277.5   scissors       2.5\n",
       " 112  280.0   scissors       2.5\n",
       " 113  282.5   scissors       2.5\n",
       " 114  285.0   scissors       2.5\n",
       " \n",
       " [72 rows x 3 columns],\n",
       " 6:      onset trial_type  duration\n",
       " 6     15.0       face       2.5\n",
       " 7     17.5       face       2.5\n",
       " 8     20.0       face       2.5\n",
       " 9     22.5       face       2.5\n",
       " 10    25.0       face       2.5\n",
       " ..     ...        ...       ...\n",
       " 110  275.0     bottle       2.5\n",
       " 111  277.5     bottle       2.5\n",
       " 112  280.0     bottle       2.5\n",
       " 113  282.5     bottle       2.5\n",
       " 114  285.0     bottle       2.5\n",
       " \n",
       " [72 rows x 3 columns],\n",
       " 7:      onset trial_type  duration\n",
       " 6     15.0       face       2.5\n",
       " 7     17.5       face       2.5\n",
       " 8     20.0       face       2.5\n",
       " 9     22.5       face       2.5\n",
       " 10    25.0       face       2.5\n",
       " ..     ...        ...       ...\n",
       " 110  275.0      house       2.5\n",
       " 111  277.5      house       2.5\n",
       " 112  280.0      house       2.5\n",
       " 113  282.5      house       2.5\n",
       " 114  285.0      house       2.5\n",
       " \n",
       " [72 rows x 3 columns],\n",
       " 8:      onset trial_type  duration\n",
       " 6     15.0       face       2.5\n",
       " 7     17.5       face       2.5\n",
       " 8     20.0       face       2.5\n",
       " 9     22.5       face       2.5\n",
       " 10    25.0       face       2.5\n",
       " ..     ...        ...       ...\n",
       " 110  275.0   scissors       2.5\n",
       " 111  277.5   scissors       2.5\n",
       " 112  280.0   scissors       2.5\n",
       " 113  282.5   scissors       2.5\n",
       " 114  285.0   scissors       2.5\n",
       " \n",
       " [72 rows x 3 columns],\n",
       " 9:      onset trial_type  duration\n",
       " 6     15.0       face       2.5\n",
       " 7     17.5       face       2.5\n",
       " 8     20.0       face       2.5\n",
       " 9     22.5       face       2.5\n",
       " 10    25.0       face       2.5\n",
       " ..     ...        ...       ...\n",
       " 110  275.0     bottle       2.5\n",
       " 111  277.5     bottle       2.5\n",
       " 112  280.0     bottle       2.5\n",
       " 113  282.5     bottle       2.5\n",
       " 114  285.0     bottle       2.5\n",
       " \n",
       " [72 rows x 3 columns],\n",
       " 10:      onset trial_type  duration\n",
       " 6     15.0        cat       2.5\n",
       " 7     17.5        cat       2.5\n",
       " 8     20.0        cat       2.5\n",
       " 9     22.5        cat       2.5\n",
       " 10    25.0        cat       2.5\n",
       " ..     ...        ...       ...\n",
       " 110  275.0   scissors       2.5\n",
       " 111  277.5   scissors       2.5\n",
       " 112  280.0   scissors       2.5\n",
       " 113  282.5   scissors       2.5\n",
       " 114  285.0   scissors       2.5\n",
       " \n",
       " [72 rows x 3 columns],\n",
       " 11:      onset trial_type  duration\n",
       " 6     15.0     bottle       2.5\n",
       " 7     17.5     bottle       2.5\n",
       " 8     20.0     bottle       2.5\n",
       " 9     22.5     bottle       2.5\n",
       " 10    25.0     bottle       2.5\n",
       " ..     ...        ...       ...\n",
       " 110  275.0   scissors       2.5\n",
       " 111  277.5   scissors       2.5\n",
       " 112  280.0   scissors       2.5\n",
       " 113  282.5   scissors       2.5\n",
       " 114  285.0   scissors       2.5\n",
       " \n",
       " [72 rows x 3 columns]}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_events(behavioral,\n",
    "               TR:float=2.5) -> dict:\n",
    "    conditions = behavioral['labels'].values\n",
    "\n",
    "    # Record these as an array of sessions\n",
    "    sessions = behavioral['chunks'].values\n",
    "    unique_sessions = behavioral['chunks'].unique()\n",
    "\n",
    "    # fMRI data: a unique file for each session\n",
    "    func_filename = haxby_dataset.func[0]\n",
    "    events = {}\n",
    "    # events will take  the form of a dictionary of Dataframes, one per session\n",
    "    for session in unique_sessions:\n",
    "        # get the condition label per session\n",
    "        conditions_session = conditions[sessions == session]\n",
    "        # get the number of scans per session, then the corresponding\n",
    "        # vector of frame times\n",
    "        n_scans = len(conditions_session)\n",
    "        frame_times = TR * np.arange(n_scans)\n",
    "        # each event last the full TR\n",
    "        duration = TR * np.ones(n_scans)\n",
    "        # Define the events object\n",
    "        events_ = pd.DataFrame(\n",
    "            {'onset': frame_times,\n",
    "             'trial_type': conditions_session,\n",
    "             'duration': duration})\n",
    "        # remove the rest condition and insert into the dictionary\n",
    "        events[session] = events_[events_.trial_type != 'rest']\n",
    "    return events,sessions,unique_sessions\n",
    "events,sessions,unique_sessions=get_events(behavioral)\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and run FirstLevelModel\n",
    "\n",
    "We generate a list of z-maps together with their session and condition index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the glm\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "glm = FirstLevelModel(t_r=TR,\n",
    "                      mask_img=haxby_dataset.mask,\n",
    "                      high_pass=.008,\n",
    "                      smoothing_fwhm=4,\n",
    "                      memory='nilearn_cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the glm on data from each session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/12 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from nilearn.image import index_img\n",
    "def sessions_glm(events,sessions,unique_sessions):\n",
    "    z_maps = []\n",
    "    conditions_label = []\n",
    "    session_label = []\n",
    "    for session in tqdm(unique_sessions):\n",
    "        # grab the fmri data for that particular session\n",
    "        fmri_session = index_img(func_filename, sessions == session)\n",
    "\n",
    "        # fit the glm\n",
    "        glm.fit(fmri_session, events=events[session])\n",
    "\n",
    "        # set up contrasts: one per condition\n",
    "        conditions = events[session].trial_type.unique()\n",
    "        for condition_ in conditions:\n",
    "            z_maps.append(glm.compute_contrast(condition_))\n",
    "            conditions_label.append(condition_)\n",
    "            session_label.append(session)\n",
    "    return z_maps,conditions_label,session_label\n",
    "z_maps,conditions_label,session_label=sessions_glm(events,sessions,unique_sessions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a report\n",
    "Since we have already computed the FirstLevelModel\n",
    "and have the contrast, we can quickly create a summary report.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import mean_img\n",
    "from nilearn.reporting import make_glm_report\n",
    "mean_img_ = mean_img(func_filename)\n",
    "report = make_glm_report(glm,\n",
    "                         contrasts=conditions,\n",
    "                         bg_img=mean_img_,\n",
    "                         )\n",
    "\n",
    "report  # This report can be viewed in a notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a jupyter notebook, the report will be automatically inserted, as above.\n",
    "We have several other ways to access the report:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report.save_as_html('report.html')\n",
    "# report.open_in_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the decoding pipeline\n",
    "To define the decoding pipeline we use Decoder object, we choose :\n",
    "\n",
    "    * a prediction model, here a Support Vector Classifier, with a linear\n",
    "      kernel\n",
    "\n",
    "    * the mask to use, here a ventral temporal ROI in the visual cortex\n",
    "\n",
    "    * although it usually helps to decode better, z-maps time series don't\n",
    "      need to be rescaled to a 0 mean, variance of 1 so we use\n",
    "      standardize=False.\n",
    "\n",
    "    * we use univariate feature selection to reduce the dimension of the\n",
    "      problem keeping only 5% of voxels which are most informative.\n",
    "\n",
    "    * a cross-validation scheme, here we use LeaveOneGroupOut\n",
    "      cross-validation on the sessions which corresponds to a\n",
    "      leave-one-session-out\n",
    "\n",
    "We fit directly this pipeline on the Niimgs outputs of the GLM, with\n",
    "corresponding conditions labels and session labels (for the cross validation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decoding import Decoder\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "decoder = Decoder(estimator='svc', mask=haxby_dataset.mask, standardize=False,\n",
    "                  screening_percentile=5, cv=LeaveOneGroupOut())\n",
    "decoder.fit(z_maps, conditions_label, groups=session_label)\n",
    "\n",
    "# Return the corresponding mean prediction accuracy compared to chance\n",
    "\n",
    "classification_accuracy = np.mean(list(decoder.cv_scores_.values()))\n",
    "chance_level = 1. / len(np.unique(conditions))\n",
    "print('Classification accuracy: {:.4f} / Chance level: {}'.format(\n",
    "    classification_accuracy, chance_level))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
