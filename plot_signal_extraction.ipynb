{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Extracting signals from a brain parcellation\n",
    "\n",
    "Here we show how to extract signals from a brain parcellation and compute\n",
    "a correlation matrix.\n",
    "\n",
    "We also show the importance of defining good confounds signals: the\n",
    "first correlation matrix is computed after regressing out simple\n",
    "confounds signals: movement regressors, white matter and CSF signals, ...\n",
    "The second one is without any confounds: all regions are connected to\n",
    "each other.\n",
    "\n",
    "\n",
    "One reference that discusses the importance of confounds is `Varoquaux and\n",
    "Craddock, Learning and comparing functional connectomes across subjects,\n",
    "NeuroImage 2013\n",
    "<http://www.sciencedirect.com/science/article/pii/S1053811913003340>`_.\n",
    "\n",
    "This is just a code example, see the `corresponding section in the\n",
    "documentation <parcellation_time_series>` for more.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>This example needs SciPy >= 1.0.0 for the reordering of the matrix.</p></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/datasets/__init__.py:89: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import gzip\n",
    "import io\n",
    "import json\n",
    "import nilearn\n",
    "import os\n",
    "import sklearn\n",
    "import tarfile\n",
    "import tempfile\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from bids_validator import BIDSValidator\n",
    "\n",
    "from fetch_difumo import fetch_difumo\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from os import listdir as ls\n",
    "from os.path import basename as bname\n",
    "from os.path import dirname as dname\n",
    "from os.path import expanduser as xpu\n",
    "from os.path import join as pjoin\n",
    "from pandas import DataFrame as df\n",
    "from tempfile import TemporaryDirectory as tmpdir\n",
    "from tempfile import TemporaryFile as tmpfile\n",
    "from tqdm import tqdm\n",
    "from typing import Union\n",
    "from collections.abc import Iterable\n",
    "from typing import Sequence\n",
    "\n",
    "import loadutils as lu\n",
    "import sniffbytes as snif\n",
    "import scanzip as szip\n",
    "import shutil\n",
    "\n",
    "from nilearn import masking\n",
    "from nilearn.plotting import plot_stat_map, plot_anat, plot_img, plot_epi\n",
    "from nilearn.image import concat_imgs, mean_img\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from cimaqprep import participant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  5.27it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  5.75it/s]\n",
      "100%|█████████████████████████████████████████| 288/288 [00:19<00:00, 14.87it/s]\n",
      "100%|█████████████████████████████████████████| 288/288 [00:19<00:00, 15.11it/s]\n",
      "100%|█████████████████████████████████████████| 120/120 [00:19<00:00,  6.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# % install_ext https://raw.github.com/cpcloud/ipython-autotime/master/autotime.py\n",
    "# % load_ext autotime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from cimaqprep.participant_data import participant_data\n",
    "subject00 = participant_data(cimaq_nov_dir = xpu('~/../../data/cisl/DATA/cimaq_20190901'),\n",
    "                             cimaq_mar_dir = xpu('~/../../data/cisl/DATA/cimaq_03-19'),\n",
    "                             events_path = xpu('~/../../data/cisl/DATA/cimaq_corrected_events/events'),\n",
    "                             behav_path = xpu('~/../../data/cisl/DATA/cimaq_corrected_behavioural/behavioural'),\n",
    "                             participants_path = xpu('~/../../data/cisl/DATA/cimaq_03-19/derivatives/CIMAQ_fmri_memory/data/participants/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the atlas and the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "\n",
    "dataset = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm',\n",
    "                                              data_dir=pjoin(dname(os.getcwd()),'harvard_oxford'))\n",
    "difumo_dir=pjoin(dname(os.getcwd()),'DiFuMo_atlases')\n",
    "difumo512_3mm=datasets.atlas.fetch_atlas_difumo(dimension=512,\n",
    "                                                resolution_mm=3,\n",
    "                                                data_dir=difumo_dir,\n",
    "                                                resume=True,\n",
    "                                                verbose=1)\n",
    "difumo512_3mm.labels\n",
    "# atlas_filename, labels = difumo512_3mm.maps,difumo512_3mm.maps[0],difumo512_3mm.maps,difumo512_3mm.labels\n",
    "atlas_filename, labels = difumo512_3mm.maps, difumo512_3mm.labels\n",
    "# # One subject fmri data\n",
    "# # data=subject_data\n",
    "# fmri_filenames=subject00.resampled_fmri_to_events\n",
    "# data = datasets.fetch_development_fmri(n_subjects=1,\n",
    "#                                        data_dir=pjoin(dname(os.getcwd()),'development_fmri'))\n",
    "# fmri_filenames = data.func[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract signals on a parcellation defined by labels\n",
    "Using the NiftiLabelsMasker\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we go from nifti files to the signal time series in a numpy array.\n",
    "#### Note how we give confounds to be regressed out during signal extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80, 41)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nilearn.image import concat_imgs, mean_img\n",
    "mean_img(concat_imgs(subject00.resampled_fmri_to_events.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'difumo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3747/3013700261.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdifumo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'difumo' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "masker = NiftiLabelsMasker(labels_img=mean_img(difumo512_3mm.maps),\n",
    "                           labels=difumo512_3mm.labels,\n",
    "                           standardize=True,\n",
    "                           memory='nilearn_cache', verbose=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class NiftiLabelsMasker in module nilearn.input_data.nifti_labels_masker:\n",
      "\n",
      "class NiftiLabelsMasker(nilearn.input_data.base_masker.BaseMasker, nilearn._utils.cache_mixin.CacheMixin)\n",
      " |  NiftiLabelsMasker(labels_img, labels=None, background_label=0, mask_img=None, smoothing_fwhm=None, standardize=False, standardize_confounds=True, high_variance_confounds=False, detrend=False, low_pass=None, high_pass=None, t_r=None, dtype=None, resampling_target='data', memory=Memory(location=None), memory_level=1, verbose=0, strategy='mean', reports=True)\n",
      " |  \n",
      " |  Class for masking of Niimg-like objects.\n",
      " |  \n",
      " |  NiftiLabelsMasker is useful when data from non-overlapping volumes should\n",
      " |  be extracted (contrarily to NiftiMapsMasker). Use case: Summarize brain\n",
      " |  signals from clusters that were obtained by prior K-means or Ward\n",
      " |  clustering.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  labels_img : Niimg-like object\n",
      " |      See http://nilearn.github.io/manipulating_images/input_output.html\n",
      " |      Region definitions, as one image of labels.\n",
      " |  \n",
      " |  labels : list of str, optional\n",
      " |      Full labels corresponding to the labels image. This is used\n",
      " |      to improve reporting quality if provided.\n",
      " |      Warning: The labels must be consistent with the label\n",
      " |      values provided through `labels_img`.\n",
      " |  \n",
      " |  background_label : number, optional\n",
      " |      Label used in labels_img to represent background.\n",
      " |      Warning: This value must be consisent with label values and\n",
      " |      image provided.\n",
      " |      Default=0.\n",
      " |  \n",
      " |  mask_img : Niimg-like object, optional\n",
      " |      See http://nilearn.github.io/manipulating_images/input_output.html\n",
      " |      Mask to apply to regions before extracting signals.\n",
      " |  \n",
      " |  smoothing_fwhm : float, optional\n",
      " |      If smoothing_fwhm is not None, it gives the full-width half maximum in\n",
      " |      millimeters of the spatial smoothing to apply to the signal.\n",
      " |  \n",
      " |  standardize : {False, True, 'zscore', 'psc'}, optional\n",
      " |      Strategy to standardize the signal.\n",
      " |      'zscore': the signal is z-scored. Timeseries are shifted\n",
      " |      to zero mean and scaled to unit variance.\n",
      " |      'psc':  Timeseries are shifted to zero mean value and scaled\n",
      " |      to percent signal change (as compared to original mean signal).\n",
      " |      True : the signal is z-scored. Timeseries are shifted\n",
      " |      to zero mean and scaled to unit variance.\n",
      " |      False : Do not standardize the data.\n",
      " |      Default=False.\n",
      " |  \n",
      " |  standardize_confounds : boolean, optional\n",
      " |      If standardize_confounds is True, the confounds are z-scored:\n",
      " |      their mean is put to 0 and their variance to 1 in the time dimension.\n",
      " |      Default=True.\n",
      " |  \n",
      " |  high_variance_confounds : boolean, optional\n",
      " |      If True, high variance confounds are computed on provided image with\n",
      " |      :func:`nilearn.image.high_variance_confounds` and default parameters\n",
      " |      and regressed out. Default=False.\n",
      " |  \n",
      " |  detrend : boolean, optional\n",
      " |      This parameter is passed to signal.clean. Please see the related\n",
      " |      documentation for details. Default=False.\n",
      " |  \n",
      " |  low_pass : None or float, optional\n",
      " |      This parameter is passed to signal.clean. Please see the related\n",
      " |      documentation for details\n",
      " |  \n",
      " |  high_pass : None or float, optional\n",
      " |      This parameter is passed to signal.clean. Please see the related\n",
      " |      documentation for details\n",
      " |  \n",
      " |  t_r : float, optional\n",
      " |      This parameter is passed to signal.clean. Please see the related\n",
      " |      documentation for details\n",
      " |  \n",
      " |  dtype : {dtype, \"auto\"}, optional\n",
      " |      Data type toward which the data should be converted. If \"auto\", the\n",
      " |      data will be converted to int32 if dtype is discrete and float32 if it\n",
      " |      is continuous.\n",
      " |  \n",
      " |  resampling_target : {\"data\", \"labels\", None}, optional\n",
      " |      Gives which image gives the final shape/size. For example, if\n",
      " |      `resampling_target` is \"data\", the atlas is resampled to the\n",
      " |      shape of the data if needed. If it is \"labels\" then mask_img\n",
      " |      and images provided to fit() are resampled to the shape and\n",
      " |      affine of maps_img. \"None\" means no resampling: if shapes and\n",
      " |      affines do not match, a ValueError is raised. Default=\"data\".\n",
      " |  \n",
      " |  memory : joblib.Memory or str, optional\n",
      " |      Used to cache the region extraction process.\n",
      " |      By default, no caching is done. If a string is given, it is the\n",
      " |      path to the caching directory.\n",
      " |  \n",
      " |  memory_level : int, optional\n",
      " |      Aggressiveness of memory caching. The higher the number, the higher\n",
      " |      the number of functions that will be cached. Zero means no caching.\n",
      " |      Default=1.\n",
      " |  \n",
      " |  verbose : integer, optional\n",
      " |      Indicate the level of verbosity. By default, nothing is printed\n",
      " |      Default=0.\n",
      " |  \n",
      " |  strategy : str, optional\n",
      " |      The name of a valid function to reduce the region with.\n",
      " |      Must be one of: sum, mean, median, mininum, maximum, variance,\n",
      " |      standard_deviation. Default='mean'.\n",
      " |  \n",
      " |  reports : boolean, optional\n",
      " |       If set to True, data is saved in order to produce a report.\n",
      " |       Default=True.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  nilearn.input_data.NiftiMasker\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      NiftiLabelsMasker\n",
      " |      nilearn.input_data.base_masker.BaseMasker\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      nilearn._utils.cache_mixin.CacheMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, labels_img, labels=None, background_label=0, mask_img=None, smoothing_fwhm=None, standardize=False, standardize_confounds=True, high_variance_confounds=False, detrend=False, low_pass=None, high_pass=None, t_r=None, dtype=None, resampling_target='data', memory=Memory(location=None), memory_level=1, verbose=0, strategy='mean', reports=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, imgs=None, y=None)\n",
      " |      Prepare signal extraction from regions.\n",
      " |      \n",
      " |      All parameters are unused, they are for scikit-learn compatibility.\n",
      " |  \n",
      " |  fit_transform(self, imgs, confounds=None, sample_mask=None)\n",
      " |      Prepare and perform signal extraction from regions.\n",
      " |  \n",
      " |  generate_report(self)\n",
      " |  \n",
      " |  inverse_transform(self, signals)\n",
      " |      Compute voxel signals from region signals\n",
      " |      \n",
      " |      Any mask given at initialization is taken into account.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      signals : (2D numpy.ndarray)\n",
      " |          Signal for each region.\n",
      " |          shape: (number of scans, number of regions)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      voxel_signals : (Nifti1Image)\n",
      " |          Signal for each voxel\n",
      " |          shape: (number of scans, number of voxels)\n",
      " |  \n",
      " |  transform_single_imgs(self, imgs, confounds=None, sample_mask=None)\n",
      " |      Extract signals from a single 4D niimg.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      imgs : 3D/4D Niimg-like object\n",
      " |          See http://nilearn.github.io/manipulating_images/input_output.html\n",
      " |          Images to process. It must boil down to a 4D image with scans\n",
      " |          number as last dimension.\n",
      " |      \n",
      " |      confounds : CSV file or array-like or pandas DataFrame, optional\n",
      " |          This parameter is passed to signal.clean. Please see the related\n",
      " |          documentation for details.\n",
      " |          shape: (number of scans, number of confounds)\n",
      " |      \n",
      " |      sample_mask : Any type compatible with numpy-array indexing, optional\n",
      " |          shape: (number of scans - number of volumes removed, )\n",
      " |          Masks the niimgs along time/fourth dimension to perform scrubbing\n",
      " |          (remove volumes with high motion) and/or non-steady-state volumes.\n",
      " |          This parameter is passed to signal.clean.\n",
      " |      \n",
      " |              .. versionadded:: 0.8.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      region_signals : 2D numpy.ndarray\n",
      " |          Signal for each label.\n",
      " |          shape: (number of scans, number of labels)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nilearn.input_data.base_masker.BaseMasker:\n",
      " |  \n",
      " |  transform(self, imgs, confounds=None, sample_mask=None)\n",
      " |      Apply mask, spatial and temporal preprocessing\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      imgs : 3D/4D Niimg-like object\n",
      " |          See http://nilearn.github.io/manipulating_images/input_output.html\n",
      " |          Images to process. It must boil down to a 4D image with scans\n",
      " |          number as last dimension.\n",
      " |      \n",
      " |      confounds : CSV file or array-like, optional\n",
      " |          This parameter is passed to signal.clean. Please see the related\n",
      " |          documentation for details.\n",
      " |          shape: (number of scans, number of confounds)\n",
      " |      \n",
      " |      sample_mask : Any type compatible with numpy-array indexing, optional\n",
      " |          shape: (number of scans - number of volumes removed, )\n",
      " |          Masks the niimgs along time/fourth dimension to perform scrubbing\n",
      " |          (remove volumes with high motion) and/or non-steady-state volumes.\n",
      " |          This parameter is passed to signal.clean.\n",
      " |      \n",
      " |              .. versionadded:: 0.8.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      region_signals : 2D numpy.ndarray\n",
      " |          Signal for each element.\n",
      " |          shape: (number of scans, number of elements)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# help(masker.fit_transform)\n",
    "help(NiftiLabelsMasker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NiftiLabelsMasker.fit_transform] loading data from Nifti1Image(\n",
      "shape=(104, 123, 104),\n",
      "affine=array([[   3.,    0.,    0.,  -96.],\n",
      "       [   0.,    3.,    0., -132.],\n",
      "       [   0.,    0.,    3.,  -78.],\n",
      "       [   0.,    0.,    0.,    1.]])\n",
      ")\n",
      "Resampling labels\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling nilearn.input_data.base_masker.filter_and_extract...\n",
      "filter_and_extract(<nibabel.nifti1.Nifti1Image object at 0x7f9213a55dd0>, <nilearn.input_data.nifti_labels_masker._ExtractionFunctor object at 0x7f922ac5a7d0>, \n",
      "{ 'background_label': 0,\n",
      "  'detrend': False,\n",
      "  'dtype': None,\n",
      "  'high_pass': None,\n",
      "  'high_variance_confounds': False,\n",
      "  'labels': rec.array([(  1, 'Superior occipital sulcus inferior LH', 'DorsAttnB', 'DorsAttnA', 0.57588 , 0.346929, 0.077031),\n",
      "           ...,\n",
      "           (512, 'Precentral sulcus superior RH', 'DorsAttnB', 'DorsAttnB', 0.622743, 0.314315, 0.062787)],\n",
      "          dtype=[('component', '<i8'), ('difumo_names', '<U90'), ('yeo_networks7', '<U16'), ('yeo_networks17', '<U16'), ('gm', '<f8'), ('wm', '<f8'), ('csf', '<f8')]),\n",
      "  'labels_img': <nibabel.nifti1.Nifti1Image object at 0x7f9213a55a10>,\n",
      "  'low_pass': None,\n",
      "  'mask_img': None,\n",
      "  'reports': True,\n",
      "  'smoothing_fwhm': None,\n",
      "  'standardize': True,\n",
      "  'standardize_confounds': True,\n",
      "  'strategy': 'mean',\n",
      "  't_r': None,\n",
      "  'target_affine': None,\n",
      "  'target_shape': None}, confounds=None, sample_mask=None, dtype=None, memory=Memory(location=nilearn_cache/joblib), memory_level=1, verbose=5)\n",
      "[NiftiLabelsMasker.transform_single_imgs] Loading data from Nifti1Image(\n",
      "shape=(80, 80, 41),\n",
      "affine=array([[-2.99872494e+00,  3.58978175e-02, -8.77196789e-02,\n",
      "         1.23723183e+02],\n",
      "       [ 8.45294818e-02,  1.89115095e+00, -2.56004500e+00,\n",
      "        -3.42206497e+01],\n",
      "       [-2.24215873e-02,  2.32857442e+00,  2.08049202e+00,\n",
      "        -1.46895874e+02],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         1.00000000e+00]])\n",
      ")\n",
      "[NiftiLabelsMasker.transform_single_imgs] Extracting region signals\n",
      "[NiftiLabelsMasker.transform_single_imgs] Cleaning extracted signals\n",
      "_______________________________________________filter_and_extract - 2.0s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "\n",
    "time_series = masker.fit_transform(imgs=mean_img(concat_imgs(fmri_filenames.values)))\n",
    "#     imgs=mean_img(concat_imgs(subject00.resampled_fmri_to_events.values)))\n",
    "#                                    confounds=data.confounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 43670)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     (-0.006619999999999999, -0.08829999999999999, ...\n",
       "1     (0.09545000000000001, 0.0405, 0.02950000000000...\n",
       "2     (0.1017, 0.0032613, -0.14100000000000001, 0.08...\n",
       "3     (-0.3145, -0.14950000000000002, -0.02515, -0.3...\n",
       "4     (-0.02975, -0.041999999999999996, -0.033850000...\n",
       "5     (-0.1625, -0.125, -0.07815, -0.209, -0.2415, -...\n",
       "6     (0.425, 0.36, 0.949, 0.20700000000000002, 0.27...\n",
       "7     (0.0036650000000000003, 0.996, 0.996, 0.996, 0...\n",
       "8     (1.395, 1.395, 1.395, 1.395, 1.395, 1.395, 1.3...\n",
       "9     (1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, ...\n",
       "10    (1.395, 1.395, 1.395, 1.395, 1.395, 1.395, 1.3...\n",
       "11    (1.4, 1.4, 1.4, 1.4, 1.4, 1.3765, 1.353, 1.353...\n",
       "12    (1.395, 1.395, 1.395, 1.395, 1.371500000000000...\n",
       "13    (1.4, 1.4, 1.4, 1.3765, 1.353, 1.3295, 1.306, ...\n",
       "14    (1.395, 1.395, 1.395, 1.3715000000000002, 1.34...\n",
       "15    (1.4, 1.4, 1.4, 1.353, 1.3295, 1.2825, 1.212, ...\n",
       "16    (1.395, 1.395, 1.3715000000000002, 1.348, 1.30...\n",
       "17    (1.4, 1.4, 1.3765, 1.3295, 1.2825, 1.212, 1.11...\n",
       "18    (1.395, 1.395, 1.3715000000000002, 1.3245, 1.2...\n",
       "19    (1.4, 1.4, 1.353, 1.306, 1.212, 1.117500000000...\n",
       "20    (1.395, 1.395, 1.348, 1.2774999999999999, 1.18...\n",
       "21    (1.4, 1.4, 1.353, 1.259, 1.1644999999999999, 1...\n",
       "22    (19.689500000000002, 53.3225, 61.7309999999999...\n",
       "23    (170.8075, 100.37, -17.026, 100.37, -24.852, 5...\n",
       "24    (1.6085, 1.24, 1.7925, 2.5295, 0.6255, 1.424, ...\n",
       "25    (-0.04655, -0.031549999999999995, -0.01003, -0...\n",
       "26    (0.08485, 0.112, 0.09235, 0.1295, 0.07745, 0.1...\n",
       "27    (-0.0034750000000000002, 0.0143, 0.03204999999...\n",
       "28    (0.00445, -0.00181, 0.00445, -0.0206, 0.01695,...\n",
       "29    (0.01865, -0.021199999999999997, -0.0371, 0.06...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subject00.resampled_confounds =\n",
    "df([(item[0],tuple(val.mid for val in pd.cut(item[1],bins=subject00.events.shape[0])))\n",
    " for item in subject00.confounds.iteritems()])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and display a correlation matrix\n",
    "### - Plot the correlation matrix\n",
    "#### - Make a large figure\n",
    "#### - Mask the main diagonal for visualization:\n",
    "#### - The labels we have start with the background (0).\n",
    "    - Hence we skip the first label.\n",
    "#### - Matrices are ordered for block-like representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "correlation_matrix = correlation_measure.fit_transform([time_series])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43670, 43670)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of labels unequal to length of matrix.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3747/1860372804.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                      \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                      \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                      reorder=True)\n\u001b[0m",
      "\u001b[0;32m~/myenv/lib/python3.7/site-packages/nilearn/plotting/matrix_plotting.py\u001b[0m in \u001b[0;36mplot_matrix\u001b[0;34m(mat, title, labels, figure, axes, colorbar, cmap, tri, auto_fit, grid, reorder, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of labels unequal to length of matrix.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreorder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of labels unequal to length of matrix."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from nilearn import plotting\n",
    "np.fill_diagonal(correlation_matrix, 0)\n",
    "plotting.plot_matrix(correlation_matrix,\n",
    "                     figure=(10, 8),\n",
    "                     labels=difumo512_3mm.labels,\n",
    "                     vmax=0.8,\n",
    "                     vmin=-0.8,\n",
    "                     reorder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same thing without confounds, to stress the importance of confounds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = masker.fit_transform(fmri_filenames)\n",
    "# Note how we did not specify confounds above. This is bad!\n",
    "\n",
    "correlation_matrix = correlation_measure.fit_transform([time_series])[0]\n",
    "\n",
    "# Mask the main diagonal for visualization:\n",
    "np.fill_diagonal(correlation_matrix, 0)\n",
    "\n",
    "plotting.plot_matrix(correlation_matrix, figure=(10, 8), labels=labels[1:],\n",
    "                     vmax=0.8, vmin=-0.8, title='No confounds', reorder=True)\n",
    "\n",
    "plotting.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
