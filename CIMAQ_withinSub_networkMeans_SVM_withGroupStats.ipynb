{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within-subject SVM classification based on beta weights (per trials) averaged within networks from different grains of MIST parcellation, for CIMAQ memory encoding task (fMRI data).\n",
    "Mean network betas reflect the engagement of a particular network for each trial.\n",
    "MIST Parcellations include: 7, 12, 20, 36, 64, 122, 197, 325, 444 networks\n",
    "\n",
    "Trials (conditions) are classifierd according to:\n",
    "- task condition (encoding or control task)\n",
    "- memory performance (hit vs miss, correct vs incorrect source)\n",
    "- stimulus category (?)\n",
    "\n",
    "Each model is ran and tested on data from the same subject, and then group statistics (confidence intervals) are computed around accuracy scores from each individual participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/datasets/__init__.py:89: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nilearn\n",
    "import scipy\n",
    "import nibabel as nb\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "from numpy import nan as NaN\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import image, plotting\n",
    "from nilearn import masking\n",
    "from nilearn import plotting\n",
    "from nilearn import datasets\n",
    "from nilearn.plotting import plot_stat_map, plot_roi, plot_anat, plot_img, show\n",
    "from nilearn.input_data import NiftiMasker, NiftiLabelsMasker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, f1_score\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import gzip\n",
    "import io\n",
    "import json\n",
    "import nilearn\n",
    "import os\n",
    "import sklearn\n",
    "import tarfile\n",
    "import tempfile\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from bids_validator import BIDSValidator\n",
    "\n",
    "from fetch_difumo import fetch_difumo\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from os import listdir as ls\n",
    "from os.path import basename as bname\n",
    "from os.path import dirname as dname\n",
    "from os.path import expanduser as xpu\n",
    "from os.path import join as pjoin\n",
    "from pandas import DataFrame as df\n",
    "from tempfile import TemporaryDirectory as tmpdir\n",
    "from tempfile import TemporaryFile as tmpfile\n",
    "from tqdm import tqdm\n",
    "from typing import Union\n",
    "from collections.abc import Iterable\n",
    "from typing import Sequence\n",
    "\n",
    "import loadutils as lu\n",
    "import sniffbytes as snif\n",
    "import scanzip as szip\n",
    "import shutil\n",
    "\n",
    "from nilearn import masking\n",
    "from nilearn.plotting import plot_stat_map, plot_anat, plot_img, plot_epi\n",
    "from nilearn.image import concat_imgs, mean_img\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "def lst_intersection(lst1, lst2):\n",
    "    '''\n",
    "    Source: https://www.geeksforgeeks.org/python-intersection-two-lists/\n",
    "    '''\n",
    "    return [value for value in lst1 if value in set(lst2)]\n",
    "\n",
    "def read_json(fpath:Union[str, os.PathLike]) -> dict:\n",
    "    with open(fpath, mode='r', encoding='UTF-8') as jfile:\n",
    "        jdict = json.load(jfile)\n",
    "    jfile.close()\n",
    "    return jdict\n",
    "        \n",
    "# with tmpfile(prefix=os.getcwd()+\"/\",\n",
    "#             dir=os.path.splitext(bname(src_path))[0]+\"/\",\n",
    "#             suffix=os.path.splitext(topname)[0]) as adir:\n",
    "#     print(adir.name)\n",
    "# tardir = xpu('~/../../data/cisl/DATA/cimaq_20190901/sourcedata')\n",
    "# topname = 'cimaq_dicoms_20190901_v3.tar.gz'\n",
    "# member = 'seriesUID_qc.tsv'\n",
    "# tardir = '/media/francois/seagate_1tb/cimaq_dicoms_20190901'\n",
    "# help(tempfile)\n",
    "# src_path = pjoin(tardir, topname)\n",
    "cimaq_nov_dir = xpu('~/../../data/cisl/DATA/cimaq_20190901')\n",
    "cimaq_mar_dir = xpu('~/../../data/cisl/DATA/cimaq_03-19')\n",
    "events_path = xpu('~/../../data/cisl/DATA/cimaq_corrected_events/events')\n",
    "behav_path = xpu('~/../../data/cisl/DATA/cimaq_corrected_behavioural/behavioural')\n",
    "participants_desc = read_json(fpath=pjoin(cimaq_mar_dir, 'participants.json'))\n",
    "dataset_desc = read_json(fpath=pjoin(cimaq_mar_dir, 'dataset_description.json'))\n",
    "# post_scan_desc = read_json(fpath=pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data/task_files/PostScanBehav_CIMAQ_memory.json'))\n",
    "# taskfile_headers = read_json(pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data/task_files/TaskFile_headers_CIMAQ_memory.json'))\n",
    "# sorted(ls(pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data/features/beta_maps')))\n",
    "task_results = pd.read_csv(pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data/participants/TaskResults/fMRI_behavMemoScores.tsv'),\n",
    "                           sep='\\t')\n",
    "MemoTaskParticipantFile = pd.read_csv(pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data/participants/MemoTaskParticipantFile.tsv'),\n",
    "                           sep='\\t')\n",
    "# MemoTaskParticipantFile\n",
    "# task_results.columns\n",
    "# sorted(ls(pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data/participants/')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list_TaskQC=pd.read_csv(pjoin(cimaq_mar_dir,\n",
    "                                  'derivatives/CIMAQ_fmri_memory/data/participants/sub_list_TaskQC.tsv'),\n",
    "                            sep='\\t')\n",
    "qc_list=lu.flatten(sub_list_TaskQC.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>cognitive_status</th>\n",
       "      <th>clinical_age_months</th>\n",
       "      <th>schooling</th>\n",
       "      <th>sex</th>\n",
       "      <th>APOE_carrier</th>\n",
       "      <th>task_randomization</th>\n",
       "      <th>QC_status</th>\n",
       "      <th>total_frames</th>\n",
       "      <th>total_scrubbed_frames</th>\n",
       "      <th>...</th>\n",
       "      <th>rt_reco_Hit</th>\n",
       "      <th>rt_reco_Miss</th>\n",
       "      <th>rt_reco_New</th>\n",
       "      <th>rt_reco_Old</th>\n",
       "      <th>rt_reco_wrongSource</th>\n",
       "      <th>rt_reco_correctSource</th>\n",
       "      <th>rt_source_FA</th>\n",
       "      <th>rt_source_Hit</th>\n",
       "      <th>rt_source_wrongSource</th>\n",
       "      <th>rt_source_correctSource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108391</td>\n",
       "      <td>SCD</td>\n",
       "      <td>843.6</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>Pass</td>\n",
       "      <td>310</td>\n",
       "      <td>131</td>\n",
       "      <td>...</td>\n",
       "      <td>1.621476</td>\n",
       "      <td>2.450333</td>\n",
       "      <td>2.582385</td>\n",
       "      <td>1.780872</td>\n",
       "      <td>1.827200</td>\n",
       "      <td>1.525791</td>\n",
       "      <td>16.804000</td>\n",
       "      <td>2.655714</td>\n",
       "      <td>4.398050</td>\n",
       "      <td>1.845326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120839</td>\n",
       "      <td>Controls</td>\n",
       "      <td>940.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>Pass</td>\n",
       "      <td>310</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>2.158792</td>\n",
       "      <td>2.985500</td>\n",
       "      <td>2.069487</td>\n",
       "      <td>2.222385</td>\n",
       "      <td>2.610000</td>\n",
       "      <td>2.086016</td>\n",
       "      <td>1.103667</td>\n",
       "      <td>0.862556</td>\n",
       "      <td>1.914000</td>\n",
       "      <td>0.692968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122922</td>\n",
       "      <td>SCD</td>\n",
       "      <td>894.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>Pass</td>\n",
       "      <td>310</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>2.359636</td>\n",
       "      <td>2.163913</td>\n",
       "      <td>2.191769</td>\n",
       "      <td>2.301923</td>\n",
       "      <td>2.589933</td>\n",
       "      <td>2.083280</td>\n",
       "      <td>4.812400</td>\n",
       "      <td>2.986782</td>\n",
       "      <td>3.402033</td>\n",
       "      <td>2.488480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127228</td>\n",
       "      <td>SCD</td>\n",
       "      <td>879.6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>Pass</td>\n",
       "      <td>310</td>\n",
       "      <td>197</td>\n",
       "      <td>...</td>\n",
       "      <td>2.238826</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>2.228667</td>\n",
       "      <td>2.270115</td>\n",
       "      <td>2.302071</td>\n",
       "      <td>2.222727</td>\n",
       "      <td>2.056000</td>\n",
       "      <td>2.226290</td>\n",
       "      <td>3.569929</td>\n",
       "      <td>1.884273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139593</td>\n",
       "      <td>MCI</td>\n",
       "      <td>906.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C</td>\n",
       "      <td>Pass</td>\n",
       "      <td>310</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>2.522919</td>\n",
       "      <td>2.654415</td>\n",
       "      <td>2.499128</td>\n",
       "      <td>2.592038</td>\n",
       "      <td>2.639591</td>\n",
       "      <td>2.351800</td>\n",
       "      <td>2.833889</td>\n",
       "      <td>3.400027</td>\n",
       "      <td>3.283455</td>\n",
       "      <td>3.571000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>979001</td>\n",
       "      <td>Controls</td>\n",
       "      <td>795.6</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C</td>\n",
       "      <td>Pass</td>\n",
       "      <td>310</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1.022971</td>\n",
       "      <td>1.272000</td>\n",
       "      <td>1.204538</td>\n",
       "      <td>1.054897</td>\n",
       "      <td>1.097100</td>\n",
       "      <td>1.010190</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.651074</td>\n",
       "      <td>0.844500</td>\n",
       "      <td>0.617724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>983291</td>\n",
       "      <td>SCD</td>\n",
       "      <td>796.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Pass</td>\n",
       "      <td>310</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>2.944533</td>\n",
       "      <td>3.128722</td>\n",
       "      <td>2.289667</td>\n",
       "      <td>2.987038</td>\n",
       "      <td>5.110400</td>\n",
       "      <td>2.511360</td>\n",
       "      <td>0.471000</td>\n",
       "      <td>0.606533</td>\n",
       "      <td>1.027800</td>\n",
       "      <td>0.522280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>988602</td>\n",
       "      <td>SCD</td>\n",
       "      <td>813.6</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>Pass</td>\n",
       "      <td>310</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>1.219658</td>\n",
       "      <td>1.734000</td>\n",
       "      <td>1.361000</td>\n",
       "      <td>1.232846</td>\n",
       "      <td>1.348091</td>\n",
       "      <td>1.197923</td>\n",
       "      <td>0.633250</td>\n",
       "      <td>0.495447</td>\n",
       "      <td>0.679545</td>\n",
       "      <td>0.464292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>996599</td>\n",
       "      <td>MCI</td>\n",
       "      <td>820.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Pass</td>\n",
       "      <td>310</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>1.839373</td>\n",
       "      <td>3.161818</td>\n",
       "      <td>2.172077</td>\n",
       "      <td>2.025872</td>\n",
       "      <td>1.978560</td>\n",
       "      <td>1.756524</td>\n",
       "      <td>1.797375</td>\n",
       "      <td>1.494627</td>\n",
       "      <td>1.393880</td>\n",
       "      <td>1.554595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>998166</td>\n",
       "      <td>Controls</td>\n",
       "      <td>889.2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Pass</td>\n",
       "      <td>288</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>2.117060</td>\n",
       "      <td>2.780143</td>\n",
       "      <td>1.698205</td>\n",
       "      <td>2.355090</td>\n",
       "      <td>2.474333</td>\n",
       "      <td>2.004237</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>1.073700</td>\n",
       "      <td>1.651083</td>\n",
       "      <td>0.891368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     participant_id cognitive_status  clinical_age_months  schooling  sex  \\\n",
       "0            108391              SCD                843.6       21.0    2   \n",
       "1            120839         Controls                940.8       14.0    2   \n",
       "2            122922              SCD                894.0       18.0    1   \n",
       "3            127228              SCD                879.6       14.0    2   \n",
       "4            139593              MCI                906.0       16.0    2   \n",
       "..              ...              ...                  ...        ...  ...   \n",
       "98           979001         Controls                795.6       19.0    1   \n",
       "99           983291              SCD                796.8       16.0    2   \n",
       "100          988602              SCD                813.6       17.0    2   \n",
       "101          996599              MCI                820.8       13.0    2   \n",
       "102          998166         Controls                889.2       14.0    2   \n",
       "\n",
       "     APOE_carrier task_randomization QC_status  total_frames  \\\n",
       "0             0.0                  B      Pass           310   \n",
       "1             0.0                  D      Pass           310   \n",
       "2             1.0                  D      Pass           310   \n",
       "3             0.0                  B      Pass           310   \n",
       "4             0.0                  C      Pass           310   \n",
       "..            ...                ...       ...           ...   \n",
       "98            0.0                  C      Pass           310   \n",
       "99            0.0                  A      Pass           310   \n",
       "100           1.0                  D      Pass           310   \n",
       "101           1.0                  A      Pass           310   \n",
       "102           0.0                  A      Pass           288   \n",
       "\n",
       "     total_scrubbed_frames  ...  rt_reco_Hit  rt_reco_Miss  rt_reco_New  \\\n",
       "0                      131  ...     1.621476      2.450333     2.582385   \n",
       "1                       50  ...     2.158792      2.985500     2.069487   \n",
       "2                        6  ...     2.359636      2.163913     2.191769   \n",
       "3                      197  ...     2.238826      2.510000     2.228667   \n",
       "4                       75  ...     2.522919      2.654415     2.499128   \n",
       "..                     ...  ...          ...           ...          ...   \n",
       "98                       9  ...     1.022971      1.272000     1.204538   \n",
       "99                      77  ...     2.944533      3.128722     2.289667   \n",
       "100                     41  ...     1.219658      1.734000     1.361000   \n",
       "101                     48  ...     1.839373      3.161818     2.172077   \n",
       "102                     18  ...     2.117060      2.780143     1.698205   \n",
       "\n",
       "     rt_reco_Old  rt_reco_wrongSource  rt_reco_correctSource  rt_source_FA  \\\n",
       "0       1.780872             1.827200               1.525791     16.804000   \n",
       "1       2.222385             2.610000               2.086016      1.103667   \n",
       "2       2.301923             2.589933               2.083280      4.812400   \n",
       "3       2.270115             2.302071               2.222727      2.056000   \n",
       "4       2.592038             2.639591               2.351800      2.833889   \n",
       "..           ...                  ...                    ...           ...   \n",
       "98      1.054897             1.097100               1.010190      0.895000   \n",
       "99      2.987038             5.110400               2.511360      0.471000   \n",
       "100     1.232846             1.348091               1.197923      0.633250   \n",
       "101     2.025872             1.978560               1.756524      1.797375   \n",
       "102     2.355090             2.474333               2.004237      0.663000   \n",
       "\n",
       "     rt_source_Hit  rt_source_wrongSource  rt_source_correctSource  \n",
       "0         2.655714               4.398050                 1.845326  \n",
       "1         0.862556               1.914000                 0.692968  \n",
       "2         2.986782               3.402033                 2.488480  \n",
       "3         2.226290               3.569929                 1.884273  \n",
       "4         3.400027               3.283455                 3.571000  \n",
       "..             ...                    ...                      ...  \n",
       "98        0.651074               0.844500                 0.617724  \n",
       "99        0.606533               1.027800                 0.522280  \n",
       "100       0.495447               0.679545                 0.464292  \n",
       "101       1.494627               1.393880                 1.554595  \n",
       "102       1.073700               1.651083                 0.891368  \n",
       "\n",
       "[103 rows x 83 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MemoTaskParticipantFile['QC_status']=['F' if row[1].participant_id\n",
    "                                      not in qc_list else 'Pass'\n",
    "                                      for row in MemoTaskParticipantFile.iterrows()]\n",
    "MemoTaskParticipantFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: import list of participants, and generate sublists of participants who have enough trials per category for each classification.\n",
    "\n",
    "Encoding vs Control task conditions (all 94)\n",
    "Stimulus category (all 94)\n",
    "Hit versus Miss (42 participants; at least 15 trials per condition)\n",
    "Correct Source versus Wrong Source (49 participants; at least 15 trials per condition)\n",
    "Correct Source versus Miss (38 participants; at least 15 trials per condition)\n",
    "*NOTE: ADD filter to exclude participants with too many scrubbed frames?? *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      108391\n",
      "1      120839\n",
      "2      122922\n",
      "3      127228\n",
      "4      139593\n",
      "        ...  \n",
      "98     979001\n",
      "99     983291\n",
      "100    988602\n",
      "101    996599\n",
      "102    998166\n",
      "Name: participant_id, Length: 94, dtype: int64\n",
      "94\n",
      "0      108391\n",
      "2      122922\n",
      "4      139593\n",
      "8      164965\n",
      "14     199801\n",
      "17     247659\n",
      "19     255499\n",
      "20     258618\n",
      "21     258912\n",
      "24     271596\n",
      "27     314409\n",
      "29     336665\n",
      "30     337021\n",
      "36     396250\n",
      "37     403131\n",
      "38     408506\n",
      "39     413474\n",
      "41     437101\n",
      "42     439776\n",
      "44     458807\n",
      "45     459801\n",
      "47     484204\n",
      "49     502616\n",
      "55     567214\n",
      "56     597569\n",
      "60     652850\n",
      "65     677561\n",
      "66     711830\n",
      "67     729722\n",
      "68     739694\n",
      "69     748676\n",
      "70     763590\n",
      "71     778749\n",
      "72     783781\n",
      "80     884343\n",
      "81     886007\n",
      "89     920577\n",
      "91     936730\n",
      "95     956130\n",
      "97     974246\n",
      "99     983291\n",
      "102    998166\n",
      "Name: participant_id, dtype: int64\n",
      "42\n",
      "0      108391\n",
      "2      122922\n",
      "4      139593\n",
      "7      150649\n",
      "9      175295\n",
      "10     178101\n",
      "12     197192\n",
      "14     199801\n",
      "16     229301\n",
      "20     258618\n",
      "22     267168\n",
      "28     326073\n",
      "29     336665\n",
      "30     337021\n",
      "31     350555\n",
      "34     385370\n",
      "38     408506\n",
      "39     413474\n",
      "40     427357\n",
      "41     437101\n",
      "44     458807\n",
      "45     459801\n",
      "46     462345\n",
      "49     502616\n",
      "53     549994\n",
      "54     555537\n",
      "55     567214\n",
      "57     619278\n",
      "60     652850\n",
      "62     659068\n",
      "64     668786\n",
      "65     677561\n",
      "66     711830\n",
      "68     739694\n",
      "69     748676\n",
      "71     778749\n",
      "72     783781\n",
      "79     878354\n",
      "80     884343\n",
      "83     893978\n",
      "85     901551\n",
      "86     906145\n",
      "88     915022\n",
      "90     932933\n",
      "91     936730\n",
      "92     938001\n",
      "93     955548\n",
      "96     968913\n",
      "101    996599\n",
      "Name: participant_id, dtype: int64\n",
      "49\n",
      "0      108391\n",
      "2      122922\n",
      "4      139593\n",
      "8      164965\n",
      "14     199801\n",
      "17     247659\n",
      "19     255499\n",
      "20     258618\n",
      "24     271596\n",
      "27     314409\n",
      "29     336665\n",
      "30     337021\n",
      "36     396250\n",
      "37     403131\n",
      "38     408506\n",
      "39     413474\n",
      "41     437101\n",
      "42     439776\n",
      "44     458807\n",
      "45     459801\n",
      "47     484204\n",
      "49     502616\n",
      "55     567214\n",
      "56     597569\n",
      "60     652850\n",
      "65     677561\n",
      "66     711830\n",
      "67     729722\n",
      "68     739694\n",
      "69     748676\n",
      "71     778749\n",
      "72     783781\n",
      "80     884343\n",
      "81     886007\n",
      "91     936730\n",
      "95     956130\n",
      "99     983291\n",
      "102    998166\n",
      "Name: participant_id, dtype: int64\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to directory with participant lists\n",
    "# data_file = '/Users/mombot/Documents/Simexp/CIMAQ/Data/Participants/Splitting/Sub_list.tsv'\n",
    "# sub_data = pd.read_csv(data_file, sep = '\\t')\n",
    "sub_data=MemoTaskParticipantFile\n",
    "# Exclude participants who failed QC\n",
    "sub_data = sub_data[sub_data['QC_status']!= 'F']\n",
    "\n",
    "## ADD filter to exclude participants with too many scrubbed frames?? ##\n",
    "\n",
    "# Set minimal number of trials needed per subject to include them in analysis\n",
    "num = 14\n",
    "\n",
    "# Encoding vs Control, and Stimulus Category classifications\n",
    "all_subs = sub_data['participant_id']\n",
    "all_diagnosis = sub_data['cognitive_status']\n",
    "print(all_subs)\n",
    "print(len(all_subs))\n",
    "\n",
    "# Hit versus Miss\n",
    "hm_data = sub_data[sub_data['hits'] > num]\n",
    "hm_data = hm_data[hm_data['miss'] > num]\n",
    "hm_subs = hm_data['participant_id']\n",
    "hm_diagnosis = hm_data['cognitive_status']\n",
    "print(hm_subs)\n",
    "print(len(hm_subs))\n",
    "\n",
    "# Correct Source versus Wrong Source \n",
    "cw_data = sub_data[sub_data['correct_source'] > num]\n",
    "cw_data = cw_data[cw_data['wrong_source'] > num]\n",
    "cw_subs = cw_data['participant_id']\n",
    "cw_diagnosis = cw_data['cognitive_status']\n",
    "print(cw_subs)\n",
    "print(len(cw_subs))\n",
    "\n",
    "# Correct Source versus Miss\n",
    "cmiss_data = sub_data[sub_data['correct_source'] > num]\n",
    "cmiss_data = cmiss_data[cmiss_data['miss'] > num]\n",
    "cmiss_subs = cmiss_data['participant_id']\n",
    "cmiss_diagnosis = cmiss_data['cognitive_status']\n",
    "print(cmiss_subs)\n",
    "print(len(cmiss_subs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MIST_12.csv',\n",
       " 'MIST_122.csv',\n",
       " 'MIST_197.csv',\n",
       " 'MIST_20.csv',\n",
       " 'MIST_325.csv',\n",
       " 'MIST_36.csv',\n",
       " 'MIST_444.csv',\n",
       " 'MIST_64.csv',\n",
       " 'MIST_7.csv',\n",
       " 'MIST_ROI.csv',\n",
       " 'desktop.ini']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels_file = '/'.join([label_dir, 'sub-'+str(sub)+'_enco_ctl.tsv'])\n",
    "# labels_file\n",
    "label_dir\n",
    "# testlabels=pd.read_csv(labels_file,sep='\\t')\n",
    "# testlabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Set up paths of directories of interest\n",
    "\n",
    "Create empty data structures to save and export classification results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MIST_12.csv',\n",
       " 'MIST_122.csv',\n",
       " 'MIST_197.csv',\n",
       " 'MIST_20.csv',\n",
       " 'MIST_325.csv',\n",
       " 'MIST_36.csv',\n",
       " 'MIST_444.csv',\n",
       " 'MIST_64.csv',\n",
       " 'MIST_7.csv',\n",
       " 'MIST_ROI.csv',\n",
       " 'desktop.ini']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(ls(pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data/templates/MIST_parcellation/Release/Parcel_Information')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MIST_PARCEL_ORDER.csv', 'MIST_PARCEL_ORDER_ROI.csv', 'desktop.ini']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [(item, ls(pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data',item)))\n",
    "#   for item in sorted(ls(pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data')))]\n",
    "mist_path=pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data/templates/MIST_parcellation')\n",
    "sorted(ls(pjoin(mist_path, 'Release/Hierarchy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/fnadeau/../../data/cisl/DATA/cimaq_03-19/derivatives/CIMAQ_fmri_memory/data/task_files/events/sub-108391_enco_ctl.tsv']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'sub-108391_enco_ctl.tsv'\n",
    "datadir=pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data')\n",
    "[itm for itm in lu.loadimages(datadir)\n",
    " if bname(itm) == 'sub-108391_enco_ctl.tsv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set paths to directories of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# beta_dir = '/Users/mombot/Documents/Simexp/CIMAQ/Data/Nistats/Betas'\n",
    "# label_dir = '/Users/mombot/Documents/Simexp/CIMAQ/Data/Nistats/Events'\n",
    "# output_dir = '/Users/mombot/Documents/Simexp/CIMAQ/Data/Nilearn/Group_results'\n",
    "# basc_dir = '/Users/mombot/Documents/Simexp/CIMAQ/Data/MIST/Release/Parcellations'\n",
    "beta_dir = pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data/features/beta_maps')\n",
    "mist_path=pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data/templates/MIST_parcellation')\n",
    "# label_dir = pjoin(mist_path, 'Release/Parcel_Information')\n",
    "label_dir = pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data/task_files/events')\n",
    "mask_dir = pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data/masks')\n",
    "output_dir = pjoin(dname(cimaq_mar_dir), 'cimaq_classification_outputs')\n",
    "basc_dir = pjoin(cimaq_mar_dir,\n",
    "                 'derivatives/CIMAQ_fmri_memory/data/templates/MIST_parcellation/Release/Parcellations')\n",
    "\n",
    "# set the parcellation level and load the parcellation map\n",
    "# 7, 12, 20, 36, 64, 122, 197, 325, 444\n",
    "numnets = [7, 20, 64, 325, 444]\n",
    "os.mkdir(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(ls())\n",
    "b_labels_path=pjoin(cimaq_mar_dir, 'derivatives/CIMAQ_fmri_memory/data/templates/MIST_parcellation/Release/Parcel_Information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. ENCODING VERSUS CONTROL TASK CLASSIFICATION\n",
    "\n",
    "Build and test model for each participant on list, and compile data in a single pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODING VERSUS CONTROL TASK CLASSIFICATION\n",
    "numnets = [7, 20, 64, 325, 444]\n",
    "for numnet in tqdm(numnets):\n",
    "\n",
    "    basc = image.load_img(pjoin(basc_dir, 'MIST_'+str(numnet)+'.nii'))\n",
    "    b_labels = pjoin(b_labels_path,'MIST_'+str(numnet)+'.csv')\n",
    "#     b_labels = '/Users/mombot/Documents/Simexp/CIMAQ/Data/MIST/Release/Parcel_Information/MIST_'+\\\n",
    "#                     str(numnet)+'.csv'\n",
    "    basc_labels = pd.read_csv(b_labels, sep=';')\n",
    "\n",
    "    # build data structure to store accuracy data and coefficients\n",
    "    enc_ctl_data = pd.DataFrame()\n",
    "    enc_ctl_data.insert(loc = 0, column = 'dccid',\n",
    "                        value = np.np.nan, allow_duplicates=True)\n",
    "    for i in range(0, 10):\n",
    "        enc_ctl_data.insert(loc = enc_ctl_data.shape[1], column = 'CV'+str(i+1)+'_acc',\n",
    "                            value = np.nan, allow_duplicates=True)\n",
    "    enc_ctl_data.insert(loc = enc_ctl_data.shape[1], column = 'TrainSet_MeanCV_acc',\n",
    "                        value = np.np.nan, allow_duplicates=True)\n",
    "    enc_ctl_data.insert(loc = enc_ctl_data.shape[1], column = 'TestSet_acc',\n",
    "                        value = np.np.nan, allow_duplicates=True)\n",
    "    netnames = basc_labels['name']\n",
    "    for i in tqdm(range(0, numnet)):\n",
    "        enc_ctl_data.insert(loc = enc_ctl_data.shape[1], column = netnames[i]+'_coef',\n",
    "                            value = np.nan, allow_duplicates=True)\n",
    "\n",
    "    for sub in tqdm(all_subs):\n",
    "        print(sub)\n",
    "        s_data = [sub]\n",
    "        # load subject's beta maps (one per trial)\n",
    "        betas = image.load_img(img=pjoin(beta_dir, str(sub), 'TrialContrasts/betas_sub'+str(sub)+'*.nii'),\n",
    "                               wildcards=True)\n",
    "        # initialize NiftiLabelMasker object    \n",
    "        sub_mask = nb.load(pjoin(mask_dir, 'func_sub'+str(sub)+'_mask_stereonl.nii'))\n",
    "        sub_label_masker = NiftiLabelsMasker(labels_img=basc, standardize=True, mask_img=sub_mask,\n",
    "                                             memory = 'nilearn_cache', verbose=0)\n",
    "\n",
    "        # transform subject's beta maps into vector of network means per trial\n",
    "        X_enc_ctl = sub_label_masker.fit_transform(betas)\n",
    "\n",
    "        # load subject's trial labels\n",
    "        labels_file = pjoin(label_dir, 'sub-'+str(sub)+'_enco_ctl.tsv')\n",
    "        enco_ctl_labels = pd.read_csv(labels_file, sep='\\t')\n",
    "        y_enco_ctl = enco_ctl_labels['condition']\n",
    "\n",
    "        # mask data to exclude trials of no interest\n",
    "        # does not apply here\n",
    "\n",
    "        # Split trials into a training and a test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_enc_ctl, # x\n",
    "            y_enco_ctl, # y\n",
    "            test_size = 0.4, # 60%/40% split\n",
    "            shuffle = True, # shuffle dataset before splitting\n",
    "            stratify = y_enco_ctl, # keep distribution of conditions consistent betw. train & test sets\n",
    "            #random_state = 123  # if set number, same shuffle each time, otherwise randomization algo\n",
    "            ) \n",
    "        print('training:', len(X_train), 'testing:', len(X_test))\n",
    "        print(y_train.value_counts(), y_test.value_counts())\n",
    "\n",
    "        # define the model\n",
    "        sub_svc = SVC(kernel='linear', class_weight='balanced')\n",
    "\n",
    "        # do cross-validation to evaluate model performance\n",
    "        # within 10 folds of training set\n",
    "        # predict\n",
    "        y_pred = cross_val_predict(sub_svc, X_train, y_train,\n",
    "                                   groups=y_train, cv=10)\n",
    "        # scores\n",
    "        cv_acc = cross_val_score(sub_svc, X_train, y_train,\n",
    "                             groups=y_train, cv=10)\n",
    "        print(cv_acc)\n",
    "\n",
    "        for i in tqdm(range(0, len(cv_acc))):\n",
    "            s_data.append(cv_acc[i])\n",
    "\n",
    "        # evaluate overall model performance on training data\n",
    "        overall_acc = accuracy_score(y_pred = y_pred, y_true = y_train)\n",
    "        overall_cr = classification_report(y_pred = y_pred, y_true = y_train)\n",
    "        print('Accuracy:',overall_acc)\n",
    "        print(overall_cr)\n",
    "\n",
    "        s_data.append(overall_acc)\n",
    "\n",
    "        # Test model on unseen data from the test set\n",
    "        sub_svc.fit(X_train, y_train)\n",
    "        y_pred = sub_svc.predict(X_test) # classify age class using testing data\n",
    "        acc = sub_svc.score(X_test, y_test) # get accuracy\n",
    "\n",
    "        cr = classification_report(y_pred=y_pred, y_true=y_test) # get prec., recall & f1\n",
    "        # print results\n",
    "        print('accuracy =', acc)\n",
    "        print(cr)  \n",
    "\n",
    "        s_data.append(acc)\n",
    "\n",
    "        # get coefficients\n",
    "        coef_ = sub_svc.coef_[0]\n",
    "        print(coef_.shape)\n",
    "        print(coef_)\n",
    "\n",
    "        sub_basc = basc_labels.copy()\n",
    "        sub_basc.insert(loc=3, column='coef', value=coef_, allow_duplicates=True)\n",
    "\n",
    "        coef = sub_basc['coef']\n",
    "        for j in tqdm(range(0, len(coef))):\n",
    "            s_data.append(coef[j])\n",
    "\n",
    "        #sub_basc.sort_values(by='coef', axis = 0, ascending = False, inplace=True)\n",
    "        #print(sub_basc.iloc[0:12, 2:4])\n",
    "\n",
    "        enc_ctl_data = enc_ctl_data.append(pd.Series(s_data, index=enc_ctl_data.columns),\n",
    "                                           ignore_index=True)\n",
    "\n",
    "    demo_data = sub_data.copy()\n",
    "    demo_data.reset_index(level=None, drop=False, inplace=True)\n",
    "    col_names = ['cognitive_status','total_scrubbed_frames','mean_FD','miss',\n",
    "                 'correct_source','wrong_source','dprime','associative_memScore']\n",
    "    cols=tuple(zip(range(1,len(col_names)),col_names)\n",
    "    for col in tqdm(cols):\n",
    "        enc_ctl_data.insert(loc=col[0],column=col[1],value=demo_data[col[1]],\n",
    "                            allow_duplicates=True)\n",
    "    enc_ctl_data.to_csv(pjoin(output_dir, 'SVC_withinSub_enc_ctl_'+str(numnet)+'networks.tsv'),\n",
    "                        sep='\\t', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████| 7/7 [00:00<00:00, 1957.73it/s]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/94 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108391\n",
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.42857143 0.28571429 0.57142857 0.42857143 0.57142857 0.57142857\n",
      " 0.42857143 0.71428571 0.71428571 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 94254.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▍                                           | 1/94 [00:01<02:32,  1.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5428571428571428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.33      0.39      0.36        23\n",
      "         Enc       0.67      0.62      0.64        47\n",
      "\n",
      "    accuracy                           0.54        70\n",
      "   macro avg       0.50      0.50      0.50        70\n",
      "weighted avg       0.56      0.54      0.55        70\n",
      "\n",
      "accuracy = 0.6808510638297872\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.53      0.56      0.55        16\n",
      "         Enc       0.77      0.74      0.75        31\n",
      "\n",
      "    accuracy                           0.68        47\n",
      "   macro avg       0.65      0.65      0.65        47\n",
      "weighted avg       0.69      0.68      0.68        47\n",
      "\n",
      "(7,)\n",
      "[-0.00979414  0.76230294 -0.38316207  0.15300677 -0.30065299  0.53401797\n",
      " -0.02227599]\n",
      "120839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.85714286 0.57142857 0.57142857 0.42857143 0.71428571 1.\n",
      " 0.85714286 0.57142857 0.42857143 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 63937.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▉                                           | 2/94 [00:11<10:12,  6.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.50      0.57      0.53        23\n",
      "         Enc       0.77      0.72      0.75        47\n",
      "\n",
      "    accuracy                           0.67        70\n",
      "   macro avg       0.64      0.64      0.64        70\n",
      "weighted avg       0.68      0.67      0.68        70\n",
      "\n",
      "accuracy = 0.574468085106383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.36      0.31      0.33        16\n",
      "         Enc       0.67      0.71      0.69        31\n",
      "\n",
      "    accuracy                           0.57        47\n",
      "   macro avg       0.51      0.51      0.51        47\n",
      "weighted avg       0.56      0.57      0.57        47\n",
      "\n",
      "(7,)\n",
      "[-0.31583132 -0.1602966  -0.20844901  1.04829926  0.23421456  0.02963901\n",
      " -0.22960851]\n",
      "122922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.85714286 0.42857143 0.57142857 0.57142857 0.85714286 0.71428571\n",
      " 0.42857143 1.         0.71428571 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 83055.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|█▍                                          | 3/94 [00:22<13:00,  8.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.48      0.65      0.56        23\n",
      "         Enc       0.79      0.66      0.72        47\n",
      "\n",
      "    accuracy                           0.66        70\n",
      "   macro avg       0.64      0.66      0.64        70\n",
      "weighted avg       0.69      0.66      0.67        70\n",
      "\n",
      "accuracy = 0.7446808510638298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.61      0.69      0.65        16\n",
      "         Enc       0.83      0.77      0.80        31\n",
      "\n",
      "    accuracy                           0.74        47\n",
      "   macro avg       0.72      0.73      0.72        47\n",
      "weighted avg       0.75      0.74      0.75        47\n",
      "\n",
      "(7,)\n",
      "[ 0.48448411  0.76331799  0.41374661  1.23453538 -0.678669    0.79083128\n",
      "  0.03096335]\n",
      "127228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.71428571 0.57142857 0.85714286 0.14285714 0.85714286\n",
      " 0.57142857 0.71428571 0.71428571 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 4897.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|█▊                                          | 4/94 [00:35<15:35, 10.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.48      0.61      0.54        23\n",
      "         Enc       0.78      0.68      0.73        47\n",
      "\n",
      "    accuracy                           0.66        70\n",
      "   macro avg       0.63      0.64      0.63        70\n",
      "weighted avg       0.68      0.66      0.67        70\n",
      "\n",
      "accuracy = 0.6808510638297872\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.53      0.56      0.55        16\n",
      "         Enc       0.77      0.74      0.75        31\n",
      "\n",
      "    accuracy                           0.68        47\n",
      "   macro avg       0.65      0.65      0.65        47\n",
      "weighted avg       0.69      0.68      0.68        47\n",
      "\n",
      "(7,)\n",
      "[ 0.06027514  0.63601991 -0.05427951  0.48893868 -0.88228485  1.0936096\n",
      "  0.43172961]\n",
      "139593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.42857143 0.42857143 0.71428571 0.85714286 0.57142857 0.42857143\n",
      " 0.57142857 0.57142857 0.28571429 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 42069.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|██▎                                         | 5/94 [00:47<15:53, 10.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5571428571428572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.38      0.52      0.44        23\n",
      "         Enc       0.71      0.57      0.64        47\n",
      "\n",
      "    accuracy                           0.56        70\n",
      "   macro avg       0.54      0.55      0.54        70\n",
      "weighted avg       0.60      0.56      0.57        70\n",
      "\n",
      "accuracy = 0.46808510638297873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.32      0.50      0.39        16\n",
      "         Enc       0.64      0.45      0.53        31\n",
      "\n",
      "    accuracy                           0.47        47\n",
      "   macro avg       0.48      0.48      0.46        47\n",
      "weighted avg       0.53      0.47      0.48        47\n",
      "\n",
      "(7,)\n",
      "[ 0.17309862 -0.6166198  -0.07839904  0.56359273  0.20297388 -0.42486749\n",
      " -0.25140485]\n",
      "147863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.85714286 0.71428571 0.42857143 0.57142857 0.71428571 0.57142857\n",
      " 0.85714286 0.71428571 0.71428571 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 72315.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|██▊                                         | 6/94 [00:58<15:56, 10.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.55      0.52      0.53        23\n",
      "         Enc       0.77      0.79      0.78        47\n",
      "\n",
      "    accuracy                           0.70        70\n",
      "   macro avg       0.66      0.65      0.66        70\n",
      "weighted avg       0.70      0.70      0.70        70\n",
      "\n",
      "accuracy = 0.5319148936170213\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.38      0.56      0.45        16\n",
      "         Enc       0.70      0.52      0.59        31\n",
      "\n",
      "    accuracy                           0.53        47\n",
      "   macro avg       0.54      0.54      0.52        47\n",
      "weighted avg       0.59      0.53      0.54        47\n",
      "\n",
      "(7,)\n",
      "[-0.06773451 -0.16144009 -0.28471106  0.94107212  0.55009186 -0.59676641\n",
      " -0.4063244 ]\n",
      "150649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.71428571 0.85714286 0.71428571 0.57142857 0.71428571\n",
      " 0.71428571 0.71428571 0.71428571 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 43873.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|███▎                                        | 7/94 [01:09<15:49, 10.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.56      0.65      0.60        23\n",
      "         Enc       0.81      0.74      0.78        47\n",
      "\n",
      "    accuracy                           0.71        70\n",
      "   macro avg       0.68      0.70      0.69        70\n",
      "weighted avg       0.73      0.71      0.72        70\n",
      "\n",
      "accuracy = 0.723404255319149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.58      0.69      0.63        16\n",
      "         Enc       0.82      0.74      0.78        31\n",
      "\n",
      "    accuracy                           0.72        47\n",
      "   macro avg       0.70      0.71      0.70        47\n",
      "weighted avg       0.74      0.72      0.73        47\n",
      "\n",
      "(7,)\n",
      "[-0.48331193  0.16739143 -0.24183443  0.0122942  -0.54774847  0.40598602\n",
      " -1.36188041]\n",
      "164965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.57142857 0.71428571 0.57142857 0.71428571 0.71428571\n",
      " 0.42857143 0.42857143 0.57142857 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 75983.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|███▋                                        | 8/94 [01:20<15:53, 11.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.41      0.57      0.47        23\n",
      "         Enc       0.74      0.60      0.66        47\n",
      "\n",
      "    accuracy                           0.59        70\n",
      "   macro avg       0.57      0.58      0.57        70\n",
      "weighted avg       0.63      0.59      0.60        70\n",
      "\n",
      "accuracy = 0.6170212765957447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.43      0.38      0.40        16\n",
      "         Enc       0.70      0.74      0.72        31\n",
      "\n",
      "    accuracy                           0.62        47\n",
      "   macro avg       0.56      0.56      0.56        47\n",
      "weighted avg       0.61      0.62      0.61        47\n",
      "\n",
      "(7,)\n",
      "[-0.57350351  0.2603522   0.02124378  0.42086306  0.11866738  0.08674799\n",
      " -0.44453039]\n",
      "175295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.28571429 0.85714286 0.85714286 0.71428571 0.85714286 0.85714286\n",
      " 0.71428571 0.71428571 0.57142857 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 7430.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████▏                                       | 9/94 [01:31<15:24, 10.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.54      0.65      0.59        23\n",
      "         Enc       0.81      0.72      0.76        47\n",
      "\n",
      "    accuracy                           0.70        70\n",
      "   macro avg       0.67      0.69      0.68        70\n",
      "weighted avg       0.72      0.70      0.71        70\n",
      "\n",
      "accuracy = 0.6382978723404256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.48      0.62      0.54        16\n",
      "         Enc       0.77      0.65      0.70        31\n",
      "\n",
      "    accuracy                           0.64        47\n",
      "   macro avg       0.62      0.64      0.62        47\n",
      "weighted avg       0.67      0.64      0.65        47\n",
      "\n",
      "(7,)\n",
      "[0.53874016 0.17814839 0.28023579 1.25872834 0.6420219  1.41714041\n",
      " 0.46445371]\n",
      "178101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.57142857 0.85714286 0.42857143 0.71428571 0.71428571\n",
      " 0.71428571 0.57142857 0.71428571 1.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 61320.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|████▌                                      | 10/94 [01:41<14:48, 10.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.54      0.57      0.55        23\n",
      "         Enc       0.78      0.77      0.77        47\n",
      "\n",
      "    accuracy                           0.70        70\n",
      "   macro avg       0.66      0.67      0.66        70\n",
      "weighted avg       0.70      0.70      0.70        70\n",
      "\n",
      "accuracy = 0.7446808510638298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.61      0.69      0.65        16\n",
      "         Enc       0.83      0.77      0.80        31\n",
      "\n",
      "    accuracy                           0.74        47\n",
      "   macro avg       0.72      0.73      0.72        47\n",
      "weighted avg       0.75      0.74      0.75        47\n",
      "\n",
      "(7,)\n",
      "[-0.78535412  0.07275724  0.11362117  0.5874773  -0.80416745  0.17756179\n",
      " -0.54025197]\n",
      "189005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.85714286 0.71428571 0.71428571 0.71428571 1.         1.\n",
      " 0.71428571 0.85714286 0.42857143 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 77672.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█████                                      | 11/94 [01:52<14:52, 10.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.64      0.78      0.71        23\n",
      "         Enc       0.88      0.79      0.83        47\n",
      "\n",
      "    accuracy                           0.79        70\n",
      "   macro avg       0.76      0.78      0.77        70\n",
      "weighted avg       0.80      0.79      0.79        70\n",
      "\n",
      "accuracy = 0.7872340425531915\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.62      0.94      0.75        16\n",
      "         Enc       0.96      0.71      0.81        31\n",
      "\n",
      "    accuracy                           0.79        47\n",
      "   macro avg       0.79      0.82      0.78        47\n",
      "weighted avg       0.84      0.79      0.79        47\n",
      "\n",
      "(7,)\n",
      "[ 0.76270933  0.92820426  0.50449263  0.66909371 -1.10842856 -0.42674763\n",
      " -0.62922496]\n",
      "197192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.42857143 0.14285714 0.71428571 0.85714286 0.42857143 0.42857143\n",
      " 0.57142857 0.42857143 0.42857143 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 71089.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█████▍                                     | 12/94 [02:03<14:45, 10.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.30      0.43      0.36        23\n",
      "         Enc       0.65      0.51      0.57        47\n",
      "\n",
      "    accuracy                           0.49        70\n",
      "   macro avg       0.48      0.47      0.46        70\n",
      "weighted avg       0.54      0.49      0.50        70\n",
      "\n",
      "accuracy = 0.5319148936170213\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.38      0.62      0.48        16\n",
      "         Enc       0.71      0.48      0.58        31\n",
      "\n",
      "    accuracy                           0.53        47\n",
      "   macro avg       0.55      0.55      0.53        47\n",
      "weighted avg       0.60      0.53      0.54        47\n",
      "\n",
      "(7,)\n",
      "[ 1.03606014  0.49438853  0.94506672  0.8287903   0.25341686 -0.02594215\n",
      "  0.42724877]\n",
      "199801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 1.         0.28571429 0.71428571 0.85714286 0.28571429\n",
      " 0.85714286 0.85714286 0.71428571 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 64527.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█████▉                                     | 13/94 [02:15<15:17, 11.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.48      0.65      0.56        23\n",
      "         Enc       0.79      0.66      0.72        47\n",
      "\n",
      "    accuracy                           0.66        70\n",
      "   macro avg       0.64      0.66      0.64        70\n",
      "weighted avg       0.69      0.66      0.67        70\n",
      "\n",
      "accuracy = 0.6808510638297872\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.53      0.62      0.57        16\n",
      "         Enc       0.79      0.71      0.75        31\n",
      "\n",
      "    accuracy                           0.68        47\n",
      "   macro avg       0.66      0.67      0.66        47\n",
      "weighted avg       0.70      0.68      0.69        47\n",
      "\n",
      "(7,)\n",
      "[-0.74635865  0.38822716  0.59780207  0.89081953 -0.39871009  0.12033234\n",
      " -1.40525167]\n",
      "219637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.42857143 0.85714286 0.42857143 0.57142857 0.85714286\n",
      " 1.         0.42857143 0.71428571 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 38800.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|██████▍                                    | 14/94 [02:26<14:48, 11.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6428571428571429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.47      0.65      0.55        23\n",
      "         Enc       0.79      0.64      0.71        47\n",
      "\n",
      "    accuracy                           0.64        70\n",
      "   macro avg       0.63      0.65      0.63        70\n",
      "weighted avg       0.68      0.64      0.65        70\n",
      "\n",
      "accuracy = 0.7446808510638298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.61      0.69      0.65        16\n",
      "         Enc       0.83      0.77      0.80        31\n",
      "\n",
      "    accuracy                           0.74        47\n",
      "   macro avg       0.72      0.73      0.72        47\n",
      "weighted avg       0.75      0.74      0.75        47\n",
      "\n",
      "(7,)\n",
      "[-0.09500359  0.55849714 -0.16421643  1.31823232 -0.19398605  0.84338765\n",
      " -1.00799896]\n",
      "229301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.57s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 67 testing: 46\n",
      "Enc    44\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    15\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.71428571 0.71428571 0.42857143 0.14285714 0.71428571\n",
      " 0.28571429 0.66666667 0.33333333 0.66666667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 63072.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|██████▊                                    | 15/94 [02:37<14:30, 11.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5373134328358209\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.36      0.43      0.39        23\n",
      "         Enc       0.67      0.59      0.63        44\n",
      "\n",
      "    accuracy                           0.54        67\n",
      "   macro avg       0.51      0.51      0.51        67\n",
      "weighted avg       0.56      0.54      0.55        67\n",
      "\n",
      "accuracy = 0.5434782608695652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.36      0.53      0.43        15\n",
      "         Enc       0.71      0.55      0.62        31\n",
      "\n",
      "    accuracy                           0.54        46\n",
      "   macro avg       0.54      0.54      0.53        46\n",
      "weighted avg       0.60      0.54      0.56        46\n",
      "\n",
      "(7,)\n",
      "[-0.03266533 -0.30995519 -0.34809358 -0.68445903  0.31051599 -0.74962763\n",
      " -0.86250122]\n",
      "247659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.55s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 65 testing: 44\n",
      "Enc    42\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    29\n",
      "CTL    15\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.71428571 0.42857143 0.42857143 0.71428571 1.\n",
      " 0.5        0.33333333 0.33333333 0.66666667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 60436.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|███████▎                                   | 16/94 [02:46<13:51, 10.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5846153846153846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.43      0.52      0.47        23\n",
      "         Enc       0.70      0.62      0.66        42\n",
      "\n",
      "    accuracy                           0.58        65\n",
      "   macro avg       0.57      0.57      0.56        65\n",
      "weighted avg       0.61      0.58      0.59        65\n",
      "\n",
      "accuracy = 0.4772727272727273\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.30      0.40      0.34        15\n",
      "         Enc       0.62      0.52      0.57        29\n",
      "\n",
      "    accuracy                           0.48        44\n",
      "   macro avg       0.46      0.46      0.45        44\n",
      "weighted avg       0.51      0.48      0.49        44\n",
      "\n",
      "(7,)\n",
      "[-0.40738525 -0.19078619 -1.19844625 -0.1520249  -0.83962394  0.18108748\n",
      " -0.43496684]\n",
      "254402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.85714286 0.71428571 0.71428571 0.57142857 0.57142857 0.85714286\n",
      " 0.42857143 0.71428571 0.71428571 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 58173.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|███████▊                                   | 17/94 [02:57<13:42, 10.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.52      0.57      0.54        23\n",
      "         Enc       0.78      0.74      0.76        47\n",
      "\n",
      "    accuracy                           0.69        70\n",
      "   macro avg       0.65      0.65      0.65        70\n",
      "weighted avg       0.69      0.69      0.69        70\n",
      "\n",
      "accuracy = 0.574468085106383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.33      0.25      0.29        16\n",
      "         Enc       0.66      0.74      0.70        31\n",
      "\n",
      "    accuracy                           0.57        47\n",
      "   macro avg       0.50      0.50      0.49        47\n",
      "weighted avg       0.55      0.57      0.56        47\n",
      "\n",
      "(7,)\n",
      "[-0.21743803 -0.34353073 -0.10343128  0.21839251 -0.62997189  0.76191344\n",
      "  0.05005798]\n",
      "255499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.57s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 69 testing: 46\n",
      "Enc    46\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    30\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.28571429 0.28571429 0.28571429 0.28571429 0.28571429 0.28571429\n",
      " 0.42857143 0.42857143 0.42857143 0.16666667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 3918.45it/s]\u001b[A\u001b[A\n",
      "/home/fnadeau/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/fnadeau/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/fnadeau/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 19%|████████▏                                  | 18/94 [03:08<13:30, 10.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3188405797101449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.32      0.96      0.48        23\n",
      "         Enc       0.00      0.00      0.00        46\n",
      "\n",
      "    accuracy                           0.32        69\n",
      "   macro avg       0.16      0.48      0.24        69\n",
      "weighted avg       0.11      0.32      0.16        69\n",
      "\n",
      "accuracy = 0.34782608695652173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.35      1.00      0.52        16\n",
      "         Enc       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           0.35        46\n",
      "   macro avg       0.17      0.50      0.26        46\n",
      "weighted avg       0.12      0.35      0.18        46\n",
      "\n",
      "(7,)\n",
      "[-0.16919077 -0.01734982  0.08873139 -0.10500137  0.02476312  0.05478366\n",
      "  0.2591488 ]\n",
      "258618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.85714286 0.85714286 0.57142857 0.85714286 0.85714286 0.85714286\n",
      " 0.71428571 0.42857143 0.71428571 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 50051.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|████████▋                                  | 19/94 [03:18<13:20, 10.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7428571428571429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.58      0.78      0.67        23\n",
      "         Enc       0.87      0.72      0.79        47\n",
      "\n",
      "    accuracy                           0.74        70\n",
      "   macro avg       0.73      0.75      0.73        70\n",
      "weighted avg       0.78      0.74      0.75        70\n",
      "\n",
      "accuracy = 0.5531914893617021\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.39      0.56      0.46        16\n",
      "         Enc       0.71      0.55      0.62        31\n",
      "\n",
      "    accuracy                           0.55        47\n",
      "   macro avg       0.55      0.56      0.54        47\n",
      "weighted avg       0.60      0.55      0.56        47\n",
      "\n",
      "(7,)\n",
      "[ 0.81764708  0.60548491  1.07271563  0.78862108 -0.68048219  0.62738941\n",
      " -0.27304952]\n",
      "258912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.57s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 67 testing: 46\n",
      "Enc    44\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    30\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.28571429 0.42857143 0.28571429 0.28571429 0.42857143 0.57142857\n",
      " 0.57142857 0.         0.66666667 0.33333333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|███████████████████████████████████████| 10/10 [00:00<00:00, 103819.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|█████████▏                                 | 20/94 [03:28<12:51, 10.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3880597014925373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.15      0.17      0.16        23\n",
      "         Enc       0.54      0.50      0.52        44\n",
      "\n",
      "    accuracy                           0.39        67\n",
      "   macro avg       0.35      0.34      0.34        67\n",
      "weighted avg       0.41      0.39      0.40        67\n",
      "\n",
      "accuracy = 0.4782608695652174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.30      0.38      0.33        16\n",
      "         Enc       0.62      0.53      0.57        30\n",
      "\n",
      "    accuracy                           0.48        46\n",
      "   macro avg       0.46      0.45      0.45        46\n",
      "weighted avg       0.51      0.48      0.49        46\n",
      "\n",
      "(7,)\n",
      "[-0.54142915 -0.72338867 -0.15876605 -0.24403766 -0.24610918  0.14417016\n",
      " -0.57499535]\n",
      "267168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.71428571 0.28571429 0.42857143 0.85714286 0.42857143\n",
      " 0.57142857 0.85714286 0.57142857 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 65638.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|█████████▌                                 | 21/94 [03:39<12:43, 10.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.43      0.52      0.47        23\n",
      "         Enc       0.74      0.66      0.70        47\n",
      "\n",
      "    accuracy                           0.61        70\n",
      "   macro avg       0.58      0.59      0.58        70\n",
      "weighted avg       0.64      0.61      0.62        70\n",
      "\n",
      "accuracy = 0.574468085106383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.39      0.44      0.41        16\n",
      "         Enc       0.69      0.65      0.67        31\n",
      "\n",
      "    accuracy                           0.57        47\n",
      "   macro avg       0.54      0.54      0.54        47\n",
      "weighted avg       0.59      0.57      0.58        47\n",
      "\n",
      "(7,)\n",
      "[-0.3434164   0.25635513 -0.36832398  0.77114285 -0.22656864  0.60310338\n",
      " -0.09459192]\n",
      "270218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.71428571 1.         0.85714286 0.57142857 0.85714286\n",
      " 0.85714286 0.42857143 0.57142857 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 78840.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██████████                                 | 22/94 [03:51<13:13, 11.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7285714285714285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.58      0.65      0.61        23\n",
      "         Enc       0.82      0.77      0.79        47\n",
      "\n",
      "    accuracy                           0.73        70\n",
      "   macro avg       0.70      0.71      0.70        70\n",
      "weighted avg       0.74      0.73      0.73        70\n",
      "\n",
      "accuracy = 0.6808510638297872\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.53      0.56      0.55        16\n",
      "         Enc       0.77      0.74      0.75        31\n",
      "\n",
      "    accuracy                           0.68        47\n",
      "   macro avg       0.65      0.65      0.65        47\n",
      "weighted avg       0.69      0.68      0.68        47\n",
      "\n",
      "(7,)\n",
      "[ 0.27656787  0.25317971 -0.71638508  0.68499131 -0.68365798  0.66459255\n",
      " -0.45606027]\n",
      "271596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.57s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 67 testing: 46\n",
      "Enc    44\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    30\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.42857143 0.85714286 0.71428571 0.85714286 0.71428571\n",
      " 0.71428571 0.66666667 0.5        0.83333333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 87199.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██████████▌                                | 23/94 [04:02<12:54, 10.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7014925373134329\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.55      0.74      0.63        23\n",
      "         Enc       0.83      0.68      0.75        44\n",
      "\n",
      "    accuracy                           0.70        67\n",
      "   macro avg       0.69      0.71      0.69        67\n",
      "weighted avg       0.74      0.70      0.71        67\n",
      "\n",
      "accuracy = 0.6521739130434783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.50      0.62      0.56        16\n",
      "         Enc       0.77      0.67      0.71        30\n",
      "\n",
      "    accuracy                           0.65        46\n",
      "   macro avg       0.63      0.65      0.63        46\n",
      "weighted avg       0.68      0.65      0.66        46\n",
      "\n",
      "(7,)\n",
      "[ 0.09522012  0.07132288 -0.66776672  0.67219276 -0.09984605  0.64501573\n",
      "  0.72293821]\n",
      "314409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.42857143 0.85714286 0.42857143 0.57142857 0.71428571 0.85714286\n",
      " 0.57142857 0.71428571 1.         0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 61320.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██████████▉                                | 24/94 [04:13<12:47, 10.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.52      0.61      0.56        23\n",
      "         Enc       0.79      0.72      0.76        47\n",
      "\n",
      "    accuracy                           0.69        70\n",
      "   macro avg       0.65      0.67      0.66        70\n",
      "weighted avg       0.70      0.69      0.69        70\n",
      "\n",
      "accuracy = 0.7446808510638298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.60      0.75      0.67        16\n",
      "         Enc       0.85      0.74      0.79        31\n",
      "\n",
      "    accuracy                           0.74        47\n",
      "   macro avg       0.73      0.75      0.73        47\n",
      "weighted avg       0.77      0.74      0.75        47\n",
      "\n",
      "(7,)\n",
      "[ 0.6786684   0.22982459  0.91827674  0.2730242  -0.32170594  0.71529551\n",
      " -0.84380662]\n",
      "326073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.71428571 0.71428571 0.71428571 0.57142857 0.71428571\n",
      " 0.71428571 0.71428571 0.57142857 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 85773.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|███████████▍                               | 25/94 [04:26<13:14, 11.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.48      0.70      0.57        23\n",
      "         Enc       0.81      0.64      0.71        47\n",
      "\n",
      "    accuracy                           0.66        70\n",
      "   macro avg       0.65      0.67      0.64        70\n",
      "weighted avg       0.70      0.66      0.67        70\n",
      "\n",
      "accuracy = 0.6382978723404256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.48      0.62      0.54        16\n",
      "         Enc       0.77      0.65      0.70        31\n",
      "\n",
      "    accuracy                           0.64        47\n",
      "   macro avg       0.62      0.64      0.62        47\n",
      "weighted avg       0.67      0.64      0.65        47\n",
      "\n",
      "(7,)\n",
      "[ 1.14452686  0.3268384   0.02320605  0.53317634 -0.44254989  0.62644576\n",
      "  0.76749367]\n",
      "336665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.57s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 67 testing: 46\n",
      "Enc    44\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    15\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.71428571 0.57142857 0.57142857 0.85714286 0.71428571\n",
      " 0.28571429 0.83333333 0.66666667 0.83333333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 84904.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|███████████▉                               | 26/94 [04:37<12:53, 11.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6716417910447762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.52      0.61      0.56        23\n",
      "         Enc       0.78      0.70      0.74        44\n",
      "\n",
      "    accuracy                           0.67        67\n",
      "   macro avg       0.65      0.66      0.65        67\n",
      "weighted avg       0.69      0.67      0.68        67\n",
      "\n",
      "accuracy = 0.6956521739130435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.52      0.80      0.63        15\n",
      "         Enc       0.87      0.65      0.74        31\n",
      "\n",
      "    accuracy                           0.70        46\n",
      "   macro avg       0.70      0.72      0.69        46\n",
      "weighted avg       0.76      0.70      0.71        46\n",
      "\n",
      "(7,)\n",
      "[-0.21165099  0.23241887 -0.27611178  1.10829361 -0.16564275  0.72304275\n",
      "  0.05722217]\n",
      "337021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.85714286 0.71428571 0.85714286 0.42857143 0.71428571 0.85714286\n",
      " 0.85714286 0.85714286 0.71428571 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 84054.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|████████████▎                              | 27/94 [04:48<12:30, 11.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.61      0.74      0.67        23\n",
      "         Enc       0.86      0.77      0.81        47\n",
      "\n",
      "    accuracy                           0.76        70\n",
      "   macro avg       0.73      0.75      0.74        70\n",
      "weighted avg       0.78      0.76      0.76        70\n",
      "\n",
      "accuracy = 0.6595744680851063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.50      0.50      0.50        16\n",
      "         Enc       0.74      0.74      0.74        31\n",
      "\n",
      "    accuracy                           0.66        47\n",
      "   macro avg       0.62      0.62      0.62        47\n",
      "weighted avg       0.66      0.66      0.66        47\n",
      "\n",
      "(7,)\n",
      "[ 0.08459203  0.96474477  0.75674239  0.14907358 -0.04590658  1.23579984\n",
      " -0.80606343]\n",
      "350555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.60s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.42857143 0.42857143 0.57142857 0.71428571 0.85714286\n",
      " 0.42857143 0.57142857 0.57142857 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 80504.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|████████████▊                              | 28/94 [04:59<12:22, 11.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.41      0.61      0.49        23\n",
      "         Enc       0.75      0.57      0.65        47\n",
      "\n",
      "    accuracy                           0.59        70\n",
      "   macro avg       0.58      0.59      0.57        70\n",
      "weighted avg       0.64      0.59      0.60        70\n",
      "\n",
      "accuracy = 0.5319148936170213\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.36      0.50      0.42        16\n",
      "         Enc       0.68      0.55      0.61        31\n",
      "\n",
      "    accuracy                           0.53        47\n",
      "   macro avg       0.52      0.52      0.51        47\n",
      "weighted avg       0.57      0.53      0.54        47\n",
      "\n",
      "(7,)\n",
      "[0.93571282 0.82446767 0.86081398 0.82994057 0.47816543 0.62741167\n",
      " 0.4698305 ]\n",
      "370092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.57s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 67 testing: 46\n",
      "Enc    44\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    30\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.85714286 0.71428571 0.85714286 0.57142857 0.42857143\n",
      " 0.71428571 0.66666667 0.83333333 1.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 67324.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|█████████████▎                             | 29/94 [05:13<13:12, 12.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7313432835820896\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.60      0.65      0.63        23\n",
      "         Enc       0.81      0.77      0.79        44\n",
      "\n",
      "    accuracy                           0.73        67\n",
      "   macro avg       0.70      0.71      0.71        67\n",
      "weighted avg       0.74      0.73      0.73        67\n",
      "\n",
      "accuracy = 0.7608695652173914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.65      0.69      0.67        16\n",
      "         Enc       0.83      0.80      0.81        30\n",
      "\n",
      "    accuracy                           0.76        46\n",
      "   macro avg       0.74      0.74      0.74        46\n",
      "weighted avg       0.76      0.76      0.76        46\n",
      "\n",
      "(7,)\n",
      "[ 0.3918941   0.57994544 -0.50390249  0.62807639 -0.72358056  0.53519357\n",
      " -1.2767655 ]\n",
      "385370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.85714286 0.57142857 0.42857143 0.71428571 0.57142857 0.57142857\n",
      " 1.         0.57142857 0.71428571 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 85423.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|█████████████▋                             | 30/94 [05:24<12:28, 11.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.52      0.61      0.56        23\n",
      "         Enc       0.79      0.72      0.76        47\n",
      "\n",
      "    accuracy                           0.69        70\n",
      "   macro avg       0.65      0.67      0.66        70\n",
      "weighted avg       0.70      0.69      0.69        70\n",
      "\n",
      "accuracy = 0.7021276595744681\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.56      0.56      0.56        16\n",
      "         Enc       0.77      0.77      0.77        31\n",
      "\n",
      "    accuracy                           0.70        47\n",
      "   macro avg       0.67      0.67      0.67        47\n",
      "weighted avg       0.70      0.70      0.70        47\n",
      "\n",
      "(7,)\n",
      "[-0.53925599  0.49862962  0.10387685  0.67973808 -0.29779405  0.53963863\n",
      " -0.32318268]\n",
      "386333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 1.         0.85714286 0.71428571 0.28571429 0.57142857\n",
      " 0.57142857 0.57142857 0.42857143 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 65128.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|██████████████▏                            | 31/94 [05:35<11:57, 11.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6428571428571429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.46      0.52      0.49        23\n",
      "         Enc       0.75      0.70      0.73        47\n",
      "\n",
      "    accuracy                           0.64        70\n",
      "   macro avg       0.61      0.61      0.61        70\n",
      "weighted avg       0.66      0.64      0.65        70\n",
      "\n",
      "accuracy = 0.5319148936170213\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.36      0.50      0.42        16\n",
      "         Enc       0.68      0.55      0.61        31\n",
      "\n",
      "    accuracy                           0.53        47\n",
      "   macro avg       0.52      0.52      0.51        47\n",
      "weighted avg       0.57      0.53      0.54        47\n",
      "\n",
      "(7,)\n",
      "[ 0.26694506  0.15455524  0.28764481  0.35137313 -0.58895204  0.18614541\n",
      "  0.70431159]\n",
      "396250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.28571429 0.71428571 0.42857143 0.71428571 0.85714286\n",
      " 0.57142857 0.42857143 0.85714286 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 70374.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|██████████████▋                            | 32/94 [05:46<11:43, 11.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.41      0.57      0.47        23\n",
      "         Enc       0.74      0.60      0.66        47\n",
      "\n",
      "    accuracy                           0.59        70\n",
      "   macro avg       0.57      0.58      0.57        70\n",
      "weighted avg       0.63      0.59      0.60        70\n",
      "\n",
      "accuracy = 0.6595744680851063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.50      0.62      0.56        16\n",
      "         Enc       0.78      0.68      0.72        31\n",
      "\n",
      "    accuracy                           0.66        47\n",
      "   macro avg       0.64      0.65      0.64        47\n",
      "weighted avg       0.68      0.66      0.67        47\n",
      "\n",
      "(7,)\n",
      "[-0.52259386  0.08308382 -0.09865653  0.65537825 -0.50234228  0.49973375\n",
      " -0.27213757]\n",
      "403131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.42857143 0.85714286 0.57142857 0.42857143 0.42857143\n",
      " 0.42857143 1.         0.57142857 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 49872.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███████████████                            | 33/94 [05:58<11:54, 11.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.41      0.65      0.50        23\n",
      "         Enc       0.76      0.53      0.62        47\n",
      "\n",
      "    accuracy                           0.57        70\n",
      "   macro avg       0.58      0.59      0.56        70\n",
      "weighted avg       0.64      0.57      0.58        70\n",
      "\n",
      "accuracy = 0.48936170212765956\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.32      0.44      0.37        16\n",
      "         Enc       0.64      0.52      0.57        31\n",
      "\n",
      "    accuracy                           0.49        47\n",
      "   macro avg       0.48      0.48      0.47        47\n",
      "weighted avg       0.53      0.49      0.50        47\n",
      "\n",
      "(7,)\n",
      "[ 0.26472542 -0.24925929  0.43634612  0.65076305 -0.45016919  0.22431928\n",
      " -0.42824435]\n",
      "408506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.57142857 0.85714286 0.85714286 0.28571429 0.42857143\n",
      " 0.28571429 0.71428571 0.71428571 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 85598.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███████████████▌                           | 34/94 [06:09<11:16, 11.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.41      0.61      0.49        23\n",
      "         Enc       0.75      0.57      0.65        47\n",
      "\n",
      "    accuracy                           0.59        70\n",
      "   macro avg       0.58      0.59      0.57        70\n",
      "weighted avg       0.64      0.59      0.60        70\n",
      "\n",
      "accuracy = 0.574468085106383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.42      0.69      0.52        16\n",
      "         Enc       0.76      0.52      0.62        31\n",
      "\n",
      "    accuracy                           0.57        47\n",
      "   macro avg       0.59      0.60      0.57        47\n",
      "weighted avg       0.65      0.57      0.58        47\n",
      "\n",
      "(7,)\n",
      "[-0.82119327 -0.94513793 -0.27122698  0.79149331  0.21480585 -0.65322652\n",
      " -0.63118196]\n",
      "413474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.42857143 0.14285714 0.71428571 0.28571429 0.42857143\n",
      " 0.42857143 0.71428571 0.42857143 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 62230.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|████████████████                           | 35/94 [06:20<10:58, 11.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.34      0.57      0.43        23\n",
      "         Enc       0.69      0.47      0.56        47\n",
      "\n",
      "    accuracy                           0.50        70\n",
      "   macro avg       0.51      0.52      0.49        70\n",
      "weighted avg       0.57      0.50      0.51        70\n",
      "\n",
      "accuracy = 0.6808510638297872\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.52      0.69      0.59        16\n",
      "         Enc       0.81      0.68      0.74        31\n",
      "\n",
      "    accuracy                           0.68        47\n",
      "   macro avg       0.67      0.68      0.67        47\n",
      "weighted avg       0.71      0.68      0.69        47\n",
      "\n",
      "(7,)\n",
      "[-0.30682422  0.58724504  0.04977405  0.46095948 -0.29283626  0.92417089\n",
      "  0.24715699]\n",
      "427357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.71428571 0.42857143 0.28571429 0.71428571 0.71428571\n",
      " 0.28571429 0.28571429 0.71428571 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 79137.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|████████████████▍                          | 36/94 [06:31<10:55, 11.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5428571428571428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.32      0.35      0.33        23\n",
      "         Enc       0.67      0.64      0.65        47\n",
      "\n",
      "    accuracy                           0.54        70\n",
      "   macro avg       0.49      0.49      0.49        70\n",
      "weighted avg       0.55      0.54      0.55        70\n",
      "\n",
      "accuracy = 0.723404255319149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.62      0.50      0.55        16\n",
      "         Enc       0.76      0.84      0.80        31\n",
      "\n",
      "    accuracy                           0.72        47\n",
      "   macro avg       0.69      0.67      0.68        47\n",
      "weighted avg       0.71      0.72      0.72        47\n",
      "\n",
      "(7,)\n",
      "[-0.10192557 -0.21064114 -0.47438608  0.63786523 -0.48871279  0.08414687\n",
      " -0.52937129]\n",
      "437101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.42857143 0.42857143 0.42857143 0.57142857 0.57142857\n",
      " 0.57142857 0.57142857 0.42857143 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 68759.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|████████████████▉                          | 37/94 [06:42<10:32, 11.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5142857142857142\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.31      0.39      0.35        23\n",
      "         Enc       0.66      0.57      0.61        47\n",
      "\n",
      "    accuracy                           0.51        70\n",
      "   macro avg       0.48      0.48      0.48        70\n",
      "weighted avg       0.54      0.51      0.53        70\n",
      "\n",
      "accuracy = 0.5531914893617021\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.37      0.44      0.40        16\n",
      "         Enc       0.68      0.61      0.64        31\n",
      "\n",
      "    accuracy                           0.55        47\n",
      "   macro avg       0.52      0.53      0.52        47\n",
      "weighted avg       0.57      0.55      0.56        47\n",
      "\n",
      "(7,)\n",
      "[ 0.60599479 -0.4093398   0.1663509   0.5403904  -0.26466707  0.22961697\n",
      " -0.04218669]\n",
      "439776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.42857143 0.71428571 0.57142857 0.71428571 0.85714286 0.71428571\n",
      " 0.71428571 0.42857143 0.57142857 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 44337.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|█████████████████▍                         | 38/94 [06:53<10:30, 11.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.48      0.52      0.50        23\n",
      "         Enc       0.76      0.72      0.74        47\n",
      "\n",
      "    accuracy                           0.66        70\n",
      "   macro avg       0.62      0.62      0.62        70\n",
      "weighted avg       0.67      0.66      0.66        70\n",
      "\n",
      "accuracy = 0.6595744680851063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.50      0.75      0.60        16\n",
      "         Enc       0.83      0.61      0.70        31\n",
      "\n",
      "    accuracy                           0.66        47\n",
      "   macro avg       0.66      0.68      0.65        47\n",
      "weighted avg       0.72      0.66      0.67        47\n",
      "\n",
      "(7,)\n",
      "[ 0.50022964  0.33611797 -0.63034236  0.35457585 -0.60047739 -0.10395914\n",
      "  1.04293409]\n",
      "441008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.71428571 0.42857143 0.85714286 0.42857143 0.71428571\n",
      " 0.85714286 0.85714286 0.85714286 0.28571429]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 76959.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|█████████████████▊                         | 39/94 [07:05<10:18, 11.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.50      0.70      0.58        23\n",
      "         Enc       0.82      0.66      0.73        47\n",
      "\n",
      "    accuracy                           0.67        70\n",
      "   macro avg       0.66      0.68      0.66        70\n",
      "weighted avg       0.71      0.67      0.68        70\n",
      "\n",
      "accuracy = 0.723404255319149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.64      0.44      0.52        16\n",
      "         Enc       0.75      0.87      0.81        31\n",
      "\n",
      "    accuracy                           0.72        47\n",
      "   macro avg       0.69      0.65      0.66        47\n",
      "weighted avg       0.71      0.72      0.71        47\n",
      "\n",
      "(7,)\n",
      "[ 0.11722099  0.46275344  0.37818725  0.40520516 -0.51214956  0.68952376\n",
      "  0.52307777]\n",
      "458807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.42857143 0.85714286 0.71428571 0.85714286 0.42857143\n",
      " 0.57142857 0.71428571 0.85714286 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 60349.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|██████████████████▎                        | 40/94 [07:16<10:12, 11.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.50      0.70      0.58        23\n",
      "         Enc       0.82      0.66      0.73        47\n",
      "\n",
      "    accuracy                           0.67        70\n",
      "   macro avg       0.66      0.68      0.66        70\n",
      "weighted avg       0.71      0.67      0.68        70\n",
      "\n",
      "accuracy = 0.5957446808510638\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.43      0.62      0.51        16\n",
      "         Enc       0.75      0.58      0.65        31\n",
      "\n",
      "    accuracy                           0.60        47\n",
      "   macro avg       0.59      0.60      0.58        47\n",
      "weighted avg       0.64      0.60      0.61        47\n",
      "\n",
      "(7,)\n",
      "[0.62687822 0.60846659 1.13305415 1.30272883 0.25275243 0.23627711\n",
      " 0.15694638]\n",
      "459801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.71428571 0.57142857 0.71428571 0.71428571 0.85714286\n",
      " 0.28571429 0.71428571 0.42857143 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 67216.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|██████████████████▊                        | 41/94 [07:27<09:48, 11.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.43      0.52      0.47        23\n",
      "         Enc       0.74      0.66      0.70        47\n",
      "\n",
      "    accuracy                           0.61        70\n",
      "   macro avg       0.58      0.59      0.58        70\n",
      "weighted avg       0.64      0.61      0.62        70\n",
      "\n",
      "accuracy = 0.6595744680851063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.50      0.38      0.43        16\n",
      "         Enc       0.71      0.81      0.76        31\n",
      "\n",
      "    accuracy                           0.66        47\n",
      "   macro avg       0.61      0.59      0.59        47\n",
      "weighted avg       0.64      0.66      0.65        47\n",
      "\n",
      "(7,)\n",
      "[-0.7167802   0.21616164 -0.00687245  0.96764604 -0.31867542  0.48946389\n",
      " -0.34597938]\n",
      "462345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 69 testing: 46\n",
      "Enc    46\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    30\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.57142857 0.57142857 0.42857143 0.57142857 0.71428571\n",
      " 0.42857143 0.85714286 0.71428571 0.66666667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 81127.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|███████████████████▏                       | 42/94 [07:38<09:36, 11.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6231884057971014\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.45      0.61      0.52        23\n",
      "         Enc       0.76      0.63      0.69        46\n",
      "\n",
      "    accuracy                           0.62        69\n",
      "   macro avg       0.61      0.62      0.60        69\n",
      "weighted avg       0.66      0.62      0.63        69\n",
      "\n",
      "accuracy = 0.7608695652173914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.63      0.75      0.69        16\n",
      "         Enc       0.85      0.77      0.81        30\n",
      "\n",
      "    accuracy                           0.76        46\n",
      "   macro avg       0.74      0.76      0.75        46\n",
      "weighted avg       0.78      0.76      0.76        46\n",
      "\n",
      "(7,)\n",
      "[ 0.17164612  0.12996626 -0.00674054  0.12506049 -1.27407039 -0.94034235\n",
      " -0.11105854]\n",
      "484204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.28571429 0.57142857 0.42857143 0.71428571 0.85714286\n",
      " 0.57142857 0.85714286 0.57142857 0.28571429]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 63550.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|███████████████████▋                       | 43/94 [07:49<09:27, 11.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.41      0.57      0.47        23\n",
      "         Enc       0.74      0.60      0.66        47\n",
      "\n",
      "    accuracy                           0.59        70\n",
      "   macro avg       0.57      0.58      0.57        70\n",
      "weighted avg       0.63      0.59      0.60        70\n",
      "\n",
      "accuracy = 0.6382978723404256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.48      0.62      0.54        16\n",
      "         Enc       0.77      0.65      0.70        31\n",
      "\n",
      "    accuracy                           0.64        47\n",
      "   macro avg       0.62      0.64      0.62        47\n",
      "weighted avg       0.67      0.64      0.65        47\n",
      "\n",
      "(7,)\n",
      "[-0.45070904  0.62684882 -0.03226004 -0.13884238 -0.8984541   0.24147071\n",
      "  0.08270697]\n",
      "490035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.42857143 0.71428571 0.85714286 0.57142857 0.57142857\n",
      " 0.71428571 0.71428571 0.42857143 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 78545.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████████████████████▏                      | 44/94 [08:00<09:08, 10.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.41      0.52      0.46        23\n",
      "         Enc       0.73      0.64      0.68        47\n",
      "\n",
      "    accuracy                           0.60        70\n",
      "   macro avg       0.57      0.58      0.57        70\n",
      "weighted avg       0.63      0.60      0.61        70\n",
      "\n",
      "accuracy = 0.7659574468085106\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.63      0.75      0.69        16\n",
      "         Enc       0.86      0.77      0.81        31\n",
      "\n",
      "    accuracy                           0.77        47\n",
      "   macro avg       0.74      0.76      0.75        47\n",
      "weighted avg       0.78      0.77      0.77        47\n",
      "\n",
      "(7,)\n",
      "[-0.89593427  0.64189879 -0.54204498 -0.12848684 -1.04902918 -0.10516268\n",
      " -0.93270589]\n",
      "502616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.57142857 0.71428571 1.         1.         0.42857143\n",
      " 0.57142857 0.85714286 0.57142857 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 83220.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████████████████████▌                      | 45/94 [08:12<09:18, 11.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.50      0.70      0.58        23\n",
      "         Enc       0.82      0.66      0.73        47\n",
      "\n",
      "    accuracy                           0.67        70\n",
      "   macro avg       0.66      0.68      0.66        70\n",
      "weighted avg       0.71      0.67      0.68        70\n",
      "\n",
      "accuracy = 0.6808510638297872\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.54      0.44      0.48        16\n",
      "         Enc       0.74      0.81      0.77        31\n",
      "\n",
      "    accuracy                           0.68        47\n",
      "   macro avg       0.64      0.62      0.63        47\n",
      "weighted avg       0.67      0.68      0.67        47\n",
      "\n",
      "(7,)\n",
      "[ 0.13062551  0.00874042 -0.4794799   1.2932297   0.07746339  0.4839486\n",
      "  0.18231627]\n",
      "517070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.71428571 0.42857143 0.42857143 0.85714286 0.57142857\n",
      " 0.28571429 0.42857143 0.71428571 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 84222.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|█████████████████████                      | 46/94 [08:23<09:08, 11.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.41      0.57      0.47        23\n",
      "         Enc       0.74      0.60      0.66        47\n",
      "\n",
      "    accuracy                           0.59        70\n",
      "   macro avg       0.57      0.58      0.57        70\n",
      "weighted avg       0.63      0.59      0.60        70\n",
      "\n",
      "accuracy = 0.6170212765957447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.43      0.38      0.40        16\n",
      "         Enc       0.70      0.74      0.72        31\n",
      "\n",
      "    accuracy                           0.62        47\n",
      "   macro avg       0.56      0.56      0.56        47\n",
      "weighted avg       0.61      0.62      0.61        47\n",
      "\n",
      "(7,)\n",
      "[ 0.18383834 -0.01707198 -0.26317639  0.00611243 -0.79553893 -0.02593053\n",
      " -0.62034785]\n",
      "520377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.57s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 69 testing: 46\n",
      "Enc    46\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    30\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.42857143 0.28571429 0.85714286 0.28571429 0.57142857\n",
      " 0.71428571 0.71428571 0.14285714 0.5       ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 84392.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████████████████████▌                     | 47/94 [08:34<08:47, 11.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5072463768115942\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.34      0.52      0.41        23\n",
      "         Enc       0.68      0.50      0.58        46\n",
      "\n",
      "    accuracy                           0.51        69\n",
      "   macro avg       0.51      0.51      0.49        69\n",
      "weighted avg       0.57      0.51      0.52        69\n",
      "\n",
      "accuracy = 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.33      0.44      0.38        16\n",
      "         Enc       0.64      0.53      0.58        30\n",
      "\n",
      "    accuracy                           0.50        46\n",
      "   macro avg       0.49      0.49      0.48        46\n",
      "weighted avg       0.53      0.50      0.51        46\n",
      "\n",
      "(7,)\n",
      "[ 0.39691531 -0.92722602 -0.08809236  0.22995018  0.74623587 -0.03583194\n",
      "  0.3664642 ]\n",
      "543589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 69 testing: 46\n",
      "Enc    46\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    30\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.42857143 0.28571429 0.71428571 0.42857143 0.57142857\n",
      " 0.71428571 0.28571429 0.42857143 0.66666667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|███████████████████████████████████████| 10/10 [00:00<00:00, 107271.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████████████████████▉                     | 48/94 [08:47<09:03, 11.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5072463768115942\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.31      0.39      0.35        23\n",
      "         Enc       0.65      0.57      0.60        46\n",
      "\n",
      "    accuracy                           0.51        69\n",
      "   macro avg       0.48      0.48      0.48        69\n",
      "weighted avg       0.54      0.51      0.52        69\n",
      "\n",
      "accuracy = 0.41304347826086957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.28      0.44      0.34        16\n",
      "         Enc       0.57      0.40      0.47        30\n",
      "\n",
      "    accuracy                           0.41        46\n",
      "   macro avg       0.43      0.42      0.41        46\n",
      "weighted avg       0.47      0.41      0.43        46\n",
      "\n",
      "(7,)\n",
      "[-0.59569113  0.35749301 -0.47002689 -0.52873725 -0.73151666  0.57273925\n",
      " -0.03490603]\n",
      "549994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.57142857 0.85714286 1.         0.42857143 0.28571429\n",
      " 0.57142857 0.85714286 0.85714286 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 58497.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|██████████████████████▍                    | 49/94 [08:59<08:42, 11.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.50      0.57      0.53        23\n",
      "         Enc       0.77      0.72      0.75        47\n",
      "\n",
      "    accuracy                           0.67        70\n",
      "   macro avg       0.64      0.64      0.64        70\n",
      "weighted avg       0.68      0.67      0.68        70\n",
      "\n",
      "accuracy = 0.6382978723404256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.47      0.44      0.45        16\n",
      "         Enc       0.72      0.74      0.73        31\n",
      "\n",
      "    accuracy                           0.64        47\n",
      "   macro avg       0.59      0.59      0.59        47\n",
      "weighted avg       0.63      0.64      0.64        47\n",
      "\n",
      "(7,)\n",
      "[ 0.22391583  0.46709573  0.1521634   0.46656376 -0.74366482  0.38380847\n",
      "  0.79786343]\n",
      "555537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.42857143 0.57142857 0.85714286 0.71428571 0.71428571\n",
      " 0.28571429 0.71428571 0.71428571 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 63840.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|██████████████████████▊                    | 50/94 [09:09<08:18, 11.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6428571428571429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.47      0.61      0.53        23\n",
      "         Enc       0.78      0.66      0.71        47\n",
      "\n",
      "    accuracy                           0.64        70\n",
      "   macro avg       0.62      0.63      0.62        70\n",
      "weighted avg       0.67      0.64      0.65        70\n",
      "\n",
      "accuracy = 0.7021276595744681\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.55      0.75      0.63        16\n",
      "         Enc       0.84      0.68      0.75        31\n",
      "\n",
      "    accuracy                           0.70        47\n",
      "   macro avg       0.69      0.71      0.69        47\n",
      "weighted avg       0.74      0.70      0.71        47\n",
      "\n",
      "(7,)\n",
      "[-0.06743958  0.62672153  0.14714311  0.19712902 -0.82204615  0.25256548\n",
      " -0.29359007]\n",
      "567214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.14285714 0.57142857 0.14285714 0.28571429 0.71428571\n",
      " 0.57142857 0.57142857 0.57142857 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 83551.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|███████████████████████▎                   | 51/94 [09:19<07:50, 10.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.31      0.43      0.36        23\n",
      "         Enc       0.66      0.53      0.59        47\n",
      "\n",
      "    accuracy                           0.50        70\n",
      "   macro avg       0.49      0.48      0.48        70\n",
      "weighted avg       0.54      0.50      0.51        70\n",
      "\n",
      "accuracy = 0.5531914893617021\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.40      0.62      0.49        16\n",
      "         Enc       0.73      0.52      0.60        31\n",
      "\n",
      "    accuracy                           0.55        47\n",
      "   macro avg       0.56      0.57      0.55        47\n",
      "weighted avg       0.62      0.55      0.56        47\n",
      "\n",
      "(7,)\n",
      "[0.27587777 0.4510429  0.48067592 0.80773318 0.74327673 0.7061137\n",
      " 0.04912281]\n",
      "597569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.42857143 0.57142857 0.85714286 0.57142857 0.57142857 0.57142857\n",
      " 0.57142857 0.28571429 0.42857143 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 63262.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|███████████████████████▊                   | 52/94 [09:30<07:34, 10.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5571428571428572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.33      0.35      0.34        23\n",
      "         Enc       0.67      0.66      0.67        47\n",
      "\n",
      "    accuracy                           0.56        70\n",
      "   macro avg       0.50      0.50      0.50        70\n",
      "weighted avg       0.56      0.56      0.56        70\n",
      "\n",
      "accuracy = 0.7659574468085106\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.67      0.62      0.65        16\n",
      "         Enc       0.81      0.84      0.83        31\n",
      "\n",
      "    accuracy                           0.77        47\n",
      "   macro avg       0.74      0.73      0.74        47\n",
      "weighted avg       0.76      0.77      0.76        47\n",
      "\n",
      "(7,)\n",
      "[-0.44973886 -0.05529113 -0.24995945  0.32952196 -0.27417465  0.08824073\n",
      " -0.65168473]\n",
      "619278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.57142857 0.42857143 0.57142857 0.85714286 0.57142857\n",
      " 0.85714286 0.71428571 0.71428571 0.28571429]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 83551.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|████████████████████████▏                  | 53/94 [09:41<07:23, 10.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.45      0.74      0.56        23\n",
      "         Enc       0.81      0.55      0.66        47\n",
      "\n",
      "    accuracy                           0.61        70\n",
      "   macro avg       0.63      0.65      0.61        70\n",
      "weighted avg       0.69      0.61      0.63        70\n",
      "\n",
      "accuracy = 0.6382978723404256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.47      0.56      0.51        16\n",
      "         Enc       0.75      0.68      0.71        31\n",
      "\n",
      "    accuracy                           0.64        47\n",
      "   macro avg       0.61      0.62      0.61        47\n",
      "weighted avg       0.66      0.64      0.64        47\n",
      "\n",
      "(7,)\n",
      "[-0.08966225  0.59207345  0.01912084  0.38357386 -0.81532885  0.04631844\n",
      " -0.42411865]\n",
      "628299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 1.         0.85714286 0.71428571 0.42857143 0.42857143\n",
      " 0.28571429 0.71428571 0.85714286 0.14285714]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 74898.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|████████████████████████▋                  | 54/94 [09:51<07:09, 10.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.41      0.52      0.46        23\n",
      "         Enc       0.73      0.64      0.68        47\n",
      "\n",
      "    accuracy                           0.60        70\n",
      "   macro avg       0.57      0.58      0.57        70\n",
      "weighted avg       0.63      0.60      0.61        70\n",
      "\n",
      "accuracy = 0.6382978723404256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.48      0.62      0.54        16\n",
      "         Enc       0.77      0.65      0.70        31\n",
      "\n",
      "    accuracy                           0.64        47\n",
      "   macro avg       0.62      0.64      0.62        47\n",
      "weighted avg       0.67      0.64      0.65        47\n",
      "\n",
      "(7,)\n",
      "[-0.62292761  0.29154598 -0.08966119  0.88395622  0.26242285 -0.11835691\n",
      " -0.06080497]\n",
      "630120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.57142857 0.57142857 0.42857143 0.42857143 0.28571429\n",
      " 0.57142857 0.28571429 0.28571429 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 80043.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████████████████████████▏                 | 55/94 [10:03<07:07, 10.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.28      0.39      0.33        23\n",
      "         Enc       0.63      0.51      0.56        47\n",
      "\n",
      "    accuracy                           0.47        70\n",
      "   macro avg       0.46      0.45      0.45        70\n",
      "weighted avg       0.52      0.47      0.49        70\n",
      "\n",
      "accuracy = 0.6382978723404256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.47      0.56      0.51        16\n",
      "         Enc       0.75      0.68      0.71        31\n",
      "\n",
      "    accuracy                           0.64        47\n",
      "   macro avg       0.61      0.62      0.61        47\n",
      "weighted avg       0.66      0.64      0.64        47\n",
      "\n",
      "(7,)\n",
      "[-0.25166357  0.58754628  0.30626248  0.64070936  0.00300547  0.43068867\n",
      "  0.04789958]\n",
      "652850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.71428571 0.71428571 0.42857143 0.71428571 0.71428571\n",
      " 0.85714286 0.57142857 0.57142857 0.14285714]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 81760.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████████████████████████▌                 | 56/94 [10:16<07:28, 11.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.43      0.52      0.47        23\n",
      "         Enc       0.74      0.66      0.70        47\n",
      "\n",
      "    accuracy                           0.61        70\n",
      "   macro avg       0.58      0.59      0.58        70\n",
      "weighted avg       0.64      0.61      0.62        70\n",
      "\n",
      "accuracy = 0.6170212765957447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.45      0.56      0.50        16\n",
      "         Enc       0.74      0.65      0.69        31\n",
      "\n",
      "    accuracy                           0.62        47\n",
      "   macro avg       0.60      0.60      0.59        47\n",
      "weighted avg       0.64      0.62      0.63        47\n",
      "\n",
      "(7,)\n",
      "[-0.13031795 -0.39219595 -0.67420094  0.95308514  0.3479744  -0.00980674\n",
      "  0.38351338]\n",
      "658178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.57142857 0.85714286 0.71428571 0.85714286 0.85714286\n",
      " 0.71428571 0.71428571 0.57142857 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 89813.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████████████████████████                 | 57/94 [10:27<07:06, 11.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.55      0.74      0.63        23\n",
      "         Enc       0.85      0.70      0.77        47\n",
      "\n",
      "    accuracy                           0.71        70\n",
      "   macro avg       0.70      0.72      0.70        70\n",
      "weighted avg       0.75      0.71      0.72        70\n",
      "\n",
      "accuracy = 0.6808510638297872\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.52      0.69      0.59        16\n",
      "         Enc       0.81      0.68      0.74        31\n",
      "\n",
      "    accuracy                           0.68        47\n",
      "   macro avg       0.67      0.68      0.67        47\n",
      "weighted avg       0.71      0.68      0.69        47\n",
      "\n",
      "(7,)\n",
      "[ 0.09256818  0.48722943  0.39365571  0.56581429 -0.29744177  1.32821842\n",
      "  0.16818479]\n",
      "659068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.85714286 0.57142857 0.71428571 0.85714286 0.42857143\n",
      " 0.85714286 0.71428571 0.28571429 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 82565.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████████████████████████▌                | 58/94 [10:39<06:52, 11.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.50      0.65      0.57        23\n",
      "         Enc       0.80      0.68      0.74        47\n",
      "\n",
      "    accuracy                           0.67        70\n",
      "   macro avg       0.65      0.67      0.65        70\n",
      "weighted avg       0.70      0.67      0.68        70\n",
      "\n",
      "accuracy = 0.5957446808510638\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.42      0.50      0.46        16\n",
      "         Enc       0.71      0.65      0.68        31\n",
      "\n",
      "    accuracy                           0.60        47\n",
      "   macro avg       0.57      0.57      0.57        47\n",
      "weighted avg       0.61      0.60      0.60        47\n",
      "\n",
      "(7,)\n",
      "[ 0.28011724  0.50785658 -0.0828913   0.40776242 -1.1501332   0.39376554\n",
      " -0.52317324]\n",
      "668786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.56s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 67 testing: 46\n",
      "Enc    44\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    30\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.42857143 0.42857143 0.71428571 0.14285714 0.71428571 0.28571429\n",
      " 0.42857143 0.33333333 0.5        0.83333333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 85948.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████████████████████████▉                | 59/94 [10:49<06:33, 11.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.47761194029850745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.31      0.43      0.36        23\n",
      "         Enc       0.63      0.50      0.56        44\n",
      "\n",
      "    accuracy                           0.48        67\n",
      "   macro avg       0.47      0.47      0.46        67\n",
      "weighted avg       0.52      0.48      0.49        67\n",
      "\n",
      "accuracy = 0.4782608695652174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.33      0.50      0.40        16\n",
      "         Enc       0.64      0.47      0.54        30\n",
      "\n",
      "    accuracy                           0.48        46\n",
      "   macro avg       0.48      0.48      0.47        46\n",
      "weighted avg       0.53      0.48      0.49        46\n",
      "\n",
      "(7,)\n",
      "[-0.11617392 -0.08356109 -0.28549811  0.11384359  0.98162939 -0.37376714\n",
      "  0.63205839]\n",
      "677561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[1.         0.71428571 0.85714286 0.85714286 0.71428571 0.42857143\n",
      " 0.42857143 0.71428571 0.28571429 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 61410.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|███████████████████████████▍               | 60/94 [11:01<06:29, 11.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6428571428571429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.47      0.65      0.55        23\n",
      "         Enc       0.79      0.64      0.71        47\n",
      "\n",
      "    accuracy                           0.64        70\n",
      "   macro avg       0.63      0.65      0.63        70\n",
      "weighted avg       0.68      0.64      0.65        70\n",
      "\n",
      "accuracy = 0.5531914893617021\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.40      0.62      0.49        16\n",
      "         Enc       0.73      0.52      0.60        31\n",
      "\n",
      "    accuracy                           0.55        47\n",
      "   macro avg       0.56      0.57      0.55        47\n",
      "weighted avg       0.62      0.55      0.56        47\n",
      "\n",
      "(7,)\n",
      "[ 0.96017879  0.0421915  -0.72338658  0.39344375  0.09662348  0.78306217\n",
      "  0.95721481]\n",
      "711830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.71428571 0.42857143 0.14285714 0.85714286 0.57142857\n",
      " 0.85714286 0.71428571 0.57142857 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 80350.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|███████████████████████████▉               | 61/94 [11:12<06:13, 11.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6285714285714286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.46      0.70      0.55        23\n",
      "         Enc       0.80      0.60      0.68        47\n",
      "\n",
      "    accuracy                           0.63        70\n",
      "   macro avg       0.63      0.65      0.62        70\n",
      "weighted avg       0.69      0.63      0.64        70\n",
      "\n",
      "accuracy = 0.6382978723404256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.48      0.62      0.54        16\n",
      "         Enc       0.77      0.65      0.70        31\n",
      "\n",
      "    accuracy                           0.64        47\n",
      "   macro avg       0.62      0.64      0.62        47\n",
      "weighted avg       0.67      0.64      0.65        47\n",
      "\n",
      "(7,)\n",
      "[0.42291238 0.9784965  0.7709539  1.21743448 0.31521971 0.5914458\n",
      " 0.02566468]\n",
      "729722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.85714286 0.71428571 0.85714286 0.85714286 0.57142857 0.85714286\n",
      " 0.71428571 0.28571429 0.85714286 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 79891.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|████████████████████████████▎              | 62/94 [11:26<06:28, 12.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.54      0.83      0.66        23\n",
      "         Enc       0.89      0.66      0.76        47\n",
      "\n",
      "    accuracy                           0.71        70\n",
      "   macro avg       0.71      0.74      0.71        70\n",
      "weighted avg       0.77      0.71      0.72        70\n",
      "\n",
      "accuracy = 0.6170212765957447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.46      0.81      0.59        16\n",
      "         Enc       0.84      0.52      0.64        31\n",
      "\n",
      "    accuracy                           0.62        47\n",
      "   macro avg       0.65      0.66      0.62        47\n",
      "weighted avg       0.71      0.62      0.62        47\n",
      "\n",
      "(7,)\n",
      "[-0.76986315  0.33368728 -0.12799769  1.17256933  0.49614681  0.21598865\n",
      " -0.84652155]\n",
      "739694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.42857143 0.57142857 0.42857143 0.42857143 0.57142857 0.57142857\n",
      " 0.57142857 0.42857143 0.42857143 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 63358.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|████████████████████████████▊              | 63/94 [11:37<06:02, 11.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.30      0.39      0.34        23\n",
      "         Enc       0.65      0.55      0.60        47\n",
      "\n",
      "    accuracy                           0.50        70\n",
      "   macro avg       0.47      0.47      0.47        70\n",
      "weighted avg       0.54      0.50      0.51        70\n",
      "\n",
      "accuracy = 0.6808510638297872\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.52      0.69      0.59        16\n",
      "         Enc       0.81      0.68      0.74        31\n",
      "\n",
      "    accuracy                           0.68        47\n",
      "   macro avg       0.67      0.68      0.67        47\n",
      "weighted avg       0.71      0.68      0.69        47\n",
      "\n",
      "(7,)\n",
      "[ 0.61235648  0.18414463  0.53099011  0.0832347  -0.02246383 -0.11837295\n",
      " -0.56945821]\n",
      "748676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.42857143 0.28571429 0.71428571 0.28571429 0.57142857 0.42857143\n",
      " 0.57142857 0.28571429 0.85714286 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 53430.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|█████████████████████████████▎             | 64/94 [11:48<05:43, 11.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.32      0.52      0.40        23\n",
      "         Enc       0.67      0.47      0.55        47\n",
      "\n",
      "    accuracy                           0.49        70\n",
      "   macro avg       0.50      0.49      0.48        70\n",
      "weighted avg       0.55      0.49      0.50        70\n",
      "\n",
      "accuracy = 0.6382978723404256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.48      0.62      0.54        16\n",
      "         Enc       0.77      0.65      0.70        31\n",
      "\n",
      "    accuracy                           0.64        47\n",
      "   macro avg       0.62      0.64      0.62        47\n",
      "weighted avg       0.67      0.64      0.65        47\n",
      "\n",
      "(7,)\n",
      "[-0.58949697  0.22047829  0.3292294   1.04657909  0.25607694 -0.02636873\n",
      "  0.14270081]\n",
      "763590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.57s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 67 testing: 46\n",
      "Enc    44\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    15\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.71428571 1.         0.71428571 0.85714286 0.71428571\n",
      " 0.85714286 1.         0.66666667 0.66666667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 56527.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|█████████████████████████████▋             | 65/94 [11:59<05:27, 11.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7761194029850746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.62      0.91      0.74        23\n",
      "         Enc       0.94      0.70      0.81        44\n",
      "\n",
      "    accuracy                           0.78        67\n",
      "   macro avg       0.78      0.81      0.77        67\n",
      "weighted avg       0.83      0.78      0.78        67\n",
      "\n",
      "accuracy = 0.5869565217391305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.42      0.67      0.51        15\n",
      "         Enc       0.77      0.55      0.64        31\n",
      "\n",
      "    accuracy                           0.59        46\n",
      "   macro avg       0.59      0.61      0.58        46\n",
      "weighted avg       0.66      0.59      0.60        46\n",
      "\n",
      "(7,)\n",
      "[ 0.29821875  0.02713267  0.45222908  1.18584328  0.78437895 -1.15545032\n",
      "  0.2356009 ]\n",
      "778749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.60s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.42857143 0.57142857 0.71428571 0.85714286 0.28571429\n",
      " 0.57142857 0.57142857 0.42857143 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|███████████████████████████████████████| 10/10 [00:00<00:00, 107546.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████████████████████████████▏            | 66/94 [12:11<05:20, 11.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5428571428571428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.38      0.61      0.47        23\n",
      "         Enc       0.73      0.51      0.60        47\n",
      "\n",
      "    accuracy                           0.54        70\n",
      "   macro avg       0.55      0.56      0.53        70\n",
      "weighted avg       0.61      0.54      0.56        70\n",
      "\n",
      "accuracy = 0.48936170212765956\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.33      0.50      0.40        16\n",
      "         Enc       0.65      0.48      0.56        31\n",
      "\n",
      "    accuracy                           0.49        47\n",
      "   macro avg       0.49      0.49      0.48        47\n",
      "weighted avg       0.54      0.49      0.50        47\n",
      "\n",
      "(7,)\n",
      "[ 0.01823252  1.02905987  0.84784362  1.01262079  0.65165221  0.09316059\n",
      " -0.143448  ]\n",
      "783781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.85714286 0.85714286 0.42857143 0.71428571 0.71428571\n",
      " 0.85714286 0.28571429 0.71428571 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 87563.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|██████████████████████████████▋            | 67/94 [12:22<05:06, 11.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.52      0.70      0.59        23\n",
      "         Enc       0.82      0.68      0.74        47\n",
      "\n",
      "    accuracy                           0.69        70\n",
      "   macro avg       0.67      0.69      0.67        70\n",
      "weighted avg       0.72      0.69      0.69        70\n",
      "\n",
      "accuracy = 0.5106382978723404\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.35      0.50      0.41        16\n",
      "         Enc       0.67      0.52      0.58        31\n",
      "\n",
      "    accuracy                           0.51        47\n",
      "   macro avg       0.51      0.51      0.50        47\n",
      "weighted avg       0.56      0.51      0.52        47\n",
      "\n",
      "(7,)\n",
      "[ 1.11221394  0.45380033  0.87808912  0.70579388 -0.24357737  0.78633698\n",
      "  0.93719119]\n",
      "785217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.14285714 0.42857143 0.57142857 0.42857143 0.28571429\n",
      " 0.71428571 0.28571429 0.85714286 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 64035.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████████████████████████████            | 68/94 [12:32<04:49, 11.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.28      0.35      0.31        23\n",
      "         Enc       0.63      0.55      0.59        47\n",
      "\n",
      "    accuracy                           0.49        70\n",
      "   macro avg       0.46      0.45      0.45        70\n",
      "weighted avg       0.52      0.49      0.50        70\n",
      "\n",
      "accuracy = 0.5319148936170213\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.41      0.88      0.56        16\n",
      "         Enc       0.85      0.35      0.50        31\n",
      "\n",
      "    accuracy                           0.53        47\n",
      "   macro avg       0.63      0.61      0.53        47\n",
      "weighted avg       0.70      0.53      0.52        47\n",
      "\n",
      "(7,)\n",
      "[-0.59363746 -0.11657224 -0.17712154  0.27364823 -0.87800071  0.87985593\n",
      "  0.05715197]\n",
      "785245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.57142857 1.         0.42857143 0.71428571 0.71428571\n",
      " 1.         0.71428571 0.71428571 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|███████████████████████████████████████| 10/10 [00:00<00:00, 103307.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████████████████████████████▌           | 69/94 [12:43<04:36, 11.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7428571428571429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.60      0.65      0.63        23\n",
      "         Enc       0.82      0.79      0.80        47\n",
      "\n",
      "    accuracy                           0.74        70\n",
      "   macro avg       0.71      0.72      0.71        70\n",
      "weighted avg       0.75      0.74      0.75        70\n",
      "\n",
      "accuracy = 0.7872340425531915\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.69      0.69      0.69        16\n",
      "         Enc       0.84      0.84      0.84        31\n",
      "\n",
      "    accuracy                           0.79        47\n",
      "   macro avg       0.76      0.76      0.76        47\n",
      "weighted avg       0.79      0.79      0.79        47\n",
      "\n",
      "(7,)\n",
      "[ 0.27600293  0.27513403 -0.20296961  0.53975771 -0.78871924  0.16818531\n",
      " -0.90331678]\n",
      "804743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.57142857 0.71428571 0.42857143 0.71428571 0.85714286\n",
      " 0.57142857 0.71428571 0.71428571 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 65433.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|████████████████████████████████           | 70/94 [12:54<04:23, 10.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6428571428571429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.47      0.65      0.55        23\n",
      "         Enc       0.79      0.64      0.71        47\n",
      "\n",
      "    accuracy                           0.64        70\n",
      "   macro avg       0.63      0.65      0.63        70\n",
      "weighted avg       0.68      0.64      0.65        70\n",
      "\n",
      "accuracy = 0.723404255319149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.59      0.62      0.61        16\n",
      "         Enc       0.80      0.77      0.79        31\n",
      "\n",
      "    accuracy                           0.72        47\n",
      "   macro avg       0.69      0.70      0.70        47\n",
      "weighted avg       0.73      0.72      0.73        47\n",
      "\n",
      "(7,)\n",
      "[ 0.58643412  0.74740932 -0.51417022  0.48107524 -0.45762281  0.56892063\n",
      "  0.59478182]\n",
      "845675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.85714286 0.42857143 0.57142857 0.71428571 0.71428571 0.71428571\n",
      " 0.71428571 0.71428571 0.85714286 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 87563.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|████████████████████████████████▍          | 71/94 [13:06<04:18, 11.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.55      0.74      0.63        23\n",
      "         Enc       0.85      0.70      0.77        47\n",
      "\n",
      "    accuracy                           0.71        70\n",
      "   macro avg       0.70      0.72      0.70        70\n",
      "weighted avg       0.75      0.71      0.72        70\n",
      "\n",
      "accuracy = 0.8085106382978723\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.68      0.81      0.74        16\n",
      "         Enc       0.89      0.81      0.85        31\n",
      "\n",
      "    accuracy                           0.81        47\n",
      "   macro avg       0.79      0.81      0.80        47\n",
      "weighted avg       0.82      0.81      0.81        47\n",
      "\n",
      "(7,)\n",
      "[-0.45311189  0.4231469  -0.44890552  0.81703164 -0.63090539  0.61086362\n",
      "  0.16309208]\n",
      "866812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.57142857 0.42857143 0.71428571 0.28571429 0.14285714\n",
      " 0.42857143 0.57142857 0.28571429 0.28571429]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 89430.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|████████████████████████████████▉          | 72/94 [13:18<04:15, 11.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44285714285714284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.29      0.48      0.36        23\n",
      "         Enc       0.62      0.43      0.51        47\n",
      "\n",
      "    accuracy                           0.44        70\n",
      "   macro avg       0.46      0.45      0.43        70\n",
      "weighted avg       0.51      0.44      0.46        70\n",
      "\n",
      "accuracy = 0.46808510638297873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.35      0.69      0.47        16\n",
      "         Enc       0.69      0.35      0.47        31\n",
      "\n",
      "    accuracy                           0.47        47\n",
      "   macro avg       0.52      0.52      0.47        47\n",
      "weighted avg       0.57      0.47      0.47        47\n",
      "\n",
      "(7,)\n",
      "[-0.41489922 -0.01383391  0.13736936  0.63356309 -0.13911547  0.32492886\n",
      " -0.00366859]\n",
      "878354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.42857143 0.71428571 0.71428571 0.57142857 0.71428571 0.42857143\n",
      " 0.28571429 0.57142857 0.42857143 0.28571429]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 97090.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|█████████████████████████████████▍         | 73/94 [13:29<03:59, 11.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5142857142857142\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.33      0.48      0.39        23\n",
      "         Enc       0.68      0.53      0.60        47\n",
      "\n",
      "    accuracy                           0.51        70\n",
      "   macro avg       0.50      0.51      0.49        70\n",
      "weighted avg       0.56      0.51      0.53        70\n",
      "\n",
      "accuracy = 0.6382978723404256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.47      0.44      0.45        16\n",
      "         Enc       0.72      0.74      0.73        31\n",
      "\n",
      "    accuracy                           0.64        47\n",
      "   macro avg       0.59      0.59      0.59        47\n",
      "weighted avg       0.63      0.64      0.64        47\n",
      "\n",
      "(7,)\n",
      "[-0.25471099 -0.483686   -0.4622741  -0.05239029 -0.14891591  0.13321083\n",
      " -0.64214492]\n",
      "884343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.28571429 0.57142857 0.28571429 0.42857143 0.42857143 0.28571429\n",
      " 0.42857143 0.42857143 0.28571429 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 77101.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|█████████████████████████████████▊         | 74/94 [13:42<03:55, 11.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.19      0.26      0.22        23\n",
      "         Enc       0.56      0.47      0.51        47\n",
      "\n",
      "    accuracy                           0.40        70\n",
      "   macro avg       0.38      0.36      0.37        70\n",
      "weighted avg       0.44      0.40      0.42        70\n",
      "\n",
      "accuracy = 0.6382978723404256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.47      0.44      0.45        16\n",
      "         Enc       0.72      0.74      0.73        31\n",
      "\n",
      "    accuracy                           0.64        47\n",
      "   macro avg       0.59      0.59      0.59        47\n",
      "weighted avg       0.63      0.64      0.64        47\n",
      "\n",
      "(7,)\n",
      "[ 0.07433931 -0.28666164  0.00568031  0.02239179 -0.45941423  0.36639109\n",
      " -0.00353473]\n",
      "886007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.57142857 0.42857143 0.57142857 0.57142857 0.57142857\n",
      " 0.42857143 0.57142857 0.28571429 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 60090.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████████▎        | 75/94 [13:53<03:40, 11.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5142857142857142\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.32      0.43      0.37        23\n",
      "         Enc       0.67      0.55      0.60        47\n",
      "\n",
      "    accuracy                           0.51        70\n",
      "   macro avg       0.49      0.49      0.49        70\n",
      "weighted avg       0.55      0.51      0.53        70\n",
      "\n",
      "accuracy = 0.574468085106383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.42      0.69      0.52        16\n",
      "         Enc       0.76      0.52      0.62        31\n",
      "\n",
      "    accuracy                           0.57        47\n",
      "   macro avg       0.59      0.60      0.57        47\n",
      "weighted avg       0.65      0.57      0.58        47\n",
      "\n",
      "(7,)\n",
      "[ 0.00132869  0.073086    0.08431042  0.34046664 -0.60930607  0.71020255\n",
      "  0.14181898]\n",
      "893978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.60s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.42857143 0.85714286 0.71428571 0.57142857 0.71428571 0.42857143\n",
      " 0.85714286 0.71428571 0.85714286 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 66052.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████████▊        | 76/94 [14:04<03:27, 11.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.54      0.65      0.59        23\n",
      "         Enc       0.81      0.72      0.76        47\n",
      "\n",
      "    accuracy                           0.70        70\n",
      "   macro avg       0.67      0.69      0.68        70\n",
      "weighted avg       0.72      0.70      0.71        70\n",
      "\n",
      "accuracy = 0.3829787234042553\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.24      0.38      0.29        16\n",
      "         Enc       0.55      0.39      0.45        31\n",
      "\n",
      "    accuracy                           0.38        47\n",
      "   macro avg       0.39      0.38      0.37        47\n",
      "weighted avg       0.44      0.38      0.40        47\n",
      "\n",
      "(7,)\n",
      "[-0.22107231 -0.2835425  -0.03420519  0.69749835  0.16888495  0.72287678\n",
      " -0.09397755]\n",
      "901551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 69 testing: 46\n",
      "Enc    46\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    30\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.85714286 0.85714286 0.57142857 0.57142857 0.71428571 0.71428571\n",
      " 0.71428571 1.         0.42857143 1.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 74631.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|███████████████████████████████████▏       | 77/94 [14:18<03:26, 12.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7391304347826086\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.58      0.78      0.67        23\n",
      "         Enc       0.87      0.72      0.79        46\n",
      "\n",
      "    accuracy                           0.74        69\n",
      "   macro avg       0.72      0.75      0.73        69\n",
      "weighted avg       0.77      0.74      0.75        69\n",
      "\n",
      "accuracy = 0.6086956521739131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.45      0.62      0.53        16\n",
      "         Enc       0.75      0.60      0.67        30\n",
      "\n",
      "    accuracy                           0.61        46\n",
      "   macro avg       0.60      0.61      0.60        46\n",
      "weighted avg       0.65      0.61      0.62        46\n",
      "\n",
      "(7,)\n",
      "[ 0.99859283  0.05336656  0.83307396  0.58704957 -0.1353054   0.74963399\n",
      " -0.2902195 ]\n",
      "906145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.85714286 0.57142857 0.71428571 0.57142857 0.42857143 0.85714286\n",
      " 0.57142857 0.28571429 0.85714286 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 75846.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|███████████████████████████████████▋       | 78/94 [14:30<03:14, 12.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.45      0.74      0.56        23\n",
      "         Enc       0.81      0.55      0.66        47\n",
      "\n",
      "    accuracy                           0.61        70\n",
      "   macro avg       0.63      0.65      0.61        70\n",
      "weighted avg       0.69      0.61      0.63        70\n",
      "\n",
      "accuracy = 0.6170212765957447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.45      0.62      0.53        16\n",
      "         Enc       0.76      0.61      0.68        31\n",
      "\n",
      "    accuracy                           0.62        47\n",
      "   macro avg       0.61      0.62      0.60        47\n",
      "weighted avg       0.66      0.62      0.63        47\n",
      "\n",
      "(7,)\n",
      "[-0.04518806  0.29429593  0.13276427  0.39173818 -0.50337872  1.30096494\n",
      " -0.20750028]\n",
      "914042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.85714286 0.85714286 0.71428571 0.71428571 0.71428571\n",
      " 0.71428571 0.71428571 0.42857143 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 63840.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████████████████████████████████▏      | 79/94 [14:43<03:03, 12.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.52      0.65      0.58        23\n",
      "         Enc       0.80      0.70      0.75        47\n",
      "\n",
      "    accuracy                           0.69        70\n",
      "   macro avg       0.66      0.68      0.66        70\n",
      "weighted avg       0.71      0.69      0.69        70\n",
      "\n",
      "accuracy = 0.723404255319149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.58      0.69      0.63        16\n",
      "         Enc       0.82      0.74      0.78        31\n",
      "\n",
      "    accuracy                           0.72        47\n",
      "   macro avg       0.70      0.71      0.70        47\n",
      "weighted avg       0.74      0.72      0.73        47\n",
      "\n",
      "(7,)\n",
      "[1.40589793 0.58057436 0.66357412 0.19040124 0.09876825 1.04112331\n",
      " 0.36679241]\n",
      "915022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.71428571 0.85714286 0.57142857 0.71428571 0.85714286\n",
      " 0.57142857 0.57142857 0.57142857 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 83551.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████████████████████████████████▌      | 80/94 [14:54<02:46, 11.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.48      0.61      0.54        23\n",
      "         Enc       0.78      0.68      0.73        47\n",
      "\n",
      "    accuracy                           0.66        70\n",
      "   macro avg       0.63      0.64      0.63        70\n",
      "weighted avg       0.68      0.66      0.67        70\n",
      "\n",
      "accuracy = 0.5957446808510638\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.41      0.44      0.42        16\n",
      "         Enc       0.70      0.68      0.69        31\n",
      "\n",
      "    accuracy                           0.60        47\n",
      "   macro avg       0.56      0.56      0.56        47\n",
      "weighted avg       0.60      0.60      0.60        47\n",
      "\n",
      "(7,)\n",
      "[-0.13420587  0.34465497  0.05828612  1.04943226 -0.41759352  0.44431732\n",
      "  0.06452887]\n",
      "920577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.71428571 0.71428571 0.57142857 0.71428571 1.         0.57142857\n",
      " 0.71428571 0.71428571 0.71428571 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 87199.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|█████████████████████████████████████      | 81/94 [15:06<02:36, 12.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7285714285714285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.57      0.74      0.64        23\n",
      "         Enc       0.85      0.72      0.78        47\n",
      "\n",
      "    accuracy                           0.73        70\n",
      "   macro avg       0.71      0.73      0.71        70\n",
      "weighted avg       0.76      0.73      0.74        70\n",
      "\n",
      "accuracy = 0.7021276595744681\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.57      0.50      0.53        16\n",
      "         Enc       0.76      0.81      0.78        31\n",
      "\n",
      "    accuracy                           0.70        47\n",
      "   macro avg       0.66      0.65      0.66        47\n",
      "weighted avg       0.69      0.70      0.70        47\n",
      "\n",
      "(7,)\n",
      "[ 0.22787329  0.01430206  0.92568364  0.60793634  0.45283434  1.37541569\n",
      " -0.08034842]\n",
      "932933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.85714286 0.85714286 0.85714286 1.         0.57142857\n",
      " 1.         0.71428571 0.57142857 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 67324.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|█████████████████████████████████████▌     | 82/94 [15:18<02:22, 11.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.64      0.78      0.71        23\n",
      "         Enc       0.88      0.79      0.83        47\n",
      "\n",
      "    accuracy                           0.79        70\n",
      "   macro avg       0.76      0.78      0.77        70\n",
      "weighted avg       0.80      0.79      0.79        70\n",
      "\n",
      "accuracy = 0.7659574468085106\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.63      0.75      0.69        16\n",
      "         Enc       0.86      0.77      0.81        31\n",
      "\n",
      "    accuracy                           0.77        47\n",
      "   macro avg       0.74      0.76      0.75        47\n",
      "weighted avg       0.78      0.77      0.77        47\n",
      "\n",
      "(7,)\n",
      "[-0.13912503 -0.10419402  0.19192044  1.24869308 -0.13196497  0.23783227\n",
      " -1.32880484]\n",
      "936730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 69 testing: 46\n",
      "Enc    46\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    30\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.28571429 0.28571429 0.28571429 0.28571429 0.28571429 0.28571429\n",
      " 0.42857143 0.42857143 0.42857143 0.5       ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 39568.91it/s]\u001b[A\u001b[A\n",
      "/home/fnadeau/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/fnadeau/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/fnadeau/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 88%|█████████████████████████████████████▉     | 83/94 [15:29<02:09, 11.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34782608695652173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.33      0.96      0.49        23\n",
      "         Enc       0.67      0.04      0.08        46\n",
      "\n",
      "    accuracy                           0.35        69\n",
      "   macro avg       0.50      0.50      0.29        69\n",
      "weighted avg       0.56      0.35      0.22        69\n",
      "\n",
      "accuracy = 0.34782608695652173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.35      1.00      0.52        16\n",
      "         Enc       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           0.35        46\n",
      "   macro avg       0.17      0.50      0.26        46\n",
      "weighted avg       0.12      0.35      0.18        46\n",
      "\n",
      "(7,)\n",
      "[ 0.09931176  0.08741733  0.06202755  0.72017478 -0.58453232  0.27366077\n",
      "  0.1545981 ]\n",
      "938001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.71428571 0.57142857 0.71428571 0.42857143 0.57142857\n",
      " 1.         0.42857143 0.71428571 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 85423.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|██████████████████████████████████████▍    | 84/94 [15:41<01:58, 11.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6428571428571429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.46      0.57      0.51        23\n",
      "         Enc       0.76      0.68      0.72        47\n",
      "\n",
      "    accuracy                           0.64        70\n",
      "   macro avg       0.61      0.62      0.61        70\n",
      "weighted avg       0.66      0.64      0.65        70\n",
      "\n",
      "accuracy = 0.574468085106383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.43      0.75      0.55        16\n",
      "         Enc       0.79      0.48      0.60        31\n",
      "\n",
      "    accuracy                           0.57        47\n",
      "   macro avg       0.61      0.62      0.57        47\n",
      "weighted avg       0.67      0.57      0.58        47\n",
      "\n",
      "(7,)\n",
      "[-0.66297038 -0.36680494 -0.55465598  0.28771901 -1.00692885 -0.38110816\n",
      " -0.95728287]\n",
      "955548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.71428571 0.57142857 0.71428571 0.85714286 0.85714286\n",
      " 0.57142857 0.71428571 0.85714286 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 57377.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|██████████████████████████████████████▉    | 85/94 [15:52<01:44, 11.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7285714285714285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.56      0.78      0.65        23\n",
      "         Enc       0.87      0.70      0.78        47\n",
      "\n",
      "    accuracy                           0.73        70\n",
      "   macro avg       0.72      0.74      0.72        70\n",
      "weighted avg       0.77      0.73      0.74        70\n",
      "\n",
      "accuracy = 0.6595744680851063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.50      0.81      0.62        16\n",
      "         Enc       0.86      0.58      0.69        31\n",
      "\n",
      "    accuracy                           0.66        47\n",
      "   macro avg       0.68      0.70      0.66        47\n",
      "weighted avg       0.74      0.66      0.67        47\n",
      "\n",
      "(7,)\n",
      "[ 0.3230123   0.78561656 -0.54059341  0.12595896 -0.0264986   0.05497248\n",
      "  1.73217261]\n",
      "956049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.42857143 0.28571429 0.71428571 0.42857143 0.57142857 0.28571429\n",
      " 0.57142857 0.57142857 0.42857143 0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 72944.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|███████████████████████████████████████▎   | 86/94 [16:03<01:32, 11.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.18      0.17      0.18        23\n",
      "         Enc       0.60      0.62      0.61        47\n",
      "\n",
      "    accuracy                           0.47        70\n",
      "   macro avg       0.39      0.40      0.39        70\n",
      "weighted avg       0.47      0.47      0.47        70\n",
      "\n",
      "accuracy = 0.574468085106383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.36      0.31      0.33        16\n",
      "         Enc       0.67      0.71      0.69        31\n",
      "\n",
      "    accuracy                           0.57        47\n",
      "   macro avg       0.51      0.51      0.51        47\n",
      "weighted avg       0.56      0.57      0.57        47\n",
      "\n",
      "(7,)\n",
      "[ 0.93033407  0.33590076  0.37720502  0.37424997 -0.01671825  0.69692198\n",
      "  0.44831876]\n",
      "956130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.85714286 0.42857143 0.71428571 0.71428571 0.71428571 0.85714286\n",
      " 0.85714286 0.71428571 0.85714286 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 91779.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|███████████████████████████████████████▊   | 87/94 [16:14<01:19, 11.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7428571428571429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.59      0.70      0.64        23\n",
      "         Enc       0.84      0.77      0.80        47\n",
      "\n",
      "    accuracy                           0.74        70\n",
      "   macro avg       0.71      0.73      0.72        70\n",
      "weighted avg       0.76      0.74      0.75        70\n",
      "\n",
      "accuracy = 0.6170212765957447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.43      0.38      0.40        16\n",
      "         Enc       0.70      0.74      0.72        31\n",
      "\n",
      "    accuracy                           0.62        47\n",
      "   macro avg       0.56      0.56      0.56        47\n",
      "weighted avg       0.61      0.62      0.61        47\n",
      "\n",
      "(7,)\n",
      "[-0.31058314 -0.3671757  -0.09662289  1.46802336 -0.66827762  0.37109694\n",
      " -0.42789952]\n",
      "968913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.60s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.85714286 0.85714286 0.71428571 0.85714286 0.42857143 0.85714286\n",
      " 0.57142857 0.42857143 1.         0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 70611.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|████████████████████████████████████████▎  | 88/94 [16:25<01:07, 11.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.55      0.70      0.62        23\n",
      "         Enc       0.83      0.72      0.77        47\n",
      "\n",
      "    accuracy                           0.71        70\n",
      "   macro avg       0.69      0.71      0.69        70\n",
      "weighted avg       0.74      0.71      0.72        70\n",
      "\n",
      "accuracy = 0.723404255319149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.58      0.69      0.63        16\n",
      "         Enc       0.82      0.74      0.78        31\n",
      "\n",
      "    accuracy                           0.72        47\n",
      "   macro avg       0.70      0.71      0.70        47\n",
      "weighted avg       0.74      0.72      0.73        47\n",
      "\n",
      "(7,)\n",
      "[ 0.14351399  0.22122681  1.69392534  0.68458337 -0.33881265  0.36092415\n",
      " -0.09139326]\n",
      "974246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.85714286 0.28571429 1.         0.85714286 0.57142857 0.85714286\n",
      " 0.71428571 0.42857143 0.42857143 0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 95979.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|████████████████████████████████████████▋  | 89/94 [16:40<01:01, 12.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.48      0.70      0.57        23\n",
      "         Enc       0.81      0.64      0.71        47\n",
      "\n",
      "    accuracy                           0.66        70\n",
      "   macro avg       0.65      0.67      0.64        70\n",
      "weighted avg       0.70      0.66      0.67        70\n",
      "\n",
      "accuracy = 0.6808510638297872\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.52      0.75      0.62        16\n",
      "         Enc       0.83      0.65      0.73        31\n",
      "\n",
      "    accuracy                           0.68        47\n",
      "   macro avg       0.68      0.70      0.67        47\n",
      "weighted avg       0.73      0.68      0.69        47\n",
      "\n",
      "(7,)\n",
      "[-0.66997148 -0.48856372  0.08947428  1.48342112  0.24552167  0.19448719\n",
      "  0.18032747]\n",
      "979001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.28571429 0.14285714 0.57142857 0.85714286 0.28571429 0.71428571\n",
      " 0.71428571 0.71428571 0.85714286 0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|███████████████████████████████████████| 10/10 [00:00<00:00, 122640.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████████████████████████████████████▏ | 90/94 [16:54<00:50, 12.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.41      0.57      0.47        23\n",
      "         Enc       0.74      0.60      0.66        47\n",
      "\n",
      "    accuracy                           0.59        70\n",
      "   macro avg       0.57      0.58      0.57        70\n",
      "weighted avg       0.63      0.59      0.60        70\n",
      "\n",
      "accuracy = 0.6170212765957447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.44      0.50      0.47        16\n",
      "         Enc       0.72      0.68      0.70        31\n",
      "\n",
      "    accuracy                           0.62        47\n",
      "   macro avg       0.58      0.59      0.59        47\n",
      "weighted avg       0.63      0.62      0.62        47\n",
      "\n",
      "(7,)\n",
      "[0.76728137 0.78141517 0.06407984 1.42471511 0.45483883 0.50462936\n",
      " 0.79532667]\n",
      "983291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.57142857 0.85714286 0.42857143 0.71428571 0.42857143\n",
      " 0.57142857 0.71428571 0.42857143 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 66894.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████████████████████████████████████▋ | 91/94 [17:05<00:36, 12.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.43      0.52      0.47        23\n",
      "         Enc       0.74      0.66      0.70        47\n",
      "\n",
      "    accuracy                           0.61        70\n",
      "   macro avg       0.58      0.59      0.58        70\n",
      "weighted avg       0.64      0.61      0.62        70\n",
      "\n",
      "accuracy = 0.574468085106383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.40      0.50      0.44        16\n",
      "         Enc       0.70      0.61      0.66        31\n",
      "\n",
      "    accuracy                           0.57        47\n",
      "   macro avg       0.55      0.56      0.55        47\n",
      "weighted avg       0.60      0.57      0.58        47\n",
      "\n",
      "(7,)\n",
      "[-0.70720563  0.60568984 -0.44162032  0.57675658 -0.67096715  0.1813432\n",
      " -0.05255294]\n",
      "988602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.57142857 0.57142857 0.85714286 0.85714286 0.42857143\n",
      " 0.57142857 0.57142857 1.         0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 74367.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|██████████████████████████████████████████ | 92/94 [17:16<00:24, 12.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.48      0.61      0.54        23\n",
      "         Enc       0.78      0.68      0.73        47\n",
      "\n",
      "    accuracy                           0.66        70\n",
      "   macro avg       0.63      0.64      0.63        70\n",
      "weighted avg       0.68      0.66      0.67        70\n",
      "\n",
      "accuracy = 0.6808510638297872\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.53      0.50      0.52        16\n",
      "         Enc       0.75      0.77      0.76        31\n",
      "\n",
      "    accuracy                           0.68        47\n",
      "   macro avg       0.64      0.64      0.64        47\n",
      "weighted avg       0.68      0.68      0.68        47\n",
      "\n",
      "(7,)\n",
      "[ 0.30181466  0.15642516  0.33382704  0.06049513 -0.7557305   0.87961058\n",
      "  0.75550115]\n",
      "996599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 testing: 47\n",
      "Enc    47\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    31\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.57142857 0.57142857 0.57142857 0.42857143 0.57142857 0.57142857\n",
      " 0.71428571 0.85714286 0.85714286 0.85714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 69905.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|██████████████████████████████████████████▌| 93/94 [17:27<00:11, 11.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.48      0.61      0.54        23\n",
      "         Enc       0.78      0.68      0.73        47\n",
      "\n",
      "    accuracy                           0.66        70\n",
      "   macro avg       0.63      0.64      0.63        70\n",
      "weighted avg       0.68      0.66      0.67        70\n",
      "\n",
      "accuracy = 0.7021276595744681\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.56      0.62      0.59        16\n",
      "         Enc       0.79      0.74      0.77        31\n",
      "\n",
      "    accuracy                           0.70        47\n",
      "   macro avg       0.67      0.68      0.68        47\n",
      "weighted avg       0.71      0.70      0.71        47\n",
      "\n",
      "(7,)\n",
      "[ 0.36103144 -0.16216427 -0.27629087 -0.53674542 -1.03137192  0.24936269\n",
      " -0.40893256]\n",
      "998166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/myenv/lib/python3.7/site-packages/nilearn/input_data/nifti_labels_masker.py:483: UserWarning: Persisting input arguments took 0.57s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  verbose=self.verbose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 67 testing: 46\n",
      "Enc    44\n",
      "CTL    23\n",
      "Name: condition, dtype: int64 Enc    30\n",
      "CTL    16\n",
      "Name: condition, dtype: int64\n",
      "[0.42857143 0.57142857 1.         0.85714286 0.71428571 0.57142857\n",
      " 0.57142857 0.83333333 1.         0.66666667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 65433.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████| 94/94 [17:38<00:00, 11.26s/it]\u001b[A\n",
      "  0%|                                                     | 0/5 [17:38<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7164179104477612\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.60      0.52      0.56        23\n",
      "         Enc       0.77      0.82      0.79        44\n",
      "\n",
      "    accuracy                           0.72        67\n",
      "   macro avg       0.68      0.67      0.67        67\n",
      "weighted avg       0.71      0.72      0.71        67\n",
      "\n",
      "accuracy = 0.7391304347826086\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CTL       0.75      0.38      0.50        16\n",
      "         Enc       0.74      0.93      0.82        30\n",
      "\n",
      "    accuracy                           0.74        46\n",
      "   macro avg       0.74      0.65      0.66        46\n",
      "weighted avg       0.74      0.74      0.71        46\n",
      "\n",
      "(7,)\n",
      "[-0.10185074  0.51882647 -0.20209084  0.78330652 -0.61375276  1.01965075\n",
      "  1.01842513]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/fnadeau/../../data/cisl/DATA/cimaq_classification_outputs/SVC_withinSub_enc_ctl_7networks.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63311/352837471.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     enc_ctl_data.to_csv(pjoin(output_dir, 'SVC_withinSub_enc_ctl_'+str(numnet)+'networks.tsv'),\n\u001b[0;32m--> 137\u001b[0;31m         sep='\\t', header=True, index=False)\n\u001b[0m",
      "\u001b[0;32m~/myenv/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3477\u001b[0m             \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m             \u001b[0mescapechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mescapechar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3480\u001b[0m         )\n\u001b[1;32m   3481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.7/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         ) as handles:\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             )\n\u001b[1;32m    708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/fnadeau/../../data/cisl/DATA/cimaq_classification_outputs/SVC_withinSub_enc_ctl_7networks.tsv'"
     ]
    }
   ],
   "source": [
    "# ENCODING VERSUS CONTROL TASK CLASSIFICATION\n",
    "numnets = [7, 20, 64, 325, 444]\n",
    "for numnet in tqdm(numnets):\n",
    "\n",
    "    basc = image.load_img(pjoin(basc_dir, 'MIST_'+str(numnet)+'.nii'))\n",
    "    b_labels = pjoin(b_labels_path,'MIST_'+str(numnet)+'.csv')\n",
    "#     b_labels = '/Users/mombot/Documents/Simexp/CIMAQ/Data/MIST/Release/Parcel_Information/MIST_'+str(numnet)+'.csv'\n",
    "    basc_labels = pd.read_csv(b_labels, sep=';')\n",
    "\n",
    "    # build data structure to store accuracy data and coefficients\n",
    "    enc_ctl_data = pd.DataFrame()\n",
    "    enc_ctl_data.insert(loc = 0, column = 'dccid', value = 'None', allow_duplicates=True)\n",
    "    for i in range(0, 10):\n",
    "        enc_ctl_data.insert(loc = enc_ctl_data.shape[1], column = 'CV'+str(i+1)+'_acc',\n",
    "                            value = NaN, allow_duplicates=True)\n",
    "    enc_ctl_data.insert(loc = enc_ctl_data.shape[1], column = 'TrainSet_MeanCV_acc',\n",
    "                        value = 'None', allow_duplicates=True)\n",
    "    enc_ctl_data.insert(loc = enc_ctl_data.shape[1], column = 'TestSet_acc',\n",
    "                        value = 'None', allow_duplicates=True)\n",
    "    netnames = basc_labels['name']\n",
    "    for i in tqdm(range(0, numnet)):\n",
    "        enc_ctl_data.insert(loc = enc_ctl_data.shape[1], column = netnames[i]+'_coef',\n",
    "                            value = NaN, allow_duplicates=True)\n",
    "\n",
    "    for sub in tqdm(all_subs):\n",
    "        print(sub)\n",
    "        s_data = [sub]\n",
    "        # load subject's beta maps (one per trial)\n",
    "        betas = image.load_img(img=pjoin(beta_dir, str(sub), 'TrialContrasts/betas_sub'+str(sub)+'*.nii'),\n",
    "                               wildcards=True)\n",
    "        # initialize NiftiLabelMasker object    \n",
    "        sub_mask = nb.load(pjoin(mask_dir, 'func_sub'+str(sub)+'_mask_stereonl.nii'))\n",
    "        sub_label_masker = NiftiLabelsMasker(labels_img=basc, standardize=True, mask_img=sub_mask,\n",
    "                                             memory = 'nilearn_cache', verbose=0)\n",
    "\n",
    "        # transform subject's beta maps into vector of network means per trial\n",
    "        X_enc_ctl = sub_label_masker.fit_transform(betas)\n",
    "\n",
    "        # load subject's trial labels\n",
    "        labels_file = pjoin(label_dir, 'sub-'+str(sub)+'_enco_ctl.tsv')\n",
    "        enco_ctl_labels = pd.read_csv(labels_file, sep='\\t')\n",
    "        y_enco_ctl = enco_ctl_labels['condition']\n",
    "\n",
    "        # mask data to exclude trials of no interest\n",
    "        # does not apply here\n",
    "\n",
    "        # Split trials into a training and a test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_enc_ctl, # x\n",
    "            y_enco_ctl, # y\n",
    "            test_size = 0.4, # 60%/40% split\n",
    "            shuffle = True, # shuffle dataset before splitting\n",
    "            stratify = y_enco_ctl, # keep distribution of conditions consistent betw. train & test sets\n",
    "            #random_state = 123  # if set number, same shuffle each time, otherwise randomization algo\n",
    "            ) \n",
    "        print('training:', len(X_train), 'testing:', len(X_test))\n",
    "        print(y_train.value_counts(), y_test.value_counts())\n",
    "\n",
    "        # define the model\n",
    "        sub_svc = SVC(kernel='linear', class_weight='balanced')\n",
    "\n",
    "        # do cross-validation to evaluate model performance\n",
    "        # within 10 folds of training set\n",
    "        # predict\n",
    "        y_pred = cross_val_predict(sub_svc, X_train, y_train,\n",
    "                                   groups=y_train, cv=10)\n",
    "        # scores\n",
    "        cv_acc = cross_val_score(sub_svc, X_train, y_train,\n",
    "                             groups=y_train, cv=10)\n",
    "        print(cv_acc)\n",
    "\n",
    "        for i in tqdm(range(0, len(cv_acc))):\n",
    "            s_data.append(cv_acc[i])\n",
    "\n",
    "        # evaluate overall model performance on training data\n",
    "        overall_acc = accuracy_score(y_pred = y_pred, y_true = y_train)\n",
    "        overall_cr = classification_report(y_pred = y_pred, y_true = y_train)\n",
    "        print('Accuracy:',overall_acc)\n",
    "        print(overall_cr)\n",
    "\n",
    "        s_data.append(overall_acc)\n",
    "\n",
    "        # Test model on unseen data from the test set\n",
    "        sub_svc.fit(X_train, y_train)\n",
    "        y_pred = sub_svc.predict(X_test) # classify age class using testing data\n",
    "        acc = sub_svc.score(X_test, y_test) # get accuracy\n",
    "\n",
    "        cr = classification_report(y_pred=y_pred, y_true=y_test) # get prec., recall & f1\n",
    "        # print results\n",
    "        print('accuracy =', acc)\n",
    "        print(cr)  \n",
    "\n",
    "        s_data.append(acc)\n",
    "\n",
    "        # get coefficients\n",
    "        coef_ = sub_svc.coef_[0]\n",
    "        print(coef_.shape)\n",
    "        print(coef_)\n",
    "\n",
    "        sub_basc = basc_labels.copy()\n",
    "        sub_basc.insert(loc=3, column='coef', value=coef_, allow_duplicates=True)\n",
    "\n",
    "        coef = sub_basc['coef']\n",
    "        for j in range(0, len(coef)):\n",
    "            s_data.append(coef[j])\n",
    "\n",
    "        #sub_basc.sort_values(by='coef', axis = 0, ascending = False, inplace=True)\n",
    "        #print(sub_basc.iloc[0:12, 2:4])\n",
    "\n",
    "        enc_ctl_data = enc_ctl_data.append(pd.Series(s_data, index=enc_ctl_data.columns),\n",
    "                                           ignore_index=True)\n",
    "\n",
    "\n",
    "    demo_data = sub_data.copy()\n",
    "    demo_data.reset_index(level=None, drop=False, inplace=True)\n",
    "\n",
    "    enc_ctl_data.insert(loc = 1, column = 'cognitive_status',\n",
    "                        value = demo_data['cognitive_status'], allow_duplicates=True)\n",
    "    enc_ctl_data.insert(loc = 2, column = 'total_scrubbed_frames',\n",
    "                        value = demo_data['total_scrubbed_frames'], allow_duplicates=True)\n",
    "    enc_ctl_data.insert(loc = 3, column = 'mean_FD',\n",
    "                        value = demo_data['mean_FD'], allow_duplicates=True)\n",
    "    enc_ctl_data.insert(loc = 4, column = 'hits',\n",
    "                        value = demo_data['hits'], allow_duplicates=True)\n",
    "    enc_ctl_data.insert(loc = 5, column = 'miss',\n",
    "                        value = demo_data['miss'], allow_duplicates=True)\n",
    "    enc_ctl_data.insert(loc = 6, column = 'correct_source',\n",
    "                        value = demo_data['correct_source'], allow_duplicates=True)\n",
    "    enc_ctl_data.insert(loc = 7, column = 'wrong_source',\n",
    "                        value = demo_data['wrong_source'], allow_duplicates=True)\n",
    "    enc_ctl_data.insert(loc = 8, column = 'dprime',\n",
    "                        value = demo_data['dprime'], allow_duplicates=True)\n",
    "    enc_ctl_data.insert(loc = 9, column = 'associative_memScore',\n",
    "                        value = demo_data['associative_memScore'], allow_duplicates=True)    \n",
    "\n",
    "    enc_ctl_data.to_csv(pjoin(output_dir, 'SVC_withinSub_enc_ctl_'+str(numnet)+'networks.tsv'),\n",
    "        sep='\\t', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MIST_12.csv',\n",
       " 'MIST_122.csv',\n",
       " 'MIST_197.csv',\n",
       " 'MIST_20.csv',\n",
       " 'MIST_325.csv',\n",
       " 'MIST_36.csv',\n",
       " 'MIST_444.csv',\n",
       " 'MIST_64.csv',\n",
       " 'MIST_7.csv',\n",
       " 'MIST_ROI.csv',\n",
       " 'desktop.ini']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(ls('/home/fnadeau/../../data/cisl/DATA/cimaq_03-19/derivatives/CIMAQ_fmri_memory/data/templates/MIST_parcellation/Release/Parcel_Information'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'basc_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63311/705956237.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnumnet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumnets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mbasc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasc_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MIST_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.nii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mb_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/mombot/Documents/Simexp/CIMAQ/Data/MIST/Release/Parcel_Information/MIST_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbasc_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'basc_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# HIT VERSUS MISS TRIAL CLASSIFICATION\n",
    "for numnet in numnets:\n",
    "\n",
    "    basc = image.load_img(os.path.join(basc_dir, 'MIST_'+str(numnet)+'.nii'))\n",
    "    b_labels = '/Users/mombot/Documents/Simexp/CIMAQ/Data/MIST/Release/Parcel_Information/MIST_'+str(numnet)+'.csv'\n",
    "    basc_labels = pd.read_csv(b_labels, sep=';')\n",
    "    \n",
    "    # build data structure to store accuracy data and coefficients\n",
    "    hit_miss_data = pd.DataFrame()\n",
    "    hit_miss_data.insert(loc = 0, column = 'dccid',\n",
    "                         value = 'None', allow_duplicates=True)\n",
    "    # hit_miss_data.insert(loc = 1, column = 'diagnosis', value = 'None', allow_duplicates=True)\n",
    "    for i in range(0, 7):\n",
    "        hit_miss_data.insert(loc = hit_miss_data.shape[1], column = 'CV'+str(i+1)+'_acc',\n",
    "                             value = NaN, allow_duplicates=True)\n",
    "    hit_miss_data.insert(loc = hit_miss_data.shape[1], column = 'TrainSet_MeanCV_acc',\n",
    "                         value = 'None', allow_duplicates=True)\n",
    "    hit_miss_data.insert(loc = hit_miss_data.shape[1], column = 'TestSet_acc',\n",
    "                         value = 'None', allow_duplicates=True)\n",
    "    netnames = basc_labels['name']\n",
    "    for i in range(0, numnet):\n",
    "        hit_miss_data.insert(loc = hit_miss_data.shape[1], column = netnames[i]+'_coef',\n",
    "                             value = NaN, allow_duplicates=True)\n",
    "\n",
    "    for sub in hm_subs:\n",
    "        print(sub)\n",
    "        s_data = [sub]\n",
    "        # load subject's beta maps (one per trial)\n",
    "        betas = image.load_img(img=os.path.join(beta_dir, str(sub), 'TrialContrasts/betas_sub'+str(sub)+'*.nii'),\n",
    "                               wildcards=True)\n",
    "        # initialize NiftiLabelMasker object    \n",
    "        sub_mask = nb.load(os.path.join(mask_dir, 'func_sub'+str(sub)+'_mask_stereonl.nii'))\n",
    "        sub_label_masker = NiftiLabelsMasker(labels_img=basc, standardize=True, mask_img=sub_mask,\n",
    "                                             memory = 'nilearn_cache', verbose=0)\n",
    "\n",
    "        # transform subject's beta maps into vector of network means per trial\n",
    "        X_hit_miss_ctl = sub_label_masker.fit_transform(betas)   \n",
    "\n",
    "        # load subject's trial labels\n",
    "        labels_file = os.path.join(label_dir, 'sub-'+str(sub)+'_ctl_miss_hit.tsv')\n",
    "        y_hit_miss_ctl = pd.read_csv(labels_file, sep='\\t')\n",
    "        y_hit_miss_ctl_labels = y_hit_miss_ctl['ctl_miss_hit']\n",
    "        # mask X and y data to exclude trials of no interest\n",
    "        hit_miss_mask = y_hit_miss_ctl_labels.isin(['hit', 'missed'])\n",
    "        y_hit_miss = y_hit_miss_ctl_labels[hit_miss_mask]      \n",
    "        X_hit_miss  = X_hit_miss_ctl[hit_miss_mask]\n",
    "\n",
    "        # Split trials into a training and a test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_hit_miss, # x\n",
    "            y_hit_miss, # y\n",
    "            test_size = 0.4, # 60%/40% split\n",
    "            shuffle = True, # shuffle dataset before splitting\n",
    "            stratify = y_hit_miss, # keep distribution of conditions consistent betw. train & test sets\n",
    "            #random_state = 123  # if set number, same shuffle each time, otherwise randomization algo\n",
    "            ) \n",
    "        print('training:', len(X_train), 'testing:', len(X_test))\n",
    "        print(y_train.value_counts(), y_test.value_counts())\n",
    "\n",
    "        # define the model\n",
    "        sub_svc = SVC(kernel='linear', class_weight='balanced')\n",
    "\n",
    "        # do cross-validation to evaluate model performance\n",
    "        # within 10 folds of training set\n",
    "        # predict\n",
    "        y_pred = cross_val_predict(sub_svc, X_train, y_train,\n",
    "                                   groups=y_train, cv=7)\n",
    "        # scores\n",
    "        cv_acc = cross_val_score(sub_svc, X_train, y_train,\n",
    "                             groups=y_train, cv=7)\n",
    "        print(cv_acc)\n",
    "\n",
    "        for i in range(0, len(cv_acc)):\n",
    "            s_data.append(cv_acc[i])\n",
    "\n",
    "        # evaluate overall model performance on training data\n",
    "        overall_acc = accuracy_score(y_pred = y_pred, y_true = y_train)\n",
    "        overall_cr = classification_report(y_pred = y_pred, y_true = y_train)\n",
    "        print('Accuracy:',overall_acc)\n",
    "        print(overall_cr)\n",
    "\n",
    "        s_data.append(overall_acc)\n",
    "\n",
    "        # Test model on unseen data from the test set\n",
    "        sub_svc.fit(X_train, y_train)\n",
    "        y_pred = sub_svc.predict(X_test) # classify age class using testing data\n",
    "        acc = sub_svc.score(X_test, y_test) # get accuracy\n",
    "\n",
    "        cr = classification_report(y_pred=y_pred, y_true=y_test) # get prec., recall & f1\n",
    "        # print results\n",
    "        print('accuracy =', acc)\n",
    "        print(cr)  \n",
    "\n",
    "        s_data.append(acc)\n",
    "\n",
    "        # get coefficients\n",
    "        coef_ = sub_svc.coef_[0]\n",
    "        print(coef_.shape)\n",
    "        print(coef_)\n",
    "\n",
    "        sub_basc = basc_labels.copy()\n",
    "        sub_basc.insert(loc=3, column='coef', value=coef_, allow_duplicates=True)\n",
    "\n",
    "        coef = sub_basc['coef']\n",
    "        for j in range(0, len(coef)):\n",
    "            s_data.append(coef[j])\n",
    "\n",
    "        #sub_basc.sort_values(by='coef', axis = 0, ascending = False, inplace=True)\n",
    "        #print(sub_basc.iloc[0:12, 2:4])\n",
    "\n",
    "        hit_miss_data = hit_miss_data.append(pd.Series(s_data, index=hit_miss_data.columns), ignore_index=True)\n",
    "\n",
    "    demo_data = hm_data.copy()\n",
    "    demo_data.reset_index(level=None, drop=False, inplace=True)\n",
    "\n",
    "    hit_miss_data.insert(loc = 1, column = 'cognitive_status', value = demo_data['cognitive_status'], allow_duplicates=True)\n",
    "    hit_miss_data.insert(loc = 2, column = 'total_scrubbed_frames', value = demo_data['total_scrubbed_frames'], allow_duplicates=True)\n",
    "    hit_miss_data.insert(loc = 3, column = 'mean_FD', value = demo_data['mean_FD'], allow_duplicates=True)\n",
    "    hit_miss_data.insert(loc = 4, column = 'hits', value = demo_data['hits'], allow_duplicates=True)\n",
    "    hit_miss_data.insert(loc = 5, column = 'miss', value = demo_data['miss'], allow_duplicates=True)\n",
    "    hit_miss_data.insert(loc = 6, column = 'correct_source', value = demo_data['correct_source'], allow_duplicates=True)\n",
    "    hit_miss_data.insert(loc = 7, column = 'wrong_source', value = demo_data['wrong_source'], allow_duplicates=True)\n",
    "    hit_miss_data.insert(loc = 8, column = 'dprime', value = demo_data['dprime'], allow_duplicates=True)\n",
    "    hit_miss_data.insert(loc = 9, column = 'associative_memScore', value = demo_data['associative_memScore'], allow_duplicates=True)    \n",
    "\n",
    "    hit_miss_data.to_csv(os.path.join(output_dir, 'SVC_withinSub_hit_miss_'+str(numnet)+'networks.tsv'),\n",
    "        sep='\\t', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'basc_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63311/3247515776.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnumnet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumnets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mbasc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasc_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MIST_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.nii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mb_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/mombot/Documents/Simexp/CIMAQ/Data/MIST/Release/Parcel_Information/MIST_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbasc_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'basc_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# CORRECT SOURCE VERSUS WRONG SOURCE TRIAL CLASSIFICATION\n",
    "\n",
    "for numnet in numnets:\n",
    "\n",
    "    basc = image.load_img(os.path.join(basc_dir, 'MIST_'+str(numnet)+'.nii'))\n",
    "    b_labels = '/Users/mombot/Documents/Simexp/CIMAQ/Data/MIST/Release/Parcel_Information/MIST_'+str(numnet)+'.csv'\n",
    "    basc_labels = pd.read_csv(b_labels, sep=';')\n",
    "\n",
    "    # build data structure to store accuracy data and coefficients\n",
    "    cs_ws_data = pd.DataFrame()\n",
    "    cs_ws_data.insert(loc = 0, column = 'dccid', value = 'None', allow_duplicates=True)\n",
    "    # cs_ws_data.insert(loc = 1, column = 'diagnosis', value = 'None', allow_duplicates=True)\n",
    "    for i in range(0, 7):\n",
    "        cs_ws_data.insert(loc = cs_ws_data.shape[1], column = 'CV'+str(i+1)+'_acc', value = NaN, allow_duplicates=True)\n",
    "    cs_ws_data.insert(loc = cs_ws_data.shape[1], column = 'TrainSet_MeanCV_acc', value = 'None', allow_duplicates=True)\n",
    "    cs_ws_data.insert(loc = cs_ws_data.shape[1], column = 'TestSet_acc', value = 'None', allow_duplicates=True)\n",
    "    netnames = basc_labels['name']\n",
    "    for i in range(0, numnet):\n",
    "        cs_ws_data.insert(loc = cs_ws_data.shape[1], column = netnames[i]+'_coef', value = NaN, allow_duplicates=True)\n",
    "\n",
    "    for sub in cw_subs:\n",
    "        print(sub)\n",
    "        s_data = [sub]\n",
    "        # load subject's beta maps (one per trial)\n",
    "        betas = image.load_img(img=os.path.join(beta_dir, str(sub), 'TrialContrasts/betas_sub'+str(sub)+'*.nii'),\n",
    "                               wildcards=True)\n",
    "        # initialize NiftiLabelMasker object    \n",
    "        sub_mask = nb.load(os.path.join(mask_dir, 'func_sub'+str(sub)+'_mask_stereonl.nii'))\n",
    "        sub_label_masker = NiftiLabelsMasker(labels_img=basc, standardize=True, mask_img=sub_mask,\n",
    "                                             memory = 'nilearn_cache', verbose=0)\n",
    "\n",
    "        # transform subject's beta maps into vector of network means per trial\n",
    "        X_cs_ws_miss_ctl = sub_label_masker.fit_transform(betas)   \n",
    "\n",
    "        # load subject's trial labels\n",
    "        labels_file = os.path.join(label_dir, 'sub-'+str(sub)+'_ctl_miss_ws_cs.tsv')\n",
    "        y_cs_ws_miss_ctl = pd.read_csv(labels_file, sep='\\t')\n",
    "        y_cs_ws_miss_ctl_labels = y_cs_ws_miss_ctl['ctl_miss_ws_cs']\n",
    "        # mask X and y data to exclude trials of no interest\n",
    "        cs_ws_mask = y_cs_ws_miss_ctl_labels.isin(['correctsource', 'wrongsource'])\n",
    "        y_cs_ws = y_cs_ws_miss_ctl_labels[cs_ws_mask]      \n",
    "        X_cs_ws  = X_cs_ws_miss_ctl[cs_ws_mask]\n",
    "\n",
    "        # Split trials into a training and a test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_cs_ws, # x\n",
    "            y_cs_ws, # y\n",
    "            test_size = 0.4, # 60%/40% split\n",
    "            shuffle = True, # shuffle dataset before splitting\n",
    "            stratify = y_cs_ws, # keep distribution of conditions consistent betw. train & test sets\n",
    "            #random_state = 123  # if set number, same shuffle each time, otherwise randomization algo\n",
    "            ) \n",
    "        print('training:', len(X_train), 'testing:', len(X_test))\n",
    "        print(y_train.value_counts(), y_test.value_counts())\n",
    "\n",
    "        # define the model\n",
    "        sub_svc = SVC(kernel='linear', class_weight='balanced')\n",
    "\n",
    "        # do cross-validation to evaluate model performance\n",
    "        # within 10 folds of training set\n",
    "        # predict\n",
    "        y_pred = cross_val_predict(sub_svc, X_train, y_train,\n",
    "                                   groups=y_train, cv=7)\n",
    "        # scores\n",
    "        cv_acc = cross_val_score(sub_svc, X_train, y_train,\n",
    "                             groups=y_train, cv=7)\n",
    "        print(cv_acc)\n",
    "\n",
    "        for i in range(0, len(cv_acc)):\n",
    "            s_data.append(cv_acc[i])\n",
    "\n",
    "        # evaluate overall model performance on training data\n",
    "        overall_acc = accuracy_score(y_pred = y_pred, y_true = y_train)\n",
    "        overall_cr = classification_report(y_pred = y_pred, y_true = y_train)\n",
    "        print('Accuracy:',overall_acc)\n",
    "        print(overall_cr)\n",
    "\n",
    "        s_data.append(overall_acc)\n",
    "\n",
    "        # Test model on unseen data from the test set\n",
    "        sub_svc.fit(X_train, y_train)\n",
    "        y_pred = sub_svc.predict(X_test) # classify age class using testing data\n",
    "        acc = sub_svc.score(X_test, y_test) # get accuracy\n",
    "\n",
    "        cr = classification_report(y_pred=y_pred, y_true=y_test) # get prec., recall & f1\n",
    "        # print results\n",
    "        print('accuracy =', acc)\n",
    "        print(cr)  \n",
    "\n",
    "        s_data.append(acc)\n",
    "\n",
    "        # get coefficients\n",
    "        coef_ = sub_svc.coef_[0]\n",
    "        print(coef_.shape)\n",
    "        print(coef_)\n",
    "\n",
    "        sub_basc = basc_labels.copy()\n",
    "        sub_basc.insert(loc=3, column='coef', value=coef_, allow_duplicates=True)\n",
    "\n",
    "        coef = sub_basc['coef']\n",
    "        for j in range(0, len(coef)):\n",
    "            s_data.append(coef[j])\n",
    "\n",
    "        #sub_basc.sort_values(by='coef', axis = 0, ascending = False, inplace=True)\n",
    "        #print(sub_basc.iloc[0:12, 2:4])\n",
    "\n",
    "        cs_ws_data = cs_ws_data.append(pd.Series(s_data, index=cs_ws_data.columns), ignore_index=True)\n",
    "\n",
    "    demo_data = cw_data.copy()\n",
    "    demo_data.reset_index(level=None, drop=False, inplace=True)\n",
    "\n",
    "    cs_ws_data.insert(loc = 1, column = 'cognitive_status', value = demo_data['cognitive_status'], allow_duplicates=True)\n",
    "    cs_ws_data.insert(loc = 2, column = 'total_scrubbed_frames', value = demo_data['total_scrubbed_frames'], allow_duplicates=True)\n",
    "    cs_ws_data.insert(loc = 3, column = 'mean_FD', value = demo_data['mean_FD'], allow_duplicates=True)\n",
    "    cs_ws_data.insert(loc = 4, column = 'hits', value = demo_data['hits'], allow_duplicates=True)\n",
    "    cs_ws_data.insert(loc = 5, column = 'miss', value = demo_data['miss'], allow_duplicates=True)\n",
    "    cs_ws_data.insert(loc = 6, column = 'correct_source', value = demo_data['correct_source'], allow_duplicates=True)\n",
    "    cs_ws_data.insert(loc = 7, column = 'wrong_source', value = demo_data['wrong_source'], allow_duplicates=True)\n",
    "    cs_ws_data.insert(loc = 8, column = 'dprime', value = demo_data['dprime'], allow_duplicates=True)\n",
    "    cs_ws_data.insert(loc = 9, column = 'associative_memScore', value = demo_data['associative_memScore'], allow_duplicates=True)    \n",
    "\n",
    "    cs_ws_data.to_csv(os.path.join(output_dir, 'SVC_withinSub_cs_ws_'+str(numnet)+'networks.tsv'),\n",
    "        sep='\\t', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108391\n",
      "training: 34 testing: 24\n",
      "correctsource    25\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    18\n",
      "missed            6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.5        0.6        0.4        0.5        0.75\n",
      " 0.5       ]\n",
      "Accuracy: 0.5588235294117647\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.81      0.52      0.63        25\n",
      "       missed       0.33      0.67      0.44         9\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        34\n",
      "    macro avg       0.57      0.59      0.54        34\n",
      " weighted avg       0.69      0.56      0.58        34\n",
      "\n",
      "accuracy = 0.4166666666666667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.70      0.39      0.50        18\n",
      "       missed       0.21      0.50      0.30         6\n",
      "\n",
      "    micro avg       0.42      0.42      0.42        24\n",
      "    macro avg       0.46      0.44      0.40        24\n",
      " weighted avg       0.58      0.42      0.45        24\n",
      "\n",
      "(7,)\n",
      "[-0.23810046  0.50311828 -0.40101328 -0.13102574 -0.98865166  0.02793903\n",
      "  0.8162333 ]\n",
      "122922\n",
      "training: 28 testing: 20\n",
      "correctsource    15\n",
      "missed           13\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           10\n",
      "correctsource    10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.4  0.25 0.25 1.   1.   0.25 1.  ]\n",
      "Accuracy: 0.5714285714285714\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.59      0.67      0.62        15\n",
      "       missed       0.55      0.46      0.50        13\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        28\n",
      "    macro avg       0.57      0.56      0.56        28\n",
      " weighted avg       0.57      0.57      0.57        28\n",
      "\n",
      "accuracy = 0.55\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.54      0.70      0.61        10\n",
      "       missed       0.57      0.40      0.47        10\n",
      "\n",
      "    micro avg       0.55      0.55      0.55        20\n",
      "    macro avg       0.55      0.55      0.54        20\n",
      " weighted avg       0.55      0.55      0.54        20\n",
      "\n",
      "(7,)\n",
      "[-0.35103214 -0.7312814  -0.09131174 -0.18337965  0.66595281 -0.54642094\n",
      "  0.44483542]\n",
      "139593\n",
      "training: 33 testing: 23\n",
      "missed           24\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           17\n",
      "correctsource     6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.33333333 0.2        1.         0.75       0.25\n",
      " 0.25      ]\n",
      "Accuracy: 0.48484848484848486\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.28      0.56      0.37         9\n",
      "       missed       0.73      0.46      0.56        24\n",
      "\n",
      "    micro avg       0.48      0.48      0.48        33\n",
      "    macro avg       0.51      0.51      0.47        33\n",
      " weighted avg       0.61      0.48      0.51        33\n",
      "\n",
      "accuracy = 0.5217391304347826\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.22      0.33      0.27         6\n",
      "       missed       0.71      0.59      0.65        17\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        23\n",
      "    macro avg       0.47      0.46      0.46        23\n",
      " weighted avg       0.59      0.52      0.55        23\n",
      "\n",
      "(7,)\n",
      "[-0.71068219  0.56842959  0.43975245  0.21120113  0.70894252 -0.63451843\n",
      " -0.5982292 ]\n",
      "164965\n",
      "training: 39 testing: 27\n",
      "correctsource    24\n",
      "missed           15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    17\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.57142857 0.33333333 0.33333333 0.4        0.8        0.8\n",
      " 0.8       ]\n",
      "Accuracy: 0.5641025641025641\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.58      0.62        24\n",
      "       missed       0.44      0.53      0.48        15\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        39\n",
      "    macro avg       0.56      0.56      0.55        39\n",
      " weighted avg       0.58      0.56      0.57        39\n",
      "\n",
      "accuracy = 0.5925925925925926\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.69      0.65      0.67        17\n",
      "       missed       0.45      0.50      0.48        10\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        27\n",
      "    macro avg       0.57      0.57      0.57        27\n",
      " weighted avg       0.60      0.59      0.60        27\n",
      "\n",
      "(7,)\n",
      "[ 0.60116967  0.64619737 -0.22159166 -0.28600652 -0.33285627 -0.10257012\n",
      "  0.4691477 ]\n",
      "199801\n",
      "training: 20 testing: 14\n",
      "missed           11\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           8\n",
      "correctsource    6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.25       0.33333333 0.         1.         0.\n",
      " 0.5       ]\n",
      "Accuracy: 0.35\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.30      0.33      0.32         9\n",
      "       missed       0.40      0.36      0.38        11\n",
      "\n",
      "    micro avg       0.35      0.35      0.35        20\n",
      "    macro avg       0.35      0.35      0.35        20\n",
      " weighted avg       0.35      0.35      0.35        20\n",
      "\n",
      "accuracy = 0.6428571428571429\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.50      0.55         6\n",
      "       missed       0.67      0.75      0.71         8\n",
      "\n",
      "    micro avg       0.64      0.64      0.64        14\n",
      "    macro avg       0.63      0.62      0.63        14\n",
      " weighted avg       0.64      0.64      0.64        14\n",
      "\n",
      "(7,)\n",
      "[ 0.18136858 -0.04415335  0.1429996   0.2931381   0.88369871  0.72386775\n",
      " -0.27669003]\n",
      "247659\n",
      "training: 34 testing: 23\n",
      "missed           25\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           17\n",
      "correctsource     6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.83333333 0.5        1.         1.         0.75       0.75\n",
      " 0.75      ]\n",
      "Accuracy: 0.7941176470588235\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.57      0.89      0.70         9\n",
      "       missed       0.95      0.76      0.84        25\n",
      "\n",
      "    micro avg       0.79      0.79      0.79        34\n",
      "    macro avg       0.76      0.82      0.77        34\n",
      " weighted avg       0.85      0.79      0.81        34\n",
      "\n",
      "accuracy = 0.6956521739130435\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.45      0.83      0.59         6\n",
      "       missed       0.92      0.65      0.76        17\n",
      "\n",
      "    micro avg       0.70      0.70      0.70        23\n",
      "    macro avg       0.69      0.74      0.67        23\n",
      " weighted avg       0.80      0.70      0.71        23\n",
      "\n",
      "(7,)\n",
      "[-0.1298089  -0.06615198 -0.87281495  0.35614297 -0.29086049 -1.91757987\n",
      " -0.13965385]\n",
      "255499\n",
      "training: 39 testing: 27\n",
      "correctsource    23\n",
      "missed           16\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.42857143 0.42857143 0.6        0.6        0.2        0.4\n",
      " 0.2       ]\n",
      "Accuracy: 0.41025641025641024\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.43      0.47        23\n",
      "       missed       0.32      0.38      0.34        16\n",
      "\n",
      "    micro avg       0.41      0.41      0.41        39\n",
      "    macro avg       0.41      0.40      0.40        39\n",
      " weighted avg       0.42      0.41      0.41        39\n",
      "\n",
      "accuracy = 0.5185185185185185\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.59      0.62      0.61        16\n",
      "       missed       0.40      0.36      0.38        11\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        27\n",
      "    macro avg       0.49      0.49      0.49        27\n",
      " weighted avg       0.51      0.52      0.51        27\n",
      "\n",
      "(7,)\n",
      "[ 1.04163661 -0.01657106 -0.01130807 -0.01185513  0.02573067  0.03220569\n",
      " -0.46605433]\n",
      "258618\n",
      "training: 36 testing: 25\n",
      "correctsource    20\n",
      "missed           16\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.83333333 0.2        0.8        0.4        0.4\n",
      " 0.75      ]\n",
      "Accuracy: 0.5833333333333334\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.63      0.60      0.62        20\n",
      "       missed       0.53      0.56      0.55        16\n",
      "\n",
      "    micro avg       0.58      0.58      0.58        36\n",
      "    macro avg       0.58      0.58      0.58        36\n",
      " weighted avg       0.59      0.58      0.58        36\n",
      "\n",
      "accuracy = 0.32\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.38      0.36      0.37        14\n",
      "       missed       0.25      0.27      0.26        11\n",
      "\n",
      "    micro avg       0.32      0.32      0.32        25\n",
      "    macro avg       0.32      0.31      0.32        25\n",
      " weighted avg       0.33      0.32      0.32        25\n",
      "\n",
      "(7,)\n",
      "[-0.50464524 -0.26744704  0.52886662 -0.62519141 -0.66358952 -0.29458584\n",
      " -0.03545823]\n",
      "271596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 testing: 25\n",
      "correctsource    22\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.6        0.6        0.4        0.6        0.6\n",
      " 0.8       ]\n",
      "Accuracy: 0.6111111111111112\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.75      0.55      0.63        22\n",
      "       missed       0.50      0.71      0.59        14\n",
      "\n",
      "    micro avg       0.61      0.61      0.61        36\n",
      "    macro avg       0.62      0.63      0.61        36\n",
      " weighted avg       0.65      0.61      0.61        36\n",
      "\n",
      "accuracy = 0.68\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.79      0.69      0.73        16\n",
      "       missed       0.55      0.67      0.60         9\n",
      "\n",
      "    micro avg       0.68      0.68      0.68        25\n",
      "    macro avg       0.67      0.68      0.67        25\n",
      " weighted avg       0.70      0.68      0.69        25\n",
      "\n",
      "(7,)\n",
      "[ 0.62358384 -0.82972445 -0.68194531 -0.41801814  0.9476673   0.24454637\n",
      "  0.25865509]\n",
      "314409\n",
      "training: 40 testing: 28\n",
      "correctsource    21\n",
      "missed           19\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           14\n",
      "correctsource    14\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.5        0.33333333 0.5        0.16666667 0.6\n",
      " 0.2       ]\n",
      "Accuracy: 0.425\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.45      0.43      0.44        21\n",
      "       missed       0.40      0.42      0.41        19\n",
      "\n",
      "    micro avg       0.42      0.42      0.42        40\n",
      "    macro avg       0.43      0.42      0.42        40\n",
      " weighted avg       0.43      0.42      0.43        40\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.43      0.46        14\n",
      "       missed       0.50      0.57      0.53        14\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        28\n",
      "    macro avg       0.50      0.50      0.50        28\n",
      " weighted avg       0.50      0.50      0.50        28\n",
      "\n",
      "(7,)\n",
      "[-0.32996177 -0.62420989  1.09319272  0.37826283 -0.31611107 -0.36786269\n",
      " -0.76798461]\n",
      "336665\n",
      "training: 30 testing: 20\n",
      "correctsource    16\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    11\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6  0.4  0.5  1.   0.5  0.75 0.5 ]\n",
      "Accuracy: 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.50      0.57        16\n",
      "       missed       0.56      0.71      0.63        14\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        30\n",
      "    macro avg       0.61      0.61      0.60        30\n",
      " weighted avg       0.61      0.60      0.60        30\n",
      "\n",
      "accuracy = 0.55\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.55      0.57        11\n",
      "       missed       0.50      0.56      0.53         9\n",
      "\n",
      "    micro avg       0.55      0.55      0.55        20\n",
      "    macro avg       0.55      0.55      0.55        20\n",
      " weighted avg       0.55      0.55      0.55        20\n",
      "\n",
      "(7,)\n",
      "[-1.17706195 -0.11536333  1.15516804  0.23551092 -0.84718837 -0.48067036\n",
      "  0.29170732]\n",
      "337021\n",
      "training: 34 testing: 23\n",
      "correctsource    24\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.33333333 0.83333333 0.5        1.         0.75\n",
      " 0.5       ]\n",
      "Accuracy: 0.6176470588235294\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.79      0.62      0.70        24\n",
      "       missed       0.40      0.60      0.48        10\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        34\n",
      "    macro avg       0.59      0.61      0.59        34\n",
      " weighted avg       0.67      0.62      0.63        34\n",
      "\n",
      "accuracy = 0.4782608695652174\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.50      0.57        16\n",
      "       missed       0.27      0.43      0.33         7\n",
      "\n",
      "    micro avg       0.48      0.48      0.48        23\n",
      "    macro avg       0.47      0.46      0.45        23\n",
      " weighted avg       0.55      0.48      0.50        23\n",
      "\n",
      "(7,)\n",
      "[-0.21768056 -0.7135406  -0.76877526  1.26007736  1.14922744  0.44362257\n",
      "  0.51857073]\n",
      "396250\n",
      "training: 39 testing: 27\n",
      "missed           21\n",
      "correctsource    18\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           14\n",
      "correctsource    13\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.5        0.5        0.33333333 0.6        0.6\n",
      " 0.6       ]\n",
      "Accuracy: 0.5128205128205128\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.47      0.50      0.49        18\n",
      "       missed       0.55      0.52      0.54        21\n",
      "\n",
      "    micro avg       0.51      0.51      0.51        39\n",
      "    macro avg       0.51      0.51      0.51        39\n",
      " weighted avg       0.51      0.51      0.51        39\n",
      "\n",
      "accuracy = 0.7407407407407407\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.69      0.85      0.76        13\n",
      "       missed       0.82      0.64      0.72        14\n",
      "\n",
      "    micro avg       0.74      0.74      0.74        27\n",
      "    macro avg       0.75      0.74      0.74        27\n",
      " weighted avg       0.76      0.74      0.74        27\n",
      "\n",
      "(7,)\n",
      "[-0.52658219 -0.20056888 -1.16599801 -0.85276964 -0.37696744 -1.3899273\n",
      " -0.53575688]\n",
      "403131\n",
      "training: 42 testing: 28\n",
      "correctsource    23\n",
      "missed           19\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.57142857 0.57142857 0.5        0.5        0.66666667 0.6\n",
      " 0.4       ]\n",
      "Accuracy: 0.5476190476190477\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.58      0.65      0.61        23\n",
      "       missed       0.50      0.42      0.46        19\n",
      "\n",
      "    micro avg       0.55      0.55      0.55        42\n",
      "    macro avg       0.54      0.54      0.53        42\n",
      " weighted avg       0.54      0.55      0.54        42\n",
      "\n",
      "accuracy = 0.4642857142857143\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.53      0.56      0.55        16\n",
      "       missed       0.36      0.33      0.35        12\n",
      "\n",
      "    micro avg       0.46      0.46      0.46        28\n",
      "    macro avg       0.45      0.45      0.45        28\n",
      " weighted avg       0.46      0.46      0.46        28\n",
      "\n",
      "(7,)\n",
      "[-0.57434608 -0.44445843  0.14495967 -0.58715485  0.33440567 -0.80075202\n",
      " -0.10595658]\n",
      "408506\n",
      "training: 30 testing: 21\n",
      "missed           16\n",
      "correctsource    14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           11\n",
      "correctsource    10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.4  0.6  0.25 0.25 0.75 1.   0.75]\n",
      "Accuracy: 0.5666666666666667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.53      0.64      0.58        14\n",
      "       missed       0.62      0.50      0.55        16\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        30\n",
      "    macro avg       0.57      0.57      0.57        30\n",
      " weighted avg       0.58      0.57      0.57        30\n",
      "\n",
      "accuracy = 0.5714285714285714\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.55      0.60      0.57        10\n",
      "       missed       0.60      0.55      0.57        11\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        21\n",
      "    macro avg       0.57      0.57      0.57        21\n",
      " weighted avg       0.57      0.57      0.57        21\n",
      "\n",
      "(7,)\n",
      "[ 0.20707607 -0.55224733 -1.1669214   0.40924466  0.7067754  -0.36915241\n",
      "  0.17096251]\n",
      "413474\n",
      "training: 37 testing: 25\n",
      "correctsource    25\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    17\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.83333333 0.33333333 0.5        0.2        0.5\n",
      " 0.5       ]\n",
      "Accuracy: 0.4594594594594595\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.63      0.48      0.55        25\n",
      "       missed       0.28      0.42      0.33        12\n",
      "\n",
      "    micro avg       0.46      0.46      0.46        37\n",
      "    macro avg       0.45      0.45      0.44        37\n",
      " weighted avg       0.52      0.46      0.48        37\n",
      "\n",
      "accuracy = 0.68\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.80      0.71      0.75        17\n",
      "       missed       0.50      0.62      0.56         8\n",
      "\n",
      "    micro avg       0.68      0.68      0.68        25\n",
      "    macro avg       0.65      0.67      0.65        25\n",
      " weighted avg       0.70      0.68      0.69        25\n",
      "\n",
      "(7,)\n",
      "[-0.02270949  0.5680996   0.43343664  1.26457364  0.76854947  0.0331953\n",
      " -0.24754033]\n",
      "437101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 testing: 25\n",
      "correctsource    22\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.6        0.4        0.4        0.6        0.4\n",
      " 0.4       ]\n",
      "Accuracy: 0.4444444444444444\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.55      0.50      0.52        22\n",
      "       missed       0.31      0.36      0.33        14\n",
      "\n",
      "    micro avg       0.44      0.44      0.44        36\n",
      "    macro avg       0.43      0.43      0.43        36\n",
      " weighted avg       0.46      0.44      0.45        36\n",
      "\n",
      "accuracy = 0.52\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.50      0.57        16\n",
      "       missed       0.38      0.56      0.45         9\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        25\n",
      "    macro avg       0.53      0.53      0.51        25\n",
      " weighted avg       0.57      0.52      0.53        25\n",
      "\n",
      "(7,)\n",
      "[ 0.53917801  0.05748084 -0.55365404 -0.02593484  0.01218995 -1.01829591\n",
      " -0.09457873]\n",
      "439776\n",
      "training: 44 testing: 30\n",
      "correctsource    33\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    23\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.42857143 0.14285714 0.42857143 0.71428571 0.33333333 0.\n",
      " 0.6       ]\n",
      "Accuracy: 0.38636363636363635\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.65      0.39      0.49        33\n",
      "       missed       0.17      0.36      0.23        11\n",
      "\n",
      "    micro avg       0.39      0.39      0.39        44\n",
      "    macro avg       0.41      0.38      0.36        44\n",
      " weighted avg       0.53      0.39      0.43        44\n",
      "\n",
      "accuracy = 0.4666666666666667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       1.00      0.30      0.47        23\n",
      "       missed       0.30      1.00      0.47         7\n",
      "\n",
      "    micro avg       0.47      0.47      0.47        30\n",
      "    macro avg       0.65      0.65      0.47        30\n",
      " weighted avg       0.84      0.47      0.47        30\n",
      "\n",
      "(7,)\n",
      "[ 0.38745734 -0.52202426  0.10672184  0.12722243  0.48149371  0.42672442\n",
      "  0.04767469]\n",
      "458807\n",
      "training: 36 testing: 24\n",
      "correctsource    21\n",
      "missed           15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.2        0.8        0.6        0.4        0.6\n",
      " 0.6       ]\n",
      "Accuracy: 0.5555555555555556\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.71      0.65        21\n",
      "       missed       0.45      0.33      0.38        15\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        36\n",
      "    macro avg       0.53      0.52      0.52        36\n",
      " weighted avg       0.54      0.56      0.54        36\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.55      0.79      0.65        14\n",
      "       missed       0.25      0.10      0.14        10\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        24\n",
      "    macro avg       0.40      0.44      0.39        24\n",
      " weighted avg       0.43      0.50      0.44        24\n",
      "\n",
      "(7,)\n",
      "[ 0.47892263 -0.96950887  0.41435675 -1.09441985 -0.1437277  -0.54547994\n",
      " -0.38879262]\n",
      "459801\n",
      "training: 31 testing: 21\n",
      "correctsource    20\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    13\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6        0.2        0.4        0.6        0.25       0.5\n",
      " 0.33333333]\n",
      "Accuracy: 0.41935483870967744\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.56      0.50      0.53        20\n",
      "       missed       0.23      0.27      0.25        11\n",
      "\n",
      "    micro avg       0.42      0.42      0.42        31\n",
      "    macro avg       0.39      0.39      0.39        31\n",
      " weighted avg       0.44      0.42      0.43        31\n",
      "\n",
      "accuracy = 0.6666666666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.75      0.69      0.72        13\n",
      "       missed       0.56      0.62      0.59         8\n",
      "\n",
      "    micro avg       0.67      0.67      0.67        21\n",
      "    macro avg       0.65      0.66      0.65        21\n",
      " weighted avg       0.68      0.67      0.67        21\n",
      "\n",
      "(7,)\n",
      "[ 0.98835583 -0.97539057  0.29166934 -0.1138297   0.48359041  0.50679682\n",
      "  0.22465284]\n",
      "484204\n",
      "training: 39 testing: 26\n",
      "correctsource    22\n",
      "missed           17\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    15\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.42857143 0.66666667 0.5        0.8        0.8        0.4\n",
      " 0.8       ]\n",
      "Accuracy: 0.6153846153846154\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.63      0.77      0.69        22\n",
      "       missed       0.58      0.41      0.48        17\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        39\n",
      "    macro avg       0.61      0.59      0.59        39\n",
      " weighted avg       0.61      0.62      0.60        39\n",
      "\n",
      "accuracy = 0.6153846153846154\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.63      0.80      0.71        15\n",
      "       missed       0.57      0.36      0.44        11\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        26\n",
      "    macro avg       0.60      0.58      0.58        26\n",
      " weighted avg       0.61      0.62      0.60        26\n",
      "\n",
      "(7,)\n",
      "[-0.67548991 -0.4406575  -1.55865507 -0.09398259  0.3179967  -0.74267314\n",
      " -0.15032054]\n",
      "502616\n",
      "training: 31 testing: 21\n",
      "correctsource    21\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    15\n",
      "missed            6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.8  0.6  0.4  0.75 0.75 0.25 0.75]\n",
      "Accuracy: 0.6129032258064516\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.85      0.52      0.65        21\n",
      "       missed       0.44      0.80      0.57        10\n",
      "\n",
      "    micro avg       0.61      0.61      0.61        31\n",
      "    macro avg       0.65      0.66      0.61        31\n",
      " weighted avg       0.72      0.61      0.62        31\n",
      "\n",
      "accuracy = 0.5714285714285714\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.71      0.67      0.69        15\n",
      "       missed       0.29      0.33      0.31         6\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        21\n",
      "    macro avg       0.50      0.50      0.50        21\n",
      " weighted avg       0.59      0.57      0.58        21\n",
      "\n",
      "(7,)\n",
      "[ 0.04247316  0.96123262 -0.18068656 -0.74041807  0.26439361 -0.04361168\n",
      "  0.18593355]\n",
      "567214\n",
      "training: 29 testing: 20\n",
      "correctsource    17\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    12\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.4        0.4        0.6        0.75       0.5        0.33333333\n",
      " 0.66666667]\n",
      "Accuracy: 0.5172413793103449\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.53      0.56        17\n",
      "       missed       0.43      0.50      0.46        12\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        29\n",
      "    macro avg       0.51      0.51      0.51        29\n",
      " weighted avg       0.53      0.52      0.52        29\n",
      "\n",
      "accuracy = 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.70      0.58      0.64        12\n",
      "       missed       0.50      0.62      0.56         8\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        20\n",
      "    macro avg       0.60      0.60      0.60        20\n",
      " weighted avg       0.62      0.60      0.60        20\n",
      "\n",
      "(7,)\n",
      "[ 0.27779768 -1.13574528 -0.28588593  0.13347156 -0.18132762 -0.74343729\n",
      " -0.37026097]\n",
      "597569\n",
      "training: 39 testing: 27\n",
      "correctsource    28\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    20\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.33333333 0.5        0.33333333 0.4        0.2\n",
      " 1.        ]\n",
      "Accuracy: 0.46153846153846156\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.50      0.57        28\n",
      "       missed       0.22      0.36      0.28        11\n",
      "\n",
      "    micro avg       0.46      0.46      0.46        39\n",
      "    macro avg       0.44      0.43      0.42        39\n",
      " weighted avg       0.54      0.46      0.49        39\n",
      "\n",
      "accuracy = 0.5925925925925926\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.85      0.55      0.67        20\n",
      "       missed       0.36      0.71      0.48         7\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        27\n",
      "    macro avg       0.60      0.63      0.57        27\n",
      " weighted avg       0.72      0.59      0.62        27\n",
      "\n",
      "(7,)\n",
      "[-0.46577137 -0.24744318 -0.94030064 -0.78463274 -0.64981138  0.08888577\n",
      "  0.09866818]\n",
      "652850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 34 testing: 23\n",
      "missed           21\n",
      "correctsource    13\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           15\n",
      "correctsource     8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.4  1.   0.8  0.6  0.6  0.6  0.25]\n",
      "Accuracy: 0.6176470588235294\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.69      0.58        13\n",
      "       missed       0.75      0.57      0.65        21\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        34\n",
      "    macro avg       0.62      0.63      0.61        34\n",
      " weighted avg       0.65      0.62      0.62        34\n",
      "\n",
      "accuracy = 0.391304347826087\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.12      0.12      0.12         8\n",
      "       missed       0.53      0.53      0.53        15\n",
      "\n",
      "    micro avg       0.39      0.39      0.39        23\n",
      "    macro avg       0.33      0.33      0.33        23\n",
      " weighted avg       0.39      0.39      0.39        23\n",
      "\n",
      "(7,)\n",
      "[ 0.83891223 -0.2933732   0.52697091 -0.02304787  0.71811541  0.15995081\n",
      "  0.27127628]\n",
      "677561\n",
      "training: 32 testing: 22\n",
      "correctsource    20\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.2        0.2        0.4        0.4        0.4        0.5\n",
      " 0.33333333]\n",
      "Accuracy: 0.34375\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.47      0.45      0.46        20\n",
      "       missed       0.15      0.17      0.16        12\n",
      "\n",
      "    micro avg       0.34      0.34      0.34        32\n",
      "    macro avg       0.31      0.31      0.31        32\n",
      " weighted avg       0.35      0.34      0.35        32\n",
      "\n",
      "accuracy = 0.4090909090909091\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.56      0.36      0.43        14\n",
      "       missed       0.31      0.50      0.38         8\n",
      "\n",
      "    micro avg       0.41      0.41      0.41        22\n",
      "    macro avg       0.43      0.43      0.41        22\n",
      " weighted avg       0.47      0.41      0.42        22\n",
      "\n",
      "(7,)\n",
      "[-0.28101906 -0.02568913  0.43201043  0.81160429  0.97799296 -0.29823551\n",
      "  0.00943299]\n",
      "711830\n",
      "training: 36 testing: 25\n",
      "missed           21\n",
      "correctsource    15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           14\n",
      "correctsource    11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5 0.6 0.2 0.2 0.6 0.4 0.4]\n",
      "Accuracy: 0.4166666666666667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.31      0.33      0.32        15\n",
      "       missed       0.50      0.48      0.49        21\n",
      "\n",
      "    micro avg       0.42      0.42      0.42        36\n",
      "    macro avg       0.41      0.40      0.41        36\n",
      " weighted avg       0.42      0.42      0.42        36\n",
      "\n",
      "accuracy = 0.44\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.38      0.45      0.42        11\n",
      "       missed       0.50      0.43      0.46        14\n",
      "\n",
      "    micro avg       0.44      0.44      0.44        25\n",
      "    macro avg       0.44      0.44      0.44        25\n",
      " weighted avg       0.45      0.44      0.44        25\n",
      "\n",
      "(7,)\n",
      "[-0.1925552  -0.51957595  0.23776381 -0.21507473 -0.12255256 -0.42022418\n",
      " -0.88741835]\n",
      "729722\n",
      "training: 40 testing: 28\n",
      "correctsource    26\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    18\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.33333333 0.5        0.66666667 1.         0.6\n",
      " 0.8       ]\n",
      "Accuracy: 0.625\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.70      0.73      0.72        26\n",
      "       missed       0.46      0.43      0.44        14\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        40\n",
      "    macro avg       0.58      0.58      0.58        40\n",
      " weighted avg       0.62      0.62      0.62        40\n",
      "\n",
      "accuracy = 0.6071428571428571\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.68      0.72      0.70        18\n",
      "       missed       0.44      0.40      0.42        10\n",
      "\n",
      "    micro avg       0.61      0.61      0.61        28\n",
      "    macro avg       0.56      0.56      0.56        28\n",
      " weighted avg       0.60      0.61      0.60        28\n",
      "\n",
      "(7,)\n",
      "[-0.30059571 -0.41433886 -0.21641197 -0.97105449 -0.07182247 -0.31200326\n",
      "  0.24767537]\n",
      "739694\n",
      "training: 32 testing: 22\n",
      "correctsource    20\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6        0.6        0.4        0.6        0.6        0.5\n",
      " 0.66666667]\n",
      "Accuracy: 0.5625\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.69      0.55      0.61        20\n",
      "       missed       0.44      0.58      0.50        12\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        32\n",
      "    macro avg       0.56      0.57      0.56        32\n",
      " weighted avg       0.59      0.56      0.57        32\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.64      0.62        14\n",
      "       missed       0.29      0.25      0.27         8\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        22\n",
      "    macro avg       0.44      0.45      0.44        22\n",
      " weighted avg       0.49      0.50      0.49        22\n",
      "\n",
      "(7,)\n",
      "[ 0.90189838  0.07421818  0.65322983  0.20178463  0.15662845 -0.41233017\n",
      " -1.24862421]\n",
      "748676\n",
      "training: 36 testing: 24\n",
      "missed           26\n",
      "correctsource    10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           18\n",
      "correctsource     6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.5        0.5        0.8        0.2        0.75\n",
      " 0.5       ]\n",
      "Accuracy: 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.25      0.40      0.31        10\n",
      "       missed       0.70      0.54      0.61        26\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        36\n",
      "    macro avg       0.47      0.47      0.46        36\n",
      " weighted avg       0.57      0.50      0.53        36\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.20      0.33      0.25         6\n",
      "       missed       0.71      0.56      0.63        18\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        24\n",
      "    macro avg       0.46      0.44      0.44        24\n",
      " weighted avg       0.59      0.50      0.53        24\n",
      "\n",
      "(7,)\n",
      "[-1.22115132 -0.01683453 -0.34501978 -0.24550127 -0.12509833 -0.80719679\n",
      " -0.58995902]\n",
      "778749\n",
      "training: 33 testing: 22\n",
      "missed           19\n",
      "correctsource    14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           13\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6 0.2 0.4 0.6 0.6 0.5 0.5]\n",
      "Accuracy: 0.48484848484848486\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.38      0.36      0.37        14\n",
      "       missed       0.55      0.58      0.56        19\n",
      "\n",
      "    micro avg       0.48      0.48      0.48        33\n",
      "    macro avg       0.47      0.47      0.47        33\n",
      " weighted avg       0.48      0.48      0.48        33\n",
      "\n",
      "accuracy = 0.36363636363636365\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.31      0.44      0.36         9\n",
      "       missed       0.44      0.31      0.36        13\n",
      "\n",
      "    micro avg       0.36      0.36      0.36        22\n",
      "    macro avg       0.38      0.38      0.36        22\n",
      " weighted avg       0.39      0.36      0.36        22\n",
      "\n",
      "(7,)\n",
      "[ 0.8569687  -0.00317735  0.36446351 -0.6879287   1.3774682  -0.1372249\n",
      "  1.01905291]\n",
      "783781\n",
      "training: 34 testing: 23\n",
      "correctsource    24\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.83333333 0.5        0.5        0.25       0.5        1.\n",
      " 0.5       ]\n",
      "Accuracy: 0.5882352941176471\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.73      0.67      0.70        24\n",
      "       missed       0.33      0.40      0.36        10\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        34\n",
      "    macro avg       0.53      0.53      0.53        34\n",
      " weighted avg       0.61      0.59      0.60        34\n",
      "\n",
      "accuracy = 0.5652173913043478\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.71      0.62      0.67        16\n",
      "       missed       0.33      0.43      0.38         7\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        23\n",
      "    macro avg       0.52      0.53      0.52        23\n",
      " weighted avg       0.60      0.57      0.58        23\n",
      "\n",
      "(7,)\n",
      "[ 1.30658209 -0.38085659  0.89128984  0.43102576  1.06620157 -0.19385697\n",
      " -0.30766955]\n",
      "884343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 32 testing: 22\n",
      "correctsource    17\n",
      "missed           15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    12\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.4        0.2        0.         0.5        0.5\n",
      " 0.25      ]\n",
      "Accuracy: 0.3125\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.33      0.29      0.31        17\n",
      "       missed       0.29      0.33      0.31        15\n",
      "\n",
      "    micro avg       0.31      0.31      0.31        32\n",
      "    macro avg       0.31      0.31      0.31        32\n",
      " weighted avg       0.31      0.31      0.31        32\n",
      "\n",
      "accuracy = 0.45454545454545453\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.42      0.45        12\n",
      "       missed       0.42      0.50      0.45        10\n",
      "\n",
      "    micro avg       0.45      0.45      0.45        22\n",
      "    macro avg       0.46      0.46      0.45        22\n",
      " weighted avg       0.46      0.45      0.45        22\n",
      "\n",
      "(7,)\n",
      "[ 0.84794208  0.02318715  0.35861866  0.16087331 -0.30883464 -0.11007593\n",
      "  0.18425098]\n",
      "886007\n",
      "training: 39 testing: 26\n",
      "correctsource    27\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    18\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.33333333 0.5        0.5        0.5        0.2\n",
      " 0.5       ]\n",
      "Accuracy: 0.4358974358974359\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.48      0.54        27\n",
      "       missed       0.22      0.33      0.27        12\n",
      "\n",
      "    micro avg       0.44      0.44      0.44        39\n",
      "    macro avg       0.42      0.41      0.40        39\n",
      " weighted avg       0.50      0.44      0.46        39\n",
      "\n",
      "accuracy = 0.6538461538461539\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.80      0.67      0.73        18\n",
      "       missed       0.45      0.62      0.53         8\n",
      "\n",
      "    micro avg       0.65      0.65      0.65        26\n",
      "    macro avg       0.63      0.65      0.63        26\n",
      " weighted avg       0.69      0.65      0.67        26\n",
      "\n",
      "(7,)\n",
      "[-0.51001271 -0.38967195  0.46898469  0.50383515  0.43514114  0.181427\n",
      " -0.10389488]\n",
      "936730\n",
      "training: 36 testing: 24\n",
      "missed           18\n",
      "correctsource    18\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           12\n",
      "correctsource    12\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5 0.5 0.5 0.5 0.5 0.  0.5]\n",
      "Accuracy: 0.4444444444444444\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.47      0.89      0.62        18\n",
      "       missed       0.00      0.00      0.00        18\n",
      "\n",
      "    micro avg       0.44      0.44      0.44        36\n",
      "    macro avg       0.24      0.44      0.31        36\n",
      " weighted avg       0.24      0.44      0.31        36\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      1.00      0.67        12\n",
      "       missed       0.00      0.00      0.00        12\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        24\n",
      "    macro avg       0.25      0.50      0.33        24\n",
      " weighted avg       0.25      0.50      0.33        24\n",
      "\n",
      "(7,)\n",
      "[-0.02909518  0.102104   -0.0853547   0.05192029  0.04474809 -0.0432548\n",
      "  0.08550915]\n",
      "956130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/may19_py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 38 testing: 26\n",
      "missed           20\n",
      "correctsource    18\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           13\n",
      "correctsource    13\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.16666667 0.33333333 0.5        0.5        0.4        0.4\n",
      " 0.        ]\n",
      "Accuracy: 0.34210526315789475\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.33      0.39      0.36        18\n",
      "       missed       0.35      0.30      0.32        20\n",
      "\n",
      "    micro avg       0.34      0.34      0.34        38\n",
      "    macro avg       0.34      0.34      0.34        38\n",
      " weighted avg       0.34      0.34      0.34        38\n",
      "\n",
      "accuracy = 0.6153846153846154\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.62      0.62        13\n",
      "       missed       0.62      0.62      0.62        13\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        26\n",
      "    macro avg       0.62      0.62      0.62        26\n",
      " weighted avg       0.62      0.62      0.62        26\n",
      "\n",
      "(7,)\n",
      "[ 0.74168596 -0.22264379  0.87630251  0.09043066  0.99685195  0.46027023\n",
      " -0.1682125 ]\n",
      "983291\n",
      "training: 40 testing: 28\n",
      "correctsource    29\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    21\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.57142857 0.5        0.5        0.16666667 0.6        0.8\n",
      " 0.2       ]\n",
      "Accuracy: 0.475\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.55      0.60        29\n",
      "       missed       0.19      0.27      0.22        11\n",
      "\n",
      "    micro avg       0.47      0.47      0.48        40\n",
      "    macro avg       0.43      0.41      0.41        40\n",
      " weighted avg       0.53      0.47      0.50        40\n",
      "\n",
      "accuracy = 0.5714285714285714\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.85      0.52      0.65        21\n",
      "       missed       0.33      0.71      0.45         7\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        28\n",
      "    macro avg       0.59      0.62      0.55        28\n",
      " weighted avg       0.72      0.57      0.60        28\n",
      "\n",
      "(7,)\n",
      "[-0.7590702   0.59602433 -0.34288083  1.11601786 -0.57680771  0.01760704\n",
      "  0.48767597]\n",
      "998166\n",
      "training: 37 testing: 26\n",
      "correctsource    21\n",
      "missed           16\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.16666667 0.5        0.4        0.4        0.4        0.4\n",
      " 0.6       ]\n",
      "Accuracy: 0.40540540540540543\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.47      0.43      0.45        21\n",
      "       missed       0.33      0.38      0.35        16\n",
      "\n",
      "    micro avg       0.41      0.41      0.41        37\n",
      "    macro avg       0.40      0.40      0.40        37\n",
      " weighted avg       0.41      0.41      0.41        37\n",
      "\n",
      "accuracy = 0.38461538461538464\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.43      0.43      0.43        14\n",
      "       missed       0.33      0.33      0.33        12\n",
      "\n",
      "    micro avg       0.38      0.38      0.38        26\n",
      "    macro avg       0.38      0.38      0.38        26\n",
      " weighted avg       0.38      0.38      0.38        26\n",
      "\n",
      "(7,)\n",
      "[0.3925812  0.6489625  0.60203721 0.6135403  0.48967417 1.51984334\n",
      " 0.1139594 ]\n",
      "108391\n",
      "training: 34 testing: 24\n",
      "correctsource    25\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    18\n",
      "missed            6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.16666667 0.16666667 0.6        0.6        0.75       0.5\n",
      " 0.25      ]\n",
      "Accuracy: 0.4117647058823529\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.65      0.44      0.52        25\n",
      "       missed       0.18      0.33      0.23         9\n",
      "\n",
      "    micro avg       0.41      0.41      0.41        34\n",
      "    macro avg       0.41      0.39      0.38        34\n",
      " weighted avg       0.52      0.41      0.45        34\n",
      "\n",
      "accuracy = 0.5833333333333334\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.83      0.56      0.67        18\n",
      "       missed       0.33      0.67      0.44         6\n",
      "\n",
      "    micro avg       0.58      0.58      0.58        24\n",
      "    macro avg       0.58      0.61      0.56        24\n",
      " weighted avg       0.71      0.58      0.61        24\n",
      "\n",
      "(20,)\n",
      "[ 1.00140731  0.36719131  0.42453235 -0.8900055  -0.40352281  0.5887293\n",
      " -0.70820724  0.94272134 -0.39499846 -0.2828627  -0.39584649 -0.43843445\n",
      " -0.13716097 -0.03018426 -0.73126067 -0.01208933  0.05447033 -1.45099137\n",
      "  0.13672852  0.55114937]\n",
      "122922\n",
      "training: 28 testing: 20\n",
      "correctsource    15\n",
      "missed           13\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           10\n",
      "correctsource    10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6  0.75 0.5  0.25 0.5  0.5  0.  ]\n",
      "Accuracy: 0.4642857142857143\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.33      0.40        15\n",
      "       missed       0.44      0.62      0.52        13\n",
      "\n",
      "    micro avg       0.46      0.46      0.46        28\n",
      "    macro avg       0.47      0.47      0.46        28\n",
      " weighted avg       0.47      0.46      0.45        28\n",
      "\n",
      "accuracy = 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.50      0.56        10\n",
      "       missed       0.58      0.70      0.64        10\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        20\n",
      "    macro avg       0.60      0.60      0.60        20\n",
      " weighted avg       0.60      0.60      0.60        20\n",
      "\n",
      "(20,)\n",
      "[ 0.33247569 -0.21491323 -0.57283961 -0.9766631  -0.34908332 -0.8905284\n",
      " -0.90452637 -0.52727695  0.25460066 -0.43044171  0.04289615  0.55184887\n",
      "  0.56947981 -0.04403523  0.26312425 -1.5578044   0.54411899 -0.4973936\n",
      " -0.39300745  0.33499377]\n",
      "139593\n",
      "training: 33 testing: 23\n",
      "missed           24\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           17\n",
      "correctsource     6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.83333333 0.5        1.         1.         1.         0.5\n",
      " 1.        ]\n",
      "Accuracy: 0.8181818181818182\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.64      0.78      0.70         9\n",
      "       missed       0.91      0.83      0.87        24\n",
      "\n",
      "    micro avg       0.82      0.82      0.82        33\n",
      "    macro avg       0.77      0.81      0.78        33\n",
      " weighted avg       0.83      0.82      0.82        33\n",
      "\n",
      "accuracy = 0.391304347826087\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.21      0.50      0.30         6\n",
      "       missed       0.67      0.35      0.46        17\n",
      "\n",
      "    micro avg       0.39      0.39      0.39        23\n",
      "    macro avg       0.44      0.43      0.38        23\n",
      " weighted avg       0.55      0.39      0.42        23\n",
      "\n",
      "(20,)\n",
      "[-0.8225431   0.18173646  0.20017201  0.24646696  0.21142299 -0.07306883\n",
      "  0.00728045 -0.14114113 -0.46557844  0.62169544 -0.64081853 -0.59752523\n",
      "  0.50864552  1.3013629  -0.39080441  0.7285835   0.2168834   0.13910396\n",
      "  0.23439299 -0.32247024]\n",
      "164965\n",
      "training: 39 testing: 27\n",
      "correctsource    24\n",
      "missed           15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    17\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.57142857 0.83333333 0.66666667 0.2        0.6        0.4\n",
      " 0.6       ]\n",
      "Accuracy: 0.5641025641025641\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.58      0.62        24\n",
      "       missed       0.44      0.53      0.48        15\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        39\n",
      "    macro avg       0.56      0.56      0.55        39\n",
      " weighted avg       0.58      0.56      0.57        39\n",
      "\n",
      "accuracy = 0.7037037037037037\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.80      0.71      0.75        17\n",
      "       missed       0.58      0.70      0.64        10\n",
      "\n",
      "    micro avg       0.70      0.70      0.70        27\n",
      "    macro avg       0.69      0.70      0.69        27\n",
      " weighted avg       0.72      0.70      0.71        27\n",
      "\n",
      "(20,)\n",
      "[ 0.43103284  1.09387967 -0.08787284  0.43703818  0.33757873  0.74960722\n",
      " -0.17174198  0.14948596 -0.47485267  0.74311311 -0.59659154  0.13703993\n",
      " -0.1515632  -0.04204397  0.06887533  0.17003106  0.38167788 -0.06043587\n",
      "  1.65243144  0.81710023]\n",
      "199801\n",
      "training: 20 testing: 14\n",
      "missed           11\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           8\n",
      "correctsource    6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.75       0.5        0.         0.66666667 0.5        0.5\n",
      " 0.5       ]\n",
      "Accuracy: 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.44      0.44      0.44         9\n",
      "       missed       0.55      0.55      0.55        11\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        20\n",
      "    macro avg       0.49      0.49      0.49        20\n",
      " weighted avg       0.50      0.50      0.50        20\n",
      "\n",
      "accuracy = 0.2857142857142857\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.25      0.33      0.29         6\n",
      "       missed       0.33      0.25      0.29         8\n",
      "\n",
      "    micro avg       0.29      0.29      0.29        14\n",
      "    macro avg       0.29      0.29      0.29        14\n",
      " weighted avg       0.30      0.29      0.29        14\n",
      "\n",
      "(20,)\n",
      "[ 0.13524478  0.33422053  0.07780158  0.29817803 -0.83577966  0.41099073\n",
      " -0.33122371  0.49897598 -0.52379018  0.50148777  0.24604057 -0.96175836\n",
      " -0.83751204  0.75383503  0.43295343 -0.1349059  -0.31865665  1.18608041\n",
      " -0.36550311  0.66869609]\n",
      "247659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 34 testing: 23\n",
      "missed           25\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           17\n",
      "correctsource     6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.5        0.4        0.6        0.5        0.5\n",
      " 0.75      ]\n",
      "Accuracy: 0.5588235294117647\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.20      0.22      0.21         9\n",
      "       missed       0.71      0.68      0.69        25\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        34\n",
      "    macro avg       0.45      0.45      0.45        34\n",
      " weighted avg       0.57      0.56      0.57        34\n",
      "\n",
      "accuracy = 0.782608695652174\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.57      0.67      0.62         6\n",
      "       missed       0.88      0.82      0.85        17\n",
      "\n",
      "    micro avg       0.78      0.78      0.78        23\n",
      "    macro avg       0.72      0.75      0.73        23\n",
      " weighted avg       0.80      0.78      0.79        23\n",
      "\n",
      "(20,)\n",
      "[ 0.0604246  -0.35679319 -0.29919692  0.00700922  0.38375484  0.39663582\n",
      " -0.41077385 -0.11583305 -0.5643577  -0.49904849  0.55002426 -1.06881869\n",
      " -0.51006967  0.58866723  0.89730988 -0.9748694  -1.65917057  0.01369263\n",
      " -0.37863566  0.03375521]\n",
      "255499\n",
      "training: 39 testing: 27\n",
      "correctsource    23\n",
      "missed           16\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.42857143 0.42857143 0.6        0.6        0.6        0.6\n",
      " 0.6       ]\n",
      "Accuracy: 0.5384615384615384\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.65      0.63        23\n",
      "       missed       0.43      0.38      0.40        16\n",
      "\n",
      "    micro avg       0.54      0.54      0.54        39\n",
      "    macro avg       0.51      0.51      0.51        39\n",
      " weighted avg       0.53      0.54      0.53        39\n",
      "\n",
      "accuracy = 0.37037037037037035\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.46      0.38      0.41        16\n",
      "       missed       0.29      0.36      0.32        11\n",
      "\n",
      "    micro avg       0.37      0.37      0.37        27\n",
      "    macro avg       0.37      0.37      0.37        27\n",
      " weighted avg       0.39      0.37      0.38        27\n",
      "\n",
      "(20,)\n",
      "[-0.10934483  0.08943262 -0.05097201 -0.42761892  0.04881168 -0.00872665\n",
      "  0.16727957 -0.02658926 -0.21623251 -0.00191724  0.06445002 -0.00431743\n",
      "  0.06402365  0.02110589 -0.12171308  0.01070731  0.16661798  0.1399346\n",
      "  0.01807336  0.08106784]\n",
      "258618\n",
      "training: 36 testing: 25\n",
      "correctsource    20\n",
      "missed           16\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.66666667 0.4        0.4        0.8        0.4\n",
      " 0.5       ]\n",
      "Accuracy: 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.56      0.50      0.53        20\n",
      "       missed       0.44      0.50      0.47        16\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        36\n",
      "    macro avg       0.50      0.50      0.50        36\n",
      " weighted avg       0.51      0.50      0.50        36\n",
      "\n",
      "accuracy = 0.56\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.58      0.79      0.67        14\n",
      "       missed       0.50      0.27      0.35        11\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        25\n",
      "    macro avg       0.54      0.53      0.51        25\n",
      " weighted avg       0.54      0.56      0.53        25\n",
      "\n",
      "(20,)\n",
      "[-0.36832061  0.25909125  0.84832183  0.67719648 -0.46185806 -0.71499904\n",
      "  0.08603575 -0.5364787   0.62035354  0.11876147  0.64477113  0.76257306\n",
      " -0.89643988  0.32294437 -0.10138703 -1.30913333  0.67862518  0.60161363\n",
      "  0.39682724  0.52588993]\n",
      "271596\n",
      "training: 36 testing: 25\n",
      "correctsource    22\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.  1.  0.4 0.4 0.8 0.6 0.8]\n",
      "Accuracy: 0.5555555555555556\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.65      0.59      0.62        22\n",
      "       missed       0.44      0.50      0.47        14\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        36\n",
      "    macro avg       0.54      0.55      0.54        36\n",
      " weighted avg       0.57      0.56      0.56        36\n",
      "\n",
      "accuracy = 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.69      0.69      0.69        16\n",
      "       missed       0.44      0.44      0.44         9\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        25\n",
      "    macro avg       0.57      0.57      0.57        25\n",
      " weighted avg       0.60      0.60      0.60        25\n",
      "\n",
      "(20,)\n",
      "[ 0.35398081  0.61842057 -0.94652931 -1.1514965   0.28717706  0.17115607\n",
      " -0.47457331 -0.57628389 -0.24088912  0.39739332 -0.97258332  0.53928595\n",
      " -0.79209075  0.08081534  0.23554582  0.03197115  1.4174317   0.2801199\n",
      "  0.39754233 -0.0747608 ]\n",
      "314409\n",
      "training: 40 testing: 28\n",
      "correctsource    21\n",
      "missed           19\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           14\n",
      "correctsource    14\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.5        0.33333333 0.83333333 0.5        0.6\n",
      " 1.        ]\n",
      "Accuracy: 0.625\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.64      0.67      0.65        21\n",
      "       missed       0.61      0.58      0.59        19\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        40\n",
      "    macro avg       0.62      0.62      0.62        40\n",
      " weighted avg       0.62      0.62      0.62        40\n",
      "\n",
      "accuracy = 0.39285714285714285\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.40      0.43      0.41        14\n",
      "       missed       0.38      0.36      0.37        14\n",
      "\n",
      "    micro avg       0.39      0.39      0.39        28\n",
      "    macro avg       0.39      0.39      0.39        28\n",
      " weighted avg       0.39      0.39      0.39        28\n",
      "\n",
      "(20,)\n",
      "[-0.70017093  0.9430142   0.38297459 -1.48095235  0.13614977 -0.03625595\n",
      "  0.29284575 -0.39826746 -0.2449912  -0.10625429  0.63174572 -0.20974206\n",
      " -0.9257593   1.29220056 -0.18564271  0.25545895  0.39443391  0.61365034\n",
      " -0.77323198 -0.45485755]\n",
      "336665\n",
      "training: 30 testing: 20\n",
      "correctsource    16\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    11\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.8  0.4  0.25 0.5  0.5  0.5  0.75]\n",
      "Accuracy: 0.5333333333333333\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.56      0.62      0.59        16\n",
      "       missed       0.50      0.43      0.46        14\n",
      "\n",
      "    micro avg       0.53      0.53      0.53        30\n",
      "    macro avg       0.53      0.53      0.52        30\n",
      " weighted avg       0.53      0.53      0.53        30\n",
      "\n",
      "accuracy = 0.55\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.36      0.47        11\n",
      "       missed       0.50      0.78      0.61         9\n",
      "\n",
      "    micro avg       0.55      0.55      0.55        20\n",
      "    macro avg       0.58      0.57      0.54        20\n",
      " weighted avg       0.59      0.55      0.53        20\n",
      "\n",
      "(20,)\n",
      "[ 0.1294788   0.70054815 -0.18151535 -0.07332797  0.34123954  0.8103141\n",
      " -0.47038996 -0.17281709  0.46956976 -0.73561119  0.58123752  0.99840426\n",
      "  0.24388572  0.85866015 -0.19957628 -0.24468341  0.26666966 -0.70338085\n",
      " -0.04458156  0.03622739]\n",
      "337021\n",
      "training: 34 testing: 23\n",
      "correctsource    24\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.33333333 0.66666667 0.5        0.5        0.5\n",
      " 1.        ]\n",
      "Accuracy: 0.5588235294117647\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.71      0.62      0.67        24\n",
      "       missed       0.31      0.40      0.35        10\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        34\n",
      "    macro avg       0.51      0.51      0.51        34\n",
      " weighted avg       0.59      0.56      0.57        34\n",
      "\n",
      "accuracy = 0.6521739130434783\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.75      0.75      0.75        16\n",
      "       missed       0.43      0.43      0.43         7\n",
      "\n",
      "    micro avg       0.65      0.65      0.65        23\n",
      "    macro avg       0.59      0.59      0.59        23\n",
      " weighted avg       0.65      0.65      0.65        23\n",
      "\n",
      "(20,)\n",
      "[-0.32502527 -0.14745357  0.12564129 -0.09079354  1.16623691 -0.21197395\n",
      " -0.32752996 -0.43124029 -0.06581756 -0.36272134 -0.28033929 -0.01001925\n",
      "  0.03219932  0.83876295  1.03040987 -0.33040083  0.96386428 -0.3156979\n",
      "  1.35391827 -0.1012236 ]\n",
      "396250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 39 testing: 27\n",
      "missed           21\n",
      "correctsource    18\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           14\n",
      "correctsource    13\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.83333333 0.66666667 0.66666667 0.33333333 0.2        0.2\n",
      " 0.8       ]\n",
      "Accuracy: 0.5384615384615384\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.67      0.57        18\n",
      "       missed       0.60      0.43      0.50        21\n",
      "\n",
      "    micro avg       0.54      0.54      0.54        39\n",
      "    macro avg       0.55      0.55      0.54        39\n",
      " weighted avg       0.55      0.54      0.53        39\n",
      "\n",
      "accuracy = 0.7777777777777778\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.77      0.77      0.77        13\n",
      "       missed       0.79      0.79      0.79        14\n",
      "\n",
      "    micro avg       0.78      0.78      0.78        27\n",
      "    macro avg       0.78      0.78      0.78        27\n",
      " weighted avg       0.78      0.78      0.78        27\n",
      "\n",
      "(20,)\n",
      "[-0.17813572 -0.30052231 -0.73105373  0.03514235 -0.31332848  0.0760731\n",
      " -0.91138605 -0.99014002 -0.87557319 -0.29344962  0.76190913 -1.24340636\n",
      " -0.78225828  0.4611592  -0.4910696  -0.64192025 -0.6240971   0.43638544\n",
      "  0.782132    0.07365103]\n",
      "403131\n",
      "training: 42 testing: 28\n",
      "correctsource    23\n",
      "missed           19\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.57142857 0.71428571 0.66666667 0.66666667 0.83333333 0.8\n",
      " 0.6       ]\n",
      "Accuracy: 0.6904761904761905\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.73      0.70      0.71        23\n",
      "       missed       0.65      0.68      0.67        19\n",
      "\n",
      "    micro avg       0.69      0.69      0.69        42\n",
      "    macro avg       0.69      0.69      0.69        42\n",
      " weighted avg       0.69      0.69      0.69        42\n",
      "\n",
      "accuracy = 0.5357142857142857\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.56      0.58        16\n",
      "       missed       0.46      0.50      0.48        12\n",
      "\n",
      "    micro avg       0.54      0.54      0.54        28\n",
      "    macro avg       0.53      0.53      0.53        28\n",
      " weighted avg       0.54      0.54      0.54        28\n",
      "\n",
      "(20,)\n",
      "[-1.23883473 -0.09507172  0.30545241  0.95618045 -0.5595334  -0.16464436\n",
      " -0.2228076  -0.18568365 -0.65144555  1.33929714  0.14995262  0.3395819\n",
      " -0.08059629 -0.29920027 -0.53954091 -0.33975572 -0.96413132  0.08434731\n",
      "  0.26692601  0.88849902]\n",
      "408506\n",
      "training: 30 testing: 21\n",
      "missed           16\n",
      "correctsource    14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           11\n",
      "correctsource    10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.8  0.4  0.5  0.75 1.   0.5  0.75]\n",
      "Accuracy: 0.6666666666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.70      0.50      0.58        14\n",
      "       missed       0.65      0.81      0.72        16\n",
      "\n",
      "    micro avg       0.67      0.67      0.67        30\n",
      "    macro avg       0.68      0.66      0.65        30\n",
      " weighted avg       0.67      0.67      0.66        30\n",
      "\n",
      "accuracy = 0.47619047619047616\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.43      0.30      0.35        10\n",
      "       missed       0.50      0.64      0.56        11\n",
      "\n",
      "    micro avg       0.48      0.48      0.48        21\n",
      "    macro avg       0.46      0.47      0.46        21\n",
      " weighted avg       0.47      0.48      0.46        21\n",
      "\n",
      "(20,)\n",
      "[ 0.12964952 -0.04708299 -0.44218699  0.05417552 -0.83349883 -0.49699035\n",
      "  0.234438   -0.12455024 -0.13611483 -0.44694713 -0.79814904 -0.42424203\n",
      "  1.20877488 -0.54643372  0.50732775 -0.34717084 -1.64692252 -0.04035527\n",
      " -0.27865844 -0.60651597]\n",
      "413474\n",
      "training: 37 testing: 25\n",
      "correctsource    25\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    17\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.66666667 0.83333333 0.66666667 1.         0.5\n",
      " 0.75      ]\n",
      "Accuracy: 0.7297297297297297\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.83      0.76      0.79        25\n",
      "       missed       0.57      0.67      0.62        12\n",
      "\n",
      "    micro avg       0.73      0.73      0.73        37\n",
      "    macro avg       0.70      0.71      0.70        37\n",
      " weighted avg       0.74      0.73      0.73        37\n",
      "\n",
      "accuracy = 0.52\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.69      0.53      0.60        17\n",
      "       missed       0.33      0.50      0.40         8\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        25\n",
      "    macro avg       0.51      0.51      0.50        25\n",
      " weighted avg       0.58      0.52      0.54        25\n",
      "\n",
      "(20,)\n",
      "[-0.60165731 -0.42210429  0.71136664 -0.09738858 -0.33203361  0.44587396\n",
      "  0.00784646 -0.33941679  0.66909855 -0.1138001  -0.51833466  0.78196492\n",
      " -0.37564489  1.25336702  0.68646251 -0.87298252  0.33714145 -0.26589094\n",
      "  0.19762933  0.27071974]\n",
      "437101\n",
      "training: 36 testing: 25\n",
      "correctsource    22\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.8        0.8        0.8        1.         0.6\n",
      " 0.6       ]\n",
      "Accuracy: 0.6944444444444444\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.76      0.73      0.74        22\n",
      "       missed       0.60      0.64      0.62        14\n",
      "\n",
      "    micro avg       0.69      0.69      0.69        36\n",
      "    macro avg       0.68      0.69      0.68        36\n",
      " weighted avg       0.70      0.69      0.70        36\n",
      "\n",
      "accuracy = 0.64\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.71      0.75      0.73        16\n",
      "       missed       0.50      0.44      0.47         9\n",
      "\n",
      "    micro avg       0.64      0.64      0.64        25\n",
      "    macro avg       0.60      0.60      0.60        25\n",
      " weighted avg       0.63      0.64      0.63        25\n",
      "\n",
      "(20,)\n",
      "[-3.21002735e-01  2.70756033e-01  2.26800565e-01  1.10153457e+00\n",
      " -4.16324875e-01  1.72525353e-02 -9.58352419e-01 -5.52637282e-04\n",
      "  2.08856569e-01 -4.37932278e-01 -4.25172814e-01  1.19349393e-01\n",
      " -2.00570372e-02 -4.89741238e-01 -2.25321019e-01 -3.39550868e-01\n",
      "  9.80584840e-01 -4.30365819e-01  2.29661127e-01  5.33684009e-01]\n",
      "439776\n",
      "training: 44 testing: 30\n",
      "correctsource    33\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    23\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.85714286 0.57142857 0.85714286 0.71428571 0.66666667 0.6\n",
      " 0.8       ]\n",
      "Accuracy: 0.7272727272727273\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.84      0.79      0.81        33\n",
      "       missed       0.46      0.55      0.50        11\n",
      "\n",
      "    micro avg       0.73      0.73      0.73        44\n",
      "    macro avg       0.65      0.67      0.66        44\n",
      " weighted avg       0.74      0.73      0.73        44\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.72      0.57      0.63        23\n",
      "       missed       0.17      0.29      0.21         7\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        30\n",
      "    macro avg       0.44      0.43      0.42        30\n",
      " weighted avg       0.59      0.50      0.54        30\n",
      "\n",
      "(20,)\n",
      "[ 0.5914916   0.39819838 -0.44248872  0.16194772 -0.68921396 -0.20465213\n",
      "  0.20828499 -0.09524535 -0.47826158 -1.31739984 -0.80985656 -0.3597987\n",
      "  0.59046136  0.69686471 -0.1563753  -0.03350926 -0.56321367  0.82383526\n",
      " -1.04763761 -0.16322504]\n",
      "458807\n",
      "training: 36 testing: 24\n",
      "correctsource    21\n",
      "missed           15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5 0.6 0.2 0.6 0.4 0.6 0.4]\n",
      "Accuracy: 0.4722222222222222\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.55      0.57      0.56        21\n",
      "       missed       0.36      0.33      0.34        15\n",
      "\n",
      "    micro avg       0.47      0.47      0.47        36\n",
      "    macro avg       0.45      0.45      0.45        36\n",
      " weighted avg       0.47      0.47      0.47        36\n",
      "\n",
      "accuracy = 0.6666666666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.71      0.71      0.71        14\n",
      "       missed       0.60      0.60      0.60        10\n",
      "\n",
      "    micro avg       0.67      0.67      0.67        24\n",
      "    macro avg       0.66      0.66      0.66        24\n",
      " weighted avg       0.67      0.67      0.67        24\n",
      "\n",
      "(20,)\n",
      "[-0.26285258 -0.00942022  0.08189987  0.29966239 -0.28307189 -1.26354474\n",
      " -0.30900746 -0.24622624  0.33373595  1.34331122  0.4333533   0.11107118\n",
      " -0.70121694 -0.64269799 -0.58767404 -0.82942242  1.02336823  0.78981636\n",
      "  0.48556342  0.16968315]\n",
      "459801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 31 testing: 21\n",
      "correctsource    20\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    13\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.2        0.6        0.2        0.2        1.         0.75\n",
      " 0.66666667]\n",
      "Accuracy: 0.4838709677419355\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.60      0.60        20\n",
      "       missed       0.27      0.27      0.27        11\n",
      "\n",
      "    micro avg       0.48      0.48      0.48        31\n",
      "    macro avg       0.44      0.44      0.44        31\n",
      " weighted avg       0.48      0.48      0.48        31\n",
      "\n",
      "accuracy = 0.7619047619047619\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.79      0.85      0.81        13\n",
      "       missed       0.71      0.62      0.67         8\n",
      "\n",
      "    micro avg       0.76      0.76      0.76        21\n",
      "    macro avg       0.75      0.74      0.74        21\n",
      " weighted avg       0.76      0.76      0.76        21\n",
      "\n",
      "(20,)\n",
      "[ 0.00909458 -0.07259715  0.91797882  1.69617243  0.46802361  0.03518441\n",
      "  0.61590285  0.52174387  0.109731   -0.0112181   1.1013743  -0.41700012\n",
      " -0.14422955  0.5502833  -0.10727102 -0.12870109 -0.00577947 -0.49551727\n",
      "  1.1494193   0.92485096]\n",
      "484204\n",
      "training: 39 testing: 26\n",
      "correctsource    22\n",
      "missed           17\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    15\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.71428571 0.33333333 0.33333333 0.8        1.         0.6\n",
      " 0.8       ]\n",
      "Accuracy: 0.6410256410256411\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.73      0.70        22\n",
      "       missed       0.60      0.53      0.56        17\n",
      "\n",
      "    micro avg       0.64      0.64      0.64        39\n",
      "    macro avg       0.63      0.63      0.63        39\n",
      " weighted avg       0.64      0.64      0.64        39\n",
      "\n",
      "accuracy = 0.7692307692307693\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.80      0.80      0.80        15\n",
      "       missed       0.73      0.73      0.73        11\n",
      "\n",
      "    micro avg       0.77      0.77      0.77        26\n",
      "    macro avg       0.76      0.76      0.76        26\n",
      " weighted avg       0.77      0.77      0.77        26\n",
      "\n",
      "(20,)\n",
      "[-0.24680008 -0.55807    -0.5624345  -0.87403821 -0.68101468 -0.51986648\n",
      "  0.80092109  0.13298296 -0.4205264   0.34349435 -0.733247    0.19741752\n",
      " -0.63172749  0.91795929 -0.20824979 -0.21101062  0.55394168 -0.08040739\n",
      " -0.97031635  0.42055066]\n",
      "502616\n",
      "training: 31 testing: 21\n",
      "correctsource    21\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    15\n",
      "missed            6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[1.   0.4  1.   0.5  1.   0.25 0.75]\n",
      "Accuracy: 0.7096774193548387\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.80      0.76      0.78        21\n",
      "       missed       0.55      0.60      0.57        10\n",
      "\n",
      "    micro avg       0.71      0.71      0.71        31\n",
      "    macro avg       0.67      0.68      0.68        31\n",
      " weighted avg       0.72      0.71      0.71        31\n",
      "\n",
      "accuracy = 0.47619047619047616\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.70      0.47      0.56        15\n",
      "       missed       0.27      0.50      0.35         6\n",
      "\n",
      "    micro avg       0.48      0.48      0.48        21\n",
      "    macro avg       0.49      0.48      0.46        21\n",
      " weighted avg       0.58      0.48      0.50        21\n",
      "\n",
      "(20,)\n",
      "[ 0.5906807  -0.30447012  0.49889934  0.54577169 -0.58828252 -0.7885916\n",
      " -0.44191188  0.35547679 -0.68629739  0.31264913  0.02257408 -0.84120451\n",
      "  0.12107382 -0.13774034  0.61361718 -0.72898208 -1.08899373  0.4617798\n",
      " -0.06468196  0.47556057]\n",
      "567214\n",
      "training: 29 testing: 20\n",
      "correctsource    17\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    12\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.8        0.8        0.6        0.75       0.5        0.\n",
      " 0.66666667]\n",
      "Accuracy: 0.6206896551724138\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.69      0.65      0.67        17\n",
      "       missed       0.54      0.58      0.56        12\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        29\n",
      "    macro avg       0.61      0.62      0.61        29\n",
      " weighted avg       0.63      0.62      0.62        29\n",
      "\n",
      "accuracy = 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.67      0.67        12\n",
      "       missed       0.50      0.50      0.50         8\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        20\n",
      "    macro avg       0.58      0.58      0.58        20\n",
      " weighted avg       0.60      0.60      0.60        20\n",
      "\n",
      "(20,)\n",
      "[-0.89869721  0.3330975  -1.38707866 -0.73735518  0.16867385 -0.32135129\n",
      " -0.79556253  0.38257846  0.52822871 -0.02909727 -0.43559662  0.12182632\n",
      " -0.34153959 -0.09324927  0.67038215 -0.68445694  0.32845229  0.01658429\n",
      "  0.17107449  0.60824268]\n",
      "597569\n",
      "training: 39 testing: 27\n",
      "correctsource    28\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    20\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.16666667 0.83333333 0.5        0.5        0.4        1.\n",
      " 0.6       ]\n",
      "Accuracy: 0.5641025641025641\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.74      0.61      0.67        28\n",
      "       missed       0.31      0.45      0.37        11\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        39\n",
      "    macro avg       0.53      0.53      0.52        39\n",
      " weighted avg       0.62      0.56      0.58        39\n",
      "\n",
      "accuracy = 0.5185185185185185\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.73      0.55      0.63        20\n",
      "       missed       0.25      0.43      0.32         7\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        27\n",
      "    macro avg       0.49      0.49      0.47        27\n",
      " weighted avg       0.61      0.52      0.55        27\n",
      "\n",
      "(20,)\n",
      "[ 0.65048572  0.68066396 -0.57848469  0.51848354  0.40429013  0.10559703\n",
      "  0.67186539 -0.43800223  0.47293253  0.3180533  -0.55419573 -0.3292096\n",
      " -0.21262559  0.5264928  -0.68671137  0.53338151 -1.33998559 -0.59558116\n",
      " -0.52035405  0.79331044]\n",
      "652850\n",
      "training: 34 testing: 23\n",
      "missed           21\n",
      "correctsource    13\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           15\n",
      "correctsource     8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.8 0.4 0.6 0.6 0.4 0.4 0.5]\n",
      "Accuracy: 0.5294117647058824\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.40      0.46      0.43        13\n",
      "       missed       0.63      0.57      0.60        21\n",
      "\n",
      "    micro avg       0.53      0.53      0.53        34\n",
      "    macro avg       0.52      0.52      0.51        34\n",
      " weighted avg       0.54      0.53      0.53        34\n",
      "\n",
      "accuracy = 0.43478260869565216\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.22      0.25      0.24         8\n",
      "       missed       0.57      0.53      0.55        15\n",
      "\n",
      "    micro avg       0.43      0.43      0.43        23\n",
      "    macro avg       0.40      0.39      0.39        23\n",
      " weighted avg       0.45      0.43      0.44        23\n",
      "\n",
      "(20,)\n",
      "[ 0.36406474 -1.18006159 -0.05048325 -1.1493666  -0.19849339 -0.0913248\n",
      " -0.6387891   1.42513777 -0.56649852 -0.47167586 -0.0147903  -0.59327672\n",
      " -0.19772638  0.22263955  0.37228377 -0.32226408  1.54487625 -0.26556742\n",
      " -0.64367874 -0.01192975]\n",
      "677561\n",
      "training: 32 testing: 22\n",
      "correctsource    20\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.4        0.6        0.6        0.4        0.6        0.5\n",
      " 0.66666667]\n",
      "Accuracy: 0.53125\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.63      0.60      0.62        20\n",
      "       missed       0.38      0.42      0.40        12\n",
      "\n",
      "    micro avg       0.53      0.53      0.53        32\n",
      "    macro avg       0.51      0.51      0.51        32\n",
      " weighted avg       0.54      0.53      0.53        32\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.64      0.50      0.56        14\n",
      "       missed       0.36      0.50      0.42         8\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        22\n",
      "    macro avg       0.50      0.50      0.49        22\n",
      " weighted avg       0.54      0.50      0.51        22\n",
      "\n",
      "(20,)\n",
      "[-0.67993892  0.35043956 -0.08800072 -1.30029233 -0.60100264  0.58499168\n",
      " -1.52247169  0.29089991 -0.54865168 -0.00902453 -0.49632328 -0.48241227\n",
      "  0.43906431 -0.43302326  1.35600162 -0.53250099 -0.84000481 -0.26762315\n",
      "  0.51323514  0.10203847]\n",
      "711830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 testing: 25\n",
      "missed           21\n",
      "correctsource    15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           14\n",
      "correctsource    11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5 0.4 0.2 0.4 0.8 0.4 0.4]\n",
      "Accuracy: 0.4444444444444444\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.33      0.33      0.33        15\n",
      "       missed       0.52      0.52      0.52        21\n",
      "\n",
      "    micro avg       0.44      0.44      0.44        36\n",
      "    macro avg       0.43      0.43      0.43        36\n",
      " weighted avg       0.44      0.44      0.44        36\n",
      "\n",
      "accuracy = 0.56\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.64      0.56        11\n",
      "       missed       0.64      0.50      0.56        14\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        25\n",
      "    macro avg       0.57      0.57      0.56        25\n",
      " weighted avg       0.58      0.56      0.56        25\n",
      "\n",
      "(20,)\n",
      "[ 0.16149333  0.50368467  0.48332637 -0.88207025  0.10250533 -0.26631053\n",
      "  0.61179862 -0.08239101  0.36032128  0.79878855  0.27866135  0.61182973\n",
      " -0.89868449 -0.40385219 -0.09049216  0.00465935  0.32272661  0.69725624\n",
      " -0.84324281  0.01280273]\n",
      "729722\n",
      "training: 40 testing: 28\n",
      "correctsource    26\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    18\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.83333333 0.66666667 0.83333333 0.5        0.33333333 0.6\n",
      " 0.2       ]\n",
      "Accuracy: 0.575\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.70      0.62      0.65        26\n",
      "       missed       0.41      0.50      0.45        14\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        40\n",
      "    macro avg       0.55      0.56      0.55        40\n",
      " weighted avg       0.60      0.57      0.58        40\n",
      "\n",
      "accuracy = 0.6785714285714286\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.80      0.67      0.73        18\n",
      "       missed       0.54      0.70      0.61        10\n",
      "\n",
      "    micro avg       0.68      0.68      0.68        28\n",
      "    macro avg       0.67      0.68      0.67        28\n",
      " weighted avg       0.71      0.68      0.68        28\n",
      "\n",
      "(20,)\n",
      "[ 0.06497642  0.42578154 -0.05120851  0.52460179 -0.81095652 -0.39088014\n",
      "  0.11652453 -0.25574967  1.10020939 -0.43338118 -0.60178348  0.54398429\n",
      " -0.11078283 -0.94669407 -0.61874132 -1.63394446 -0.04912436  0.91781145\n",
      " -0.12916042  0.75569038]\n",
      "739694\n",
      "training: 32 testing: 22\n",
      "correctsource    20\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.2        0.8        0.         0.2        0.2        0.\n",
      " 0.33333333]\n",
      "Accuracy: 0.25\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.38      0.30      0.33        20\n",
      "       missed       0.12      0.17      0.14        12\n",
      "\n",
      "    micro avg       0.25      0.25      0.25        32\n",
      "    macro avg       0.25      0.23      0.24        32\n",
      " weighted avg       0.28      0.25      0.26        32\n",
      "\n",
      "accuracy = 0.5909090909090909\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.86      0.43      0.57        14\n",
      "       missed       0.47      0.88      0.61         8\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        22\n",
      "    macro avg       0.66      0.65      0.59        22\n",
      " weighted avg       0.72      0.59      0.58        22\n",
      "\n",
      "(20,)\n",
      "[ 0.37153581 -0.60678167 -0.15566998 -0.49604367 -0.06537544  0.61764295\n",
      "  0.74176004 -0.44554485  0.63637616 -0.15316311  0.2212828   0.07634755\n",
      " -0.93679379  0.26732291  0.07419668 -1.23316463 -0.34222743  0.87147864\n",
      " -0.16940836 -0.80926026]\n",
      "748676\n",
      "training: 36 testing: 24\n",
      "missed           26\n",
      "correctsource    10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           18\n",
      "correctsource     6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.16666667 0.66666667 0.66666667 0.6        0.6        0.25\n",
      " 0.75      ]\n",
      "Accuracy: 0.5277777777777778\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.29      0.50      0.37        10\n",
      "       missed       0.74      0.54      0.62        26\n",
      "\n",
      "    micro avg       0.53      0.53      0.53        36\n",
      "    macro avg       0.52      0.52      0.50        36\n",
      " weighted avg       0.61      0.53      0.55        36\n",
      "\n",
      "accuracy = 0.7083333333333334\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.43      0.50      0.46         6\n",
      "       missed       0.82      0.78      0.80        18\n",
      "\n",
      "    micro avg       0.71      0.71      0.71        24\n",
      "    macro avg       0.63      0.64      0.63        24\n",
      " weighted avg       0.72      0.71      0.72        24\n",
      "\n",
      "(20,)\n",
      "[-0.53959485  0.7935861  -0.02465954 -0.4600725  -0.60696959 -0.21278252\n",
      " -0.34719773 -0.31417055  0.10164851 -0.09067634 -0.04910666 -0.70104263\n",
      " -0.18951072 -0.56752895 -0.57630235  0.12873047  0.46133903  0.47326288\n",
      " -0.00577992  0.39894772]\n",
      "778749\n",
      "training: 33 testing: 22\n",
      "missed           19\n",
      "correctsource    14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           13\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.8  0.4  0.6  0.2  0.4  0.75 0.25]\n",
      "Accuracy: 0.48484848484848486\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.38      0.36      0.37        14\n",
      "       missed       0.55      0.58      0.56        19\n",
      "\n",
      "    micro avg       0.48      0.48      0.48        33\n",
      "    macro avg       0.47      0.47      0.47        33\n",
      " weighted avg       0.48      0.48      0.48        33\n",
      "\n",
      "accuracy = 0.45454545454545453\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.33      0.33      0.33         9\n",
      "       missed       0.54      0.54      0.54        13\n",
      "\n",
      "    micro avg       0.45      0.45      0.45        22\n",
      "    macro avg       0.44      0.44      0.44        22\n",
      " weighted avg       0.45      0.45      0.45        22\n",
      "\n",
      "(20,)\n",
      "[-0.22721799 -0.02290613  0.28610112  0.13137457  0.3336166  -0.03229863\n",
      "  0.22676945  0.1168462  -0.89154035 -0.58046113  0.15185813 -0.8510003\n",
      "  0.3388541  -0.079741   -0.07691651  0.08581654  1.54433242  0.42659211\n",
      "  0.17273422 -0.49746371]\n",
      "783781\n",
      "training: 34 testing: 23\n",
      "correctsource    24\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.66666667 0.83333333 0.75       0.5        0.75\n",
      " 0.25      ]\n",
      "Accuracy: 0.6470588235294118\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.77      0.71      0.74        24\n",
      "       missed       0.42      0.50      0.45        10\n",
      "\n",
      "    micro avg       0.65      0.65      0.65        34\n",
      "    macro avg       0.59      0.60      0.60        34\n",
      " weighted avg       0.67      0.65      0.66        34\n",
      "\n",
      "accuracy = 0.5652173913043478\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.80      0.50      0.62        16\n",
      "       missed       0.38      0.71      0.50         7\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        23\n",
      "    macro avg       0.59      0.61      0.56        23\n",
      " weighted avg       0.67      0.57      0.58        23\n",
      "\n",
      "(20,)\n",
      "[ 0.64653902  0.45261228 -0.43301482  0.69317048  0.27027254  0.1525753\n",
      " -0.47156326  0.2991518   0.35458516 -0.58099109  0.9340082  -0.87151742\n",
      " -0.84734583 -0.02066842  0.07711597 -0.34133309  0.78079988 -1.09543971\n",
      "  0.23393756  0.94133315]\n",
      "884343\n",
      "training: 32 testing: 22\n",
      "correctsource    17\n",
      "missed           15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    12\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5  0.6  0.2  0.25 0.5  0.5  0.25]\n",
      "Accuracy: 0.40625\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.44      0.41      0.42        17\n",
      "       missed       0.38      0.40      0.39        15\n",
      "\n",
      "    micro avg       0.41      0.41      0.41        32\n",
      "    macro avg       0.41      0.41      0.41        32\n",
      " weighted avg       0.41      0.41      0.41        32\n",
      "\n",
      "accuracy = 0.5909090909090909\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.64      0.58      0.61        12\n",
      "       missed       0.55      0.60      0.57        10\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        22\n",
      "    macro avg       0.59      0.59      0.59        22\n",
      " weighted avg       0.60      0.59      0.59        22\n",
      "\n",
      "(20,)\n",
      "[ 0.10162533 -0.11029995  0.04799114 -0.48494363 -0.49203544  0.51511419\n",
      " -0.35979058 -0.07434375 -0.99577205  1.01389049  0.52450548  0.25482859\n",
      " -0.51486171 -0.87661487  0.20765691 -0.42660759  0.14133799 -0.21104367\n",
      "  1.37079889  0.80491981]\n",
      "886007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 39 testing: 26\n",
      "correctsource    27\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    18\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.66666667 0.33333333 0.33333333 0.66666667 0.8\n",
      " 0.5       ]\n",
      "Accuracy: 0.5384615384615384\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.71      0.56      0.63        27\n",
      "       missed       0.33      0.50      0.40        12\n",
      "\n",
      "    micro avg       0.54      0.54      0.54        39\n",
      "    macro avg       0.52      0.53      0.51        39\n",
      " weighted avg       0.60      0.54      0.56        39\n",
      "\n",
      "accuracy = 0.4230769230769231\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.50      0.55        18\n",
      "       missed       0.18      0.25      0.21         8\n",
      "\n",
      "    micro avg       0.42      0.42      0.42        26\n",
      "    macro avg       0.39      0.38      0.38        26\n",
      " weighted avg       0.47      0.42      0.44        26\n",
      "\n",
      "(20,)\n",
      "[ 0.35001555 -0.35045534  0.93206017  0.56030487 -0.51245698 -0.74946954\n",
      " -0.59013668 -0.74155563 -0.72512806 -0.63125268  0.65713089 -0.24265928\n",
      " -0.94571608  1.19054946  0.02538993  0.05539218 -1.26652905 -0.378698\n",
      " -0.07984965  0.33680415]\n",
      "936730\n",
      "training: 36 testing: 24\n",
      "missed           18\n",
      "correctsource    18\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           12\n",
      "correctsource    12\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.33333333 0.33333333 0.5        0.25       0.25\n",
      " 0.5       ]\n",
      "Accuracy: 0.3888888888888889\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.42      0.61      0.50        18\n",
      "       missed       0.30      0.17      0.21        18\n",
      "\n",
      "    micro avg       0.39      0.39      0.39        36\n",
      "    macro avg       0.36      0.39      0.36        36\n",
      " weighted avg       0.36      0.39      0.36        36\n",
      "\n",
      "accuracy = 0.5416666666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.52      1.00      0.69        12\n",
      "       missed       1.00      0.08      0.15        12\n",
      "\n",
      "    micro avg       0.54      0.54      0.54        24\n",
      "    macro avg       0.76      0.54      0.42        24\n",
      " weighted avg       0.76      0.54      0.42        24\n",
      "\n",
      "(20,)\n",
      "[-0.02316008 -0.1178554  -0.05566709  0.00174687 -0.11663605 -0.0695719\n",
      " -0.00301296  0.00875359 -0.01659188  0.01967994  0.03778283  0.01695139\n",
      " -0.56976839 -0.00968592 -0.27375673  0.18397286 -0.00815239 -0.13643483\n",
      "  0.32797174 -0.00539406]\n",
      "956130\n",
      "training: 38 testing: 26\n",
      "missed           20\n",
      "correctsource    18\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           13\n",
      "correctsource    13\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.5        0.5        0.33333333 0.4        0.4\n",
      " 0.75      ]\n",
      "Accuracy: 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.47      0.44      0.46        18\n",
      "       missed       0.52      0.55      0.54        20\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        38\n",
      "    macro avg       0.50      0.50      0.50        38\n",
      " weighted avg       0.50      0.50      0.50        38\n",
      "\n",
      "accuracy = 0.5769230769230769\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.57      0.62      0.59        13\n",
      "       missed       0.58      0.54      0.56        13\n",
      "\n",
      "    micro avg       0.58      0.58      0.58        26\n",
      "    macro avg       0.58      0.58      0.58        26\n",
      " weighted avg       0.58      0.58      0.58        26\n",
      "\n",
      "(20,)\n",
      "[ 0.33186163 -0.04279205 -0.23915701  0.96428783 -0.70408741 -0.07734205\n",
      "  0.7977101  -0.12189574  0.19195706 -0.30113093  0.65543583 -0.95917837\n",
      "  1.42956166 -0.33177386 -0.18464901 -0.88944107  0.1207593   0.30565741\n",
      " -0.23862618 -0.57064084]\n",
      "983291\n",
      "training: 40 testing: 28\n",
      "correctsource    29\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    21\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.85714286 0.66666667 0.66666667 0.33333333 0.6        0.4\n",
      " 0.8       ]\n",
      "Accuracy: 0.625\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.77      0.69      0.73        29\n",
      "       missed       0.36      0.45      0.40        11\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        40\n",
      "    macro avg       0.56      0.57      0.56        40\n",
      " weighted avg       0.66      0.62      0.64        40\n",
      "\n",
      "accuracy = 0.6071428571428571\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.75      0.71      0.73        21\n",
      "       missed       0.25      0.29      0.27         7\n",
      "\n",
      "    micro avg       0.61      0.61      0.61        28\n",
      "    macro avg       0.50      0.50      0.50        28\n",
      " weighted avg       0.62      0.61      0.62        28\n",
      "\n",
      "(20,)\n",
      "[ 0.810711    0.21710332  0.69853949 -0.72584936  0.05965779 -1.62890815\n",
      " -0.29052536 -0.76045258  1.20979143 -0.24607115  0.03116068 -0.17297932\n",
      "  1.17275022  0.06200074 -0.50180466  0.38012971 -0.89157316  0.25637376\n",
      " -0.53314019 -0.43486985]\n",
      "998166\n",
      "training: 37 testing: 26\n",
      "correctsource    21\n",
      "missed           16\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.33333333 0.         0.6        0.6        0.2\n",
      " 0.6       ]\n",
      "Accuracy: 0.3783783783783784\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.45      0.43      0.44        21\n",
      "       missed       0.29      0.31      0.30        16\n",
      "\n",
      "    micro avg       0.38      0.38      0.38        37\n",
      "    macro avg       0.37      0.37      0.37        37\n",
      " weighted avg       0.38      0.38      0.38        37\n",
      "\n",
      "accuracy = 0.4230769230769231\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.47      0.57      0.52        14\n",
      "       missed       0.33      0.25      0.29        12\n",
      "\n",
      "    micro avg       0.42      0.42      0.42        26\n",
      "    macro avg       0.40      0.41      0.40        26\n",
      " weighted avg       0.41      0.42      0.41        26\n",
      "\n",
      "(20,)\n",
      "[-0.00319032  0.1676196  -0.11083493  0.75679622  0.79801944  0.65168702\n",
      "  0.39573277  0.65352775  0.09363006 -0.8878731  -0.32645155 -0.28253016\n",
      " -0.73613529 -0.07219712 -0.13714687 -0.3264485  -0.41882357  0.40123335\n",
      " -0.61548654  0.46699026]\n",
      "108391\n",
      "training: 34 testing: 24\n",
      "correctsource    25\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    18\n",
      "missed            6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.83333333 0.8        0.6        1.         1.\n",
      " 0.75      ]\n",
      "Accuracy: 0.7941176470588235\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.82      0.92      0.87        25\n",
      "       missed       0.67      0.44      0.53         9\n",
      "\n",
      "    micro avg       0.79      0.79      0.79        34\n",
      "    macro avg       0.74      0.68      0.70        34\n",
      " weighted avg       0.78      0.79      0.78        34\n",
      "\n",
      "accuracy = 0.4166666666666667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.56      0.59        18\n",
      "       missed       0.00      0.00      0.00         6\n",
      "\n",
      "    micro avg       0.42      0.42      0.42        24\n",
      "    macro avg       0.31      0.28      0.29        24\n",
      " weighted avg       0.47      0.42      0.44        24\n",
      "\n",
      "(64,)\n",
      "[ 0.12256259 -0.13106947 -0.16392048  0.21165709  0.10165355 -0.0996425\n",
      "  0.10661115  0.09945193  0.09573168  0.27347368  0.38191868  0.19800589\n",
      " -0.0089013   0.33961981  0.18136197 -0.14821542  0.1335453  -0.12812612\n",
      " -0.06622682  0.17156719  0.31490959  0.1206232   0.22821531 -0.20607685\n",
      "  0.00337896 -0.1264951   0.04336082  0.06826588 -0.06361407 -0.28138868\n",
      "  0.26940803 -0.11117139  0.05132484 -0.06594551  0.16484336  0.08255453\n",
      "  0.11307207  0.25577021 -0.09235081 -0.05201463 -0.06703971 -0.0958684\n",
      " -0.04413396 -0.05082241  0.04167004  0.12192157  0.04965313 -0.06063469\n",
      "  0.13070656 -0.20094553  0.10835005 -0.07869946 -0.28815201  0.0736959\n",
      "  0.29363218 -0.09211389  0.06534419  0.13896957  0.03574027 -0.0722243\n",
      " -0.48307058 -0.00280922 -0.02668569 -0.18868348]\n",
      "122922\n",
      "training: 28 testing: 20\n",
      "correctsource    15\n",
      "missed           13\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           10\n",
      "correctsource    10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.8        0.75       0.25       0.25       0.5        0.5\n",
      " 0.66666667]\n",
      "Accuracy: 0.5357142857142857\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.58      0.47      0.52        15\n",
      "       missed       0.50      0.62      0.55        13\n",
      "\n",
      "    micro avg       0.54      0.54      0.54        28\n",
      "    macro avg       0.54      0.54      0.54        28\n",
      " weighted avg       0.54      0.54      0.53        28\n",
      "\n",
      "accuracy = 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.50      0.56        10\n",
      "       missed       0.58      0.70      0.64        10\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        20\n",
      "    macro avg       0.60      0.60      0.60        20\n",
      " weighted avg       0.60      0.60      0.60        20\n",
      "\n",
      "(64,)\n",
      "[ 0.1326576   0.16545402  0.3895143   0.08116675 -0.10668707  0.10152318\n",
      " -0.06355701 -0.17085771 -0.00707311 -0.04122178  0.26875004  0.27067177\n",
      " -0.19142105 -0.21375153 -0.06909616 -0.03599858  0.24370984  0.20257185\n",
      " -0.12220737  0.20067655 -0.17161905  0.01056136  0.04460984 -0.08789323\n",
      " -0.12632769  0.35346303 -0.19256305 -0.27954302  0.05001308 -0.25533961\n",
      "  0.10759091 -0.1768407   0.08504258 -0.10891655  0.27201563 -0.34522493\n",
      "  0.12173796  0.05439802 -0.06819949 -0.03607775 -0.07832208  0.23550232\n",
      "  0.04283672 -0.18873102 -0.13183277 -0.24934358  0.07970957  0.08843678\n",
      " -0.28777055 -0.23154069 -0.03993812  0.08126551  0.18681112 -0.00942538\n",
      " -0.20814065 -0.0822615  -0.32060536  0.10436353 -0.07993033 -0.17261358\n",
      "  0.05279174  0.2183118  -0.10945027 -0.02991652]\n",
      "139593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 33 testing: 23\n",
      "missed           24\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           17\n",
      "correctsource     6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.33333333 0.6        0.25       0.75       0.5\n",
      " 0.75      ]\n",
      "Accuracy: 0.48484848484848486\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.10      0.11      0.11         9\n",
      "       missed       0.65      0.62      0.64        24\n",
      "\n",
      "    micro avg       0.48      0.48      0.48        33\n",
      "    macro avg       0.38      0.37      0.37        33\n",
      " weighted avg       0.50      0.48      0.49        33\n",
      "\n",
      "accuracy = 0.6956521739130435\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.43      0.50      0.46         6\n",
      "       missed       0.81      0.76      0.79        17\n",
      "\n",
      "    micro avg       0.70      0.70      0.70        23\n",
      "    macro avg       0.62      0.63      0.62        23\n",
      " weighted avg       0.71      0.70      0.70        23\n",
      "\n",
      "(64,)\n",
      "[-0.16195523 -0.46182438  0.28543235  0.22807219 -0.04273241 -0.1838738\n",
      " -0.42808922 -0.11771998  0.18766117  0.06178865 -0.02921393 -0.27534894\n",
      " -0.23045737  0.23459665 -0.09472847  0.37715714  0.06731835  0.0357781\n",
      " -0.02133409  0.27394685 -0.22604854  0.25622783 -0.21077169  0.32022545\n",
      "  0.12419637 -0.04667315  0.54077537  0.21335746  0.00545391 -0.32366011\n",
      " -0.08784477 -0.0622958  -0.0354086  -0.30242901 -0.06402218 -0.15342151\n",
      "  0.38163293 -0.09604396  0.17907062  0.04311559 -0.04492594  0.34277634\n",
      " -0.34886841  0.30903496  0.42420342 -0.06762031  0.23275085  0.07179478\n",
      "  0.25489302 -0.36278794  0.09793582  0.38594151  0.33529634  0.25309389\n",
      "  0.23090964 -0.05712602 -0.01130085  0.00109543 -0.04606383 -0.42153093\n",
      "  0.0905419   0.3182209   0.30557708  0.13606041]\n",
      "164965\n",
      "training: 39 testing: 27\n",
      "correctsource    24\n",
      "missed           15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    17\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.71428571 0.5        0.66666667 0.6        0.8        0.8\n",
      " 0.6       ]\n",
      "Accuracy: 0.6666666666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.70      0.79      0.75        24\n",
      "       missed       0.58      0.47      0.52        15\n",
      "\n",
      "    micro avg       0.67      0.67      0.67        39\n",
      "    macro avg       0.64      0.63      0.63        39\n",
      " weighted avg       0.66      0.67      0.66        39\n",
      "\n",
      "accuracy = 0.5185185185185185\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.61      0.65      0.63        17\n",
      "       missed       0.33      0.30      0.32        10\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        27\n",
      "    macro avg       0.47      0.47      0.47        27\n",
      " weighted avg       0.51      0.52      0.51        27\n",
      "\n",
      "(64,)\n",
      "[ 0.37729234  0.32709611  0.17821834  0.05782254  0.10554181  0.03530117\n",
      "  0.15703594  0.10381958  0.06255825 -0.09416979  0.04804272  0.1143777\n",
      " -0.13352922  0.42111978 -0.28725894  0.0122931   0.19544067  0.2097064\n",
      " -0.15050632 -0.17521716  0.08647028 -0.10467562  0.23711779 -0.01725318\n",
      " -0.08336165 -0.06898689 -0.10598062 -0.13801931  0.15182483  0.02060659\n",
      " -0.0318135  -0.00216627 -0.03068932 -0.10496774 -0.01561798  0.04320201\n",
      "  0.11418435 -0.42184141  0.20473648 -0.06533021 -0.27884622  0.26719944\n",
      " -0.18219248  0.02953529 -0.19012046  0.00540609  0.19956307  0.07713369\n",
      "  0.16685818 -0.02622527  0.30058134 -0.30661952 -0.06014282  0.17302281\n",
      " -0.20153238  0.01818645  0.11089379  0.20106311 -0.094609   -0.13114359\n",
      "  0.01523872 -0.03183022 -0.09580174  0.00824967]\n",
      "199801\n",
      "training: 20 testing: 14\n",
      "missed           11\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           8\n",
      "correctsource    6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.25       0.66666667 1.         1.         0.5\n",
      " 0.5       ]\n",
      "Accuracy: 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.55      0.67      0.60         9\n",
      "       missed       0.67      0.55      0.60        11\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        20\n",
      "    macro avg       0.61      0.61      0.60        20\n",
      " weighted avg       0.61      0.60      0.60        20\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.44      0.67      0.53         6\n",
      "       missed       0.60      0.38      0.46         8\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        14\n",
      "    macro avg       0.52      0.52      0.50        14\n",
      " weighted avg       0.53      0.50      0.49        14\n",
      "\n",
      "(64,)\n",
      "[ 0.0606301   0.07660668 -0.01797853  0.0128367   0.03073379  0.09967619\n",
      " -0.06056057  0.08213286 -0.09456787 -0.18520178  0.11385525 -0.08651037\n",
      "  0.03489607 -0.12773273 -0.02801403  0.15885553 -0.13538006  0.09941269\n",
      "  0.14905133  0.02548865 -0.25636364  0.15765162  0.09637485 -0.12931924\n",
      " -0.13797084 -0.11339178 -0.03775397 -0.09298239  0.11898081 -0.12895576\n",
      "  0.16291995  0.24376051  0.14582691  0.0538665   0.06248773  0.29772138\n",
      " -0.230107   -0.01667145  0.04402174  0.11326959 -0.07494909  0.04856651\n",
      " -0.04932753  0.13101267  0.13545852 -0.08093014  0.00217996 -0.02030705\n",
      "  0.23724569  0.05162827  0.19900916  0.22397921  0.12525404 -0.03095821\n",
      "  0.01996624 -0.07858798  0.23077844  0.24589339 -0.05348687 -0.19849475\n",
      "  0.04879802 -0.04035825 -0.02809991  0.02805443]\n",
      "247659\n",
      "training: 34 testing: 23\n",
      "missed           25\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           17\n",
      "correctsource     6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.83333333 1.         0.6        1.         0.75\n",
      " 0.25      ]\n",
      "Accuracy: 0.7352941176470589\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.67      0.57         9\n",
      "       missed       0.86      0.76      0.81        25\n",
      "\n",
      "    micro avg       0.74      0.74      0.74        34\n",
      "    macro avg       0.68      0.71      0.69        34\n",
      " weighted avg       0.77      0.74      0.75        34\n",
      "\n",
      "accuracy = 0.4782608695652174\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.25      0.50      0.33         6\n",
      "       missed       0.73      0.47      0.57        17\n",
      "\n",
      "    micro avg       0.48      0.48      0.48        23\n",
      "    macro avg       0.49      0.49      0.45        23\n",
      " weighted avg       0.60      0.48      0.51        23\n",
      "\n",
      "(64,)\n",
      "[-0.33578312 -0.08079119  0.26485862  0.06178801 -0.2148455   0.21204768\n",
      "  0.13589905  0.10429299  0.0450246  -0.04036697 -0.34617122 -0.14398197\n",
      "  0.09686003  0.15556449 -0.0195027   0.09417913 -0.07719652 -0.33118423\n",
      " -0.02740792 -0.00659256  0.06980779 -0.02053354  0.14931829 -0.10482968\n",
      " -0.01164614 -0.24532674 -0.08017278 -0.19828037 -0.17646024  0.00387866\n",
      "  0.18147656 -0.06182488 -0.24868242  0.0243104  -0.04445336  0.20034044\n",
      " -0.15273334  0.00326506 -0.1995857   0.05993916  0.00445214 -0.00159087\n",
      "  0.23286477 -0.16322117 -0.09906449  0.13601978 -0.15656045  0.51468638\n",
      " -0.1002703  -0.09865994 -0.0916638  -0.11232506  0.00449428 -0.01942408\n",
      " -0.14998168  0.00199151 -0.06428264  0.0704078  -0.10153734 -0.34220193\n",
      " -0.08810307  0.02257182 -0.19111057  0.12156314]\n",
      "255499\n",
      "training: 39 testing: 27\n",
      "correctsource    23\n",
      "missed           16\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.57142857 0.57142857 0.6        0.6        0.6        0.6\n",
      " 0.4       ]\n",
      "Accuracy: 0.5641025641025641\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.59      0.87      0.70        23\n",
      "       missed       0.40      0.12      0.19        16\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        39\n",
      "    macro avg       0.49      0.50      0.45        39\n",
      " weighted avg       0.51      0.56      0.49        39\n",
      "\n",
      "accuracy = 0.5555555555555556\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.58      0.88      0.70        16\n",
      "       missed       0.33      0.09      0.14        11\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        27\n",
      "    macro avg       0.46      0.48      0.42        27\n",
      " weighted avg       0.48      0.56      0.47        27\n",
      "\n",
      "(64,)\n",
      "[ 3.18590707e-02  8.12404361e-02 -2.13453764e-01  4.73528373e-02\n",
      " -3.08741705e-01 -2.18904674e-01  2.17803587e-02 -8.86229428e-02\n",
      " -5.49815134e-02  9.46003677e-02 -2.45257025e-02 -3.15512335e-01\n",
      " -3.81075622e-02  6.28918171e-02 -4.50731475e-03 -3.18971502e-01\n",
      " -4.22973497e-03  2.58094734e-02  2.98906028e-02  1.12609168e-02\n",
      " -1.93008881e-01  7.77899599e-03  1.51125747e-01  1.12855703e+00\n",
      "  3.64231149e-03  1.38632025e-02 -4.11688441e-02 -3.93474169e-01\n",
      " -1.13161345e-01 -1.10227757e-01  5.41728284e-02 -2.57297279e-02\n",
      " -1.20203503e+00  6.22811329e-02  9.55357957e-03  5.73701138e-02\n",
      "  2.79424677e-02 -7.39887073e-02  3.89643724e-02  1.42676241e-03\n",
      " -3.21827720e-02  3.17390846e-01 -2.57360598e-01 -2.08477369e-02\n",
      "  3.35322547e-02  6.40368203e-02  2.69606269e-04  1.12679972e-02\n",
      "  5.32648506e-02  1.04695982e-01  1.27380983e-02  3.47478998e-01\n",
      "  3.73108473e-02  2.49630718e-02  1.12501525e-01  1.76025498e-02\n",
      " -6.94940306e-03  2.82149224e-02 -1.17108539e-01  3.20584797e-02\n",
      "  1.87625148e-01  4.74081216e-02 -1.81748131e-02  4.50613485e-02]\n",
      "258618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 testing: 25\n",
      "correctsource    20\n",
      "missed           16\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.66666667 0.4        0.8        0.6        0.6\n",
      " 0.75      ]\n",
      "Accuracy: 0.6388888888888888\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.68      0.65      0.67        20\n",
      "       missed       0.59      0.62      0.61        16\n",
      "\n",
      "    micro avg       0.64      0.64      0.64        36\n",
      "    macro avg       0.64      0.64      0.64        36\n",
      " weighted avg       0.64      0.64      0.64        36\n",
      "\n",
      "accuracy = 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.61      0.79      0.69        14\n",
      "       missed       0.57      0.36      0.44        11\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        25\n",
      "    macro avg       0.59      0.57      0.57        25\n",
      " weighted avg       0.59      0.60      0.58        25\n",
      "\n",
      "(64,)\n",
      "[ 2.98633956e-02 -2.32872773e-01 -2.70278276e-01 -4.44710898e-01\n",
      " -1.49607122e-01 -1.35256167e-01  4.02561520e-02  5.60767260e-01\n",
      "  2.51974637e-02 -5.42054024e-02  3.71287012e-01  1.31381523e-01\n",
      "  4.09241066e-01  6.09685266e-02 -7.28563419e-02 -2.96434931e-01\n",
      " -2.74780730e-01  1.93903206e-01 -7.15477391e-02 -3.42614931e-01\n",
      "  4.21079098e-01 -1.42740900e-01  9.28213214e-02 -3.02229637e-02\n",
      " -5.24861163e-02  5.31248717e-02  6.87956600e-03 -2.61881836e-01\n",
      " -1.35698121e-02  3.45589577e-01  2.52063674e-01 -1.39391735e-04\n",
      "  1.53258404e-01  1.80717951e-01  2.59488211e-01  1.94742155e-01\n",
      "  1.70438359e-01  6.82844882e-02 -4.09314869e-01 -1.33930414e-01\n",
      "  2.08718678e-01  3.44000639e-01  6.03502188e-02 -2.90859813e-02\n",
      "  7.29543248e-02 -8.37173681e-02  4.27403284e-02  4.30861757e-02\n",
      "  8.72692025e-02  1.03376613e-02  7.94681235e-02  6.49550952e-03\n",
      " -1.71988136e-01 -3.48247718e-02  2.52267278e-01  1.85374040e-01\n",
      " -3.57050306e-01 -3.72145895e-01 -2.30983596e-01  4.50819603e-01\n",
      " -3.12319680e-01  1.65257676e-01 -9.33975632e-02  7.56751744e-02]\n",
      "271596\n",
      "training: 36 testing: 25\n",
      "correctsource    22\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.  0.4 0.4 0.8 0.8 0.6 0.6]\n",
      "Accuracy: 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.59      0.59      0.59        22\n",
      "       missed       0.36      0.36      0.36        14\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        36\n",
      "    macro avg       0.47      0.47      0.47        36\n",
      " weighted avg       0.50      0.50      0.50        36\n",
      "\n",
      "accuracy = 0.52\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.64      0.56      0.60        16\n",
      "       missed       0.36      0.44      0.40         9\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        25\n",
      "    macro avg       0.50      0.50      0.50        25\n",
      " weighted avg       0.54      0.52      0.53        25\n",
      "\n",
      "(64,)\n",
      "[-0.42497674  0.27645533  0.14537614  0.04948809  0.39195097  0.04790196\n",
      "  0.2069003  -0.00851687 -0.06092021  0.16673555  0.29877122  0.05877954\n",
      "  0.08216386  0.1733034   0.3007196   0.02359283  0.04746255  0.03278361\n",
      "  0.25939959  0.08089508  0.45125525 -0.09155664 -0.04547885  0.26006899\n",
      " -0.36489182 -0.00668935  0.07687349 -0.01169554 -0.28438905 -0.07517003\n",
      "  0.06533142  0.0181777   0.14773872 -0.11624538 -0.27476745 -0.1434516\n",
      " -0.04302093  0.02160732  0.01316405 -0.2981078   0.27661405  0.15960266\n",
      " -0.03726474  0.13121412 -0.30207321 -0.13000159  0.01381244 -0.02083063\n",
      "  0.15493742 -0.06473353 -0.03046998 -0.20402598 -0.29217569 -0.10155141\n",
      "  0.09540542 -0.48725561 -0.38155193  0.11744972  0.15209709  0.11380367\n",
      " -0.13192484 -0.22282532  0.17332446  0.00580287]\n",
      "314409\n",
      "training: 40 testing: 28\n",
      "correctsource    21\n",
      "missed           19\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           14\n",
      "correctsource    14\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.66666667 0.5        0.66666667 1.         0.6\n",
      " 0.8       ]\n",
      "Accuracy: 0.7\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.71      0.71      0.71        21\n",
      "       missed       0.68      0.68      0.68        19\n",
      "\n",
      "    micro avg       0.70      0.70      0.70        40\n",
      "    macro avg       0.70      0.70      0.70        40\n",
      " weighted avg       0.70      0.70      0.70        40\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.36      0.42        14\n",
      "       missed       0.50      0.64      0.56        14\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        28\n",
      "    macro avg       0.50      0.50      0.49        28\n",
      " weighted avg       0.50      0.50      0.49        28\n",
      "\n",
      "(64,)\n",
      "[-0.07466881  0.06171196 -0.2358864   0.05111361 -0.08839843 -0.16633554\n",
      " -0.12430927 -0.16590154  0.44367389 -0.17747712 -0.359483   -0.30358057\n",
      " -0.39684008  0.20353789  0.05117961 -0.43844232 -0.01228719 -0.37243172\n",
      "  0.07305013 -0.20317859  0.12985663 -0.07520136  0.48785587 -0.01371712\n",
      "  0.2356362   0.15169079  0.13359908 -0.36071011 -0.16576782 -0.05985811\n",
      " -0.20429196  0.18141726  0.20674126  0.17448496 -0.29754779 -0.34131447\n",
      "  0.20431699  0.10198224 -0.54344145 -0.33248516 -0.30994144  0.02383057\n",
      "  0.22403638  0.25211558 -0.10379806 -0.1960154  -0.0627343  -0.14520379\n",
      "  0.17880091  0.23884834  0.22468452 -0.28412785 -0.06474132  0.02341094\n",
      "  0.11899952 -0.20458878  0.04855543  0.00514564  0.42381067  0.00946885\n",
      "  0.08186922 -0.29777704 -0.1958426   0.09305614]\n",
      "336665\n",
      "training: 30 testing: 20\n",
      "correctsource    16\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    11\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.8  0.2  0.5  0.5  0.25 0.5  0.5 ]\n",
      "Accuracy: 0.4666666666666667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.38      0.43        16\n",
      "       missed       0.44      0.57      0.50        14\n",
      "\n",
      "    micro avg       0.47      0.47      0.47        30\n",
      "    macro avg       0.47      0.47      0.46        30\n",
      " weighted avg       0.47      0.47      0.46        30\n",
      "\n",
      "accuracy = 0.4\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.44      0.36      0.40        11\n",
      "       missed       0.36      0.44      0.40         9\n",
      "\n",
      "    micro avg       0.40      0.40      0.40        20\n",
      "    macro avg       0.40      0.40      0.40        20\n",
      " weighted avg       0.41      0.40      0.40        20\n",
      "\n",
      "(64,)\n",
      "[ 0.21390059 -0.2278775  -0.05943585 -0.05515812 -0.17863008 -0.04551945\n",
      " -0.06243903 -0.32427365  0.15196434 -0.24609647 -0.00343603  0.02364801\n",
      " -0.06370233 -0.14426986  0.21694443  0.31038076 -0.30813297 -0.05608215\n",
      "  0.50244422 -0.2530367   0.09129098  0.16550691  0.17425113 -0.03587926\n",
      " -0.40624442 -0.15074809 -0.19316208 -0.2691559  -0.16582492 -0.17213046\n",
      " -0.22798032 -0.28200566 -0.18519059  0.17167271 -0.03564775  0.08732361\n",
      "  0.08472164 -0.08625932 -0.57235073  0.01370109 -0.04169122  0.01883685\n",
      " -0.25604576  0.0834235   0.08811981  0.12520633  0.23616864 -0.1496324\n",
      "  0.08334328 -0.42971452 -0.02644797  0.11682527  0.42296482  0.26507882\n",
      "  0.3304124   0.21527124 -0.12960064  0.0658221  -0.03800371  0.32480191\n",
      " -0.09880905  0.12326493 -0.04049568  0.22580581]\n",
      "337021\n",
      "training: 34 testing: 23\n",
      "correctsource    24\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.83333333 0.66666667 0.66666667 1.         0.75       0.5\n",
      " 0.75      ]\n",
      "Accuracy: 0.7352941176470589\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.80      0.83      0.82        24\n",
      "       missed       0.56      0.50      0.53        10\n",
      "\n",
      "    micro avg       0.74      0.74      0.74        34\n",
      "    macro avg       0.68      0.67      0.67        34\n",
      " weighted avg       0.73      0.74      0.73        34\n",
      "\n",
      "accuracy = 0.6086956521739131\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.88      0.76        16\n",
      "       missed       0.00      0.00      0.00         7\n",
      "\n",
      "    micro avg       0.61      0.61      0.61        23\n",
      "    macro avg       0.33      0.44      0.38        23\n",
      " weighted avg       0.46      0.61      0.53        23\n",
      "\n",
      "(64,)\n",
      "[-3.08381803e-01 -2.68025314e-01 -3.60901968e-02  1.74160232e-01\n",
      "  6.07052393e-02 -3.98202837e-02 -1.67339344e-01 -3.56827420e-01\n",
      " -2.04854380e-01  2.15497645e-01 -2.24735891e-02 -3.05454776e-02\n",
      " -1.90730244e-01 -2.03796914e-01 -1.50721777e-01 -8.18718453e-02\n",
      "  1.89392996e-01  3.12408934e-03  1.53515470e-01 -1.20620119e-01\n",
      " -2.19369455e-01  3.12559256e-02  4.57846610e-02 -3.24453406e-01\n",
      " -4.77119991e-02  7.50971823e-02  2.34534263e-02  1.00312878e-01\n",
      " -1.94908504e-01  6.99123812e-02  4.15077123e-03  1.41262369e-01\n",
      "  1.82719646e-01  2.47256023e-01  2.81898635e-01  2.63504257e-01\n",
      "  2.02520930e-01  2.88916754e-02  4.59570403e-02 -2.23118056e-01\n",
      "  3.41650207e-02  1.65925160e-01  6.09423630e-02 -1.08041107e-01\n",
      " -7.83453172e-02 -2.13188585e-01  1.84234388e-01  5.22044193e-02\n",
      " -1.52776774e-01 -1.98544975e-02  2.70640175e-02  2.65126484e-01\n",
      " -1.43278244e-01 -1.27774344e-04  3.00168156e-02  1.31592911e-02\n",
      " -1.69300604e-01  2.95708556e-01  1.35482008e-01  1.38894793e-02\n",
      "  5.94161383e-02  1.86003223e-01 -1.05037599e-01 -1.33708031e-01]\n",
      "396250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 39 testing: 27\n",
      "missed           21\n",
      "correctsource    18\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           14\n",
      "correctsource    13\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.5        0.83333333 0.66666667 0.6        0.6\n",
      " 0.4       ]\n",
      "Accuracy: 0.6153846153846154\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.57      0.67      0.62        18\n",
      "       missed       0.67      0.57      0.62        21\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        39\n",
      "    macro avg       0.62      0.62      0.62        39\n",
      " weighted avg       0.62      0.62      0.62        39\n",
      "\n",
      "accuracy = 0.7037037037037037\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.77      0.71        13\n",
      "       missed       0.75      0.64      0.69        14\n",
      "\n",
      "    micro avg       0.70      0.70      0.70        27\n",
      "    macro avg       0.71      0.71      0.70        27\n",
      " weighted avg       0.71      0.70      0.70        27\n",
      "\n",
      "(64,)\n",
      "[-3.55902118e-01  5.42481776e-01  1.05765614e-01  1.81795706e-01\n",
      "  1.51301192e-01 -7.09511398e-02  4.97683502e-01  6.17387672e-03\n",
      " -9.91811604e-02 -1.82467155e-01 -1.75546993e-01  2.23682087e-01\n",
      " -3.34420509e-01  2.77910178e-01  1.14010518e-02 -1.08287864e-01\n",
      " -5.33540337e-01 -3.62515665e-01 -3.33224232e-02  1.84411203e-01\n",
      "  2.13860297e-01  7.49851648e-02  4.05516762e-01 -4.27718705e-02\n",
      "  2.08082530e-01 -2.09101341e-01 -2.99160371e-01 -4.82311689e-01\n",
      "  1.29081470e-01 -5.12290751e-02  1.80577508e-01  8.91047557e-02\n",
      "  1.33371790e-01 -2.13292269e-02 -2.60779732e-01  2.51627163e-01\n",
      " -9.54139548e-02  1.41645791e-01 -9.12533660e-02 -2.70643725e-01\n",
      " -1.18806180e-01 -3.07580740e-01 -9.63464581e-02 -2.54992188e-02\n",
      "  9.46516641e-02 -3.59139239e-02 -5.75962517e-04  1.26445801e-01\n",
      " -3.30737767e-01 -1.95553082e-01  5.77663514e-02  5.12728011e-03\n",
      " -2.47868215e-01 -2.94783316e-01  2.15950061e-02 -8.16125225e-02\n",
      " -2.12545392e-01 -9.55741973e-02 -3.25829613e-01 -1.46833140e-01\n",
      "  9.31414696e-02 -4.32298414e-02 -6.88629729e-01  2.72785996e-01]\n",
      "403131\n",
      "training: 42 testing: 28\n",
      "correctsource    23\n",
      "missed           19\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.57142857 0.85714286 0.83333333 0.83333333 0.66666667 0.8\n",
      " 0.4       ]\n",
      "Accuracy: 0.7142857142857143\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.70      0.83      0.76        23\n",
      "       missed       0.73      0.58      0.65        19\n",
      "\n",
      "    micro avg       0.71      0.71      0.71        42\n",
      "    macro avg       0.72      0.70      0.70        42\n",
      " weighted avg       0.72      0.71      0.71        42\n",
      "\n",
      "accuracy = 0.39285714285714285\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.46      0.38      0.41        16\n",
      "       missed       0.33      0.42      0.37        12\n",
      "\n",
      "    micro avg       0.39      0.39      0.39        28\n",
      "    macro avg       0.40      0.40      0.39        28\n",
      " weighted avg       0.41      0.39      0.40        28\n",
      "\n",
      "(64,)\n",
      "[-0.21038045  0.13056388  0.10710841 -0.127914   -0.01953689  0.37573888\n",
      "  0.08429559  0.20224529  0.01173298  0.08893153 -0.14613912 -0.26536357\n",
      " -0.04437061  0.06285744 -0.27593138 -0.14419087 -0.28516554 -0.10993279\n",
      " -0.06651721 -0.05892192 -0.23835821 -0.09711244  0.05776288 -0.10175559\n",
      " -0.06578613  0.11304023 -0.04452744 -0.15696066  0.15325484 -0.21758151\n",
      "  0.00571116 -0.16536263 -0.01452674 -0.003494   -0.17745456  0.55741729\n",
      "  0.14847637 -0.01818754  0.10546759 -0.26499773  0.15762413  0.10911613\n",
      "  0.10844456 -0.09346482  0.11804489 -0.05260196 -0.01048598  0.17023355\n",
      "  0.02064498  0.22954739  0.17187899 -0.09231172 -0.00976594  0.07125512\n",
      "  0.04171887 -0.22216202 -0.10638143 -0.21936518  0.0086959   0.37711943\n",
      "  0.06024948  0.16908837 -0.22219299 -0.15985457]\n",
      "408506\n",
      "training: 30 testing: 21\n",
      "missed           16\n",
      "correctsource    14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           11\n",
      "correctsource    10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.2  0.2  0.25 0.5  0.75 0.75 0.5 ]\n",
      "Accuracy: 0.43333333333333335\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.38      0.36      0.37        14\n",
      "       missed       0.47      0.50      0.48        16\n",
      "\n",
      "    micro avg       0.43      0.43      0.43        30\n",
      "    macro avg       0.43      0.43      0.43        30\n",
      " weighted avg       0.43      0.43      0.43        30\n",
      "\n",
      "accuracy = 0.5238095238095238\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.50      0.50        10\n",
      "       missed       0.55      0.55      0.55        11\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        21\n",
      "    macro avg       0.52      0.52      0.52        21\n",
      " weighted avg       0.52      0.52      0.52        21\n",
      "\n",
      "(64,)\n",
      "[ 3.28997723e-01 -9.27193947e-03  5.37080573e-01 -1.27454483e-01\n",
      " -2.84941490e-01 -4.87689435e-02  3.19236040e-01 -6.23905695e-02\n",
      " -1.16922126e-02 -1.88586950e-02 -4.86497038e-01 -4.26476023e-01\n",
      " -1.06611897e-01 -2.16790524e-01 -2.88659825e-01 -1.18510507e-01\n",
      "  2.09724383e-05 -2.57869056e-01 -2.12162916e-01  1.12589242e-01\n",
      " -2.13853959e-01 -6.63306255e-02 -2.17601738e-01  1.76387455e-01\n",
      " -2.92204925e-02  1.61610449e-01  5.66315769e-01 -2.09165528e-01\n",
      "  5.28139182e-02  1.20927518e-01  1.19670747e-01 -1.09116273e-01\n",
      "  1.55958502e-01 -3.97090386e-02 -1.66743380e-01 -2.86943140e-01\n",
      "  1.03490900e-01  1.95986003e-02 -2.43357312e-01 -8.46241164e-02\n",
      " -1.48765318e-01 -1.32536032e-01 -2.01390974e-01 -3.23601806e-01\n",
      " -2.47543745e-02 -2.56210020e-01 -2.43597347e-02 -6.84738976e-02\n",
      "  7.13958665e-02 -6.26392019e-02 -7.52648208e-02 -1.58494446e-01\n",
      " -9.74998482e-02 -5.10448709e-02  3.15486176e-01 -2.39008167e-01\n",
      " -4.01374555e-02 -1.17795959e-01 -6.11539241e-02  3.55568711e-02\n",
      "  1.48351837e-01 -1.06068405e-01  2.14146889e-01 -2.08906117e-02]\n",
      "413474\n",
      "training: 37 testing: 25\n",
      "correctsource    25\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    17\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.83333333 0.66666667 0.5        0.8        0.5\n",
      " 0.5       ]\n",
      "Accuracy: 0.6216216216216216\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.72      0.72      0.72        25\n",
      "       missed       0.42      0.42      0.42        12\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        37\n",
      "    macro avg       0.57      0.57      0.57        37\n",
      " weighted avg       0.62      0.62      0.62        37\n",
      "\n",
      "accuracy = 0.56\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.71      0.69        17\n",
      "       missed       0.29      0.25      0.27         8\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        25\n",
      "    macro avg       0.48      0.48      0.48        25\n",
      " weighted avg       0.54      0.56      0.55        25\n",
      "\n",
      "(64,)\n",
      "[ 0.04173511 -0.02846676  0.46939981 -0.01133138  0.43366597  0.29823162\n",
      " -0.42942518  0.104809    0.1318924   0.36459131 -0.36847867  0.33458435\n",
      "  0.09851543 -0.23596451  0.20231854 -0.01442515 -0.53419193  0.40030237\n",
      "  0.37833269 -0.12808694  0.5338687   0.11004697 -0.01763844 -0.0275317\n",
      " -0.21250592 -0.16979649 -0.05593803  0.30933051  0.05849775 -0.0614003\n",
      " -0.19602671  0.15326787 -0.04740435 -0.13150537  0.07559179 -0.06208657\n",
      " -0.25970187  0.12392658  0.01343826 -0.06854099 -0.30420082  0.45073441\n",
      " -0.07412149 -0.24689461 -0.35918117 -0.12831095  0.07984641 -0.05440124\n",
      " -0.00727611 -0.08134887  0.17455818 -0.09357432 -0.01034066  0.37374397\n",
      "  0.17547407  0.07331528 -0.56851484  0.38970144 -0.19293932  0.05463626\n",
      "  0.06314451 -0.2524507   0.29548126 -0.24584754]\n",
      "437101\n",
      "training: 36 testing: 25\n",
      "correctsource    22\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.4        0.2        0.8        0.8        0.4\n",
      " 0.6       ]\n",
      "Accuracy: 0.5555555555555556\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.55      0.60        22\n",
      "       missed       0.44      0.57      0.50        14\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        36\n",
      "    macro avg       0.56      0.56      0.55        36\n",
      " weighted avg       0.58      0.56      0.56        36\n",
      "\n",
      "accuracy = 0.76\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.81      0.81      0.81        16\n",
      "       missed       0.67      0.67      0.67         9\n",
      "\n",
      "    micro avg       0.76      0.76      0.76        25\n",
      "    macro avg       0.74      0.74      0.74        25\n",
      " weighted avg       0.76      0.76      0.76        25\n",
      "\n",
      "(64,)\n",
      "[-0.22236238 -0.30387183 -0.19629896  0.25247969  0.16808653 -0.11119016\n",
      " -0.03853313  0.07559261  0.09967115 -0.16337209  0.06436885  0.26298189\n",
      " -0.02563166  0.13372276 -0.16050554  0.08453149  0.07721785  0.1027344\n",
      "  0.32298688 -0.0780304   0.27836959  0.27323299 -0.04237519 -0.08335255\n",
      "  0.28657703  0.34324422 -0.3421244   0.06925218  0.10848582 -0.26890202\n",
      "  0.4959601   0.37999792  0.4076292  -0.33931115  0.73558059 -0.17443643\n",
      "  0.00249533 -0.06517021  0.26280161  0.29366206 -0.16339207 -0.22188419\n",
      " -0.05352817  0.32149481 -0.03825823 -0.15141647 -0.1987676  -0.10988601\n",
      " -0.1663882   0.32959145  0.05978394 -0.154487   -0.35846386  0.07498214\n",
      "  0.1668844   0.08649408 -0.35897156  0.10526972 -0.09254738  0.25962643\n",
      "  0.02286846 -0.37930972  0.1067548  -0.64725534]\n",
      "439776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 44 testing: 30\n",
      "correctsource    33\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    23\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.42857143 0.85714286 0.71428571 0.57142857 0.66666667 0.6\n",
      " 0.6       ]\n",
      "Accuracy: 0.6363636363636364\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.79      0.70      0.74        33\n",
      "       missed       0.33      0.45      0.38        11\n",
      "\n",
      "    micro avg       0.64      0.64      0.64        44\n",
      "    macro avg       0.56      0.58      0.56        44\n",
      " weighted avg       0.68      0.64      0.65        44\n",
      "\n",
      "accuracy = 0.7333333333333333\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.80      0.87      0.83        23\n",
      "       missed       0.40      0.29      0.33         7\n",
      "\n",
      "    micro avg       0.73      0.73      0.73        30\n",
      "    macro avg       0.60      0.58      0.58        30\n",
      " weighted avg       0.71      0.73      0.72        30\n",
      "\n",
      "(64,)\n",
      "[-0.4488761  -0.35506191 -0.05962309  0.01722569 -0.21102033 -0.08708973\n",
      " -0.06244639 -0.49323731  0.22101074  0.02283705  0.63878361  0.07682766\n",
      "  0.33972586  0.17888874 -0.02485171  0.35374873  0.55729948  0.01788272\n",
      " -0.39437229  0.48734514 -0.13526464 -0.09583095 -0.24851745  0.24420707\n",
      " -0.09752592 -0.1979599   0.22867561  0.26615702  0.34947973  0.29191576\n",
      " -0.33792877 -0.55545261  0.15822879  0.57306355 -0.04309425 -0.26875176\n",
      " -0.31556855 -0.51727561 -0.27041578 -0.23929692 -0.23450753  0.01154646\n",
      "  0.00940695 -0.09313912  0.0837145  -0.43862376  0.31215158  0.02465142\n",
      "  0.02313185  0.0821192  -0.07594761 -0.42854585 -0.00854799 -0.2826804\n",
      "  0.21032877  0.45711544  0.32682784  0.09404878  0.13591097  0.3391676\n",
      "  0.44904805  0.20595134  0.05459845  0.26184793]\n",
      "458807\n",
      "training: 36 testing: 24\n",
      "correctsource    21\n",
      "missed           15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.8        0.6        0.4        0.8        0.8\n",
      " 0.4       ]\n",
      "Accuracy: 0.5833333333333334\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.71      0.67        21\n",
      "       missed       0.50      0.40      0.44        15\n",
      "\n",
      "    micro avg       0.58      0.58      0.58        36\n",
      "    macro avg       0.56      0.56      0.56        36\n",
      " weighted avg       0.57      0.58      0.57        36\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.29      0.40        14\n",
      "       missed       0.44      0.80      0.57        10\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        24\n",
      "    macro avg       0.56      0.54      0.49        24\n",
      " weighted avg       0.57      0.50      0.47        24\n",
      "\n",
      "(64,)\n",
      "[ 0.1593834   0.01953109  0.23268148 -0.18836687 -0.15256777  0.10096232\n",
      " -0.44383786 -0.10322271 -0.27691378 -0.08918069  0.06884325  0.24535693\n",
      " -0.00570086  0.08667757  0.19645454 -0.26534811  0.31176865 -0.05814281\n",
      " -0.17984244  0.10157946 -0.05071395 -0.18957026  0.42966011  0.07809268\n",
      " -0.28744455  0.03890445 -0.13528377  0.01966537  0.36830911 -0.145342\n",
      "  0.13528478  0.1342892   0.09985402  0.0027812   0.26505284  0.37087798\n",
      " -0.33530229  0.22823295  0.19872512  0.2522872  -0.18603445  0.07623797\n",
      " -0.12027973  0.12764208  0.18881323 -0.2161736   0.09948057 -0.26773393\n",
      " -0.2897745   0.28312597  0.29920535  0.1906703  -0.082016    0.17627485\n",
      " -0.02350433  0.05035319  0.08934802 -0.17600181 -0.31367825 -0.48215099\n",
      " -0.0526729   0.03565624 -0.22057583  0.39957693]\n",
      "459801\n",
      "training: 31 testing: 21\n",
      "correctsource    20\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    13\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[1.         0.6        0.8        0.4        0.75       1.\n",
      " 0.66666667]\n",
      "Accuracy: 0.7419354838709677\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.77      0.85      0.81        20\n",
      "       missed       0.67      0.55      0.60        11\n",
      "\n",
      "    micro avg       0.74      0.74      0.74        31\n",
      "    macro avg       0.72      0.70      0.70        31\n",
      " weighted avg       0.74      0.74      0.74        31\n",
      "\n",
      "accuracy = 0.6666666666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.92      0.77        13\n",
      "       missed       0.67      0.25      0.36         8\n",
      "\n",
      "    micro avg       0.67      0.67      0.67        21\n",
      "    macro avg       0.67      0.59      0.57        21\n",
      " weighted avg       0.67      0.67      0.62        21\n",
      "\n",
      "(64,)\n",
      "[-0.09842151 -0.13391276  0.03404614  0.08567123  0.18491948  0.11119541\n",
      "  0.04739206  0.03157363 -0.02827573  0.05543356  0.09681932  0.0273105\n",
      "  0.05620481  0.01276324 -0.00364206 -0.32244623 -0.26090791 -0.30160348\n",
      " -0.01183405  0.04301167 -0.31397011  0.15046139  0.14018824 -0.14058457\n",
      " -0.0471395   0.06802196 -0.33573735 -0.0064792   0.24883831 -0.18502912\n",
      "  0.10739082 -0.13380033  0.01971576 -0.19563935  0.19885268  0.00356308\n",
      "  0.03154291 -0.03844469 -0.1322534  -0.06318389  0.06682409  0.25127307\n",
      " -0.06298565  0.26786702  0.07644201 -0.01063119  0.15846356  0.30582006\n",
      "  0.16645926 -0.08466114 -0.00289192  0.29721866  0.27140875 -0.15657897\n",
      "  0.15115546 -0.05058048 -0.13111232  0.05791751 -0.10670748  0.24056177\n",
      "  0.11032748  0.06624619  0.12050065  0.0223048 ]\n",
      "484204\n",
      "training: 39 testing: 26\n",
      "correctsource    22\n",
      "missed           17\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    15\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.85714286 0.66666667 0.5        0.6        0.6        0.6\n",
      " 0.8       ]\n",
      "Accuracy: 0.6666666666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.68      0.77      0.72        22\n",
      "       missed       0.64      0.53      0.58        17\n",
      "\n",
      "    micro avg       0.67      0.67      0.67        39\n",
      "    macro avg       0.66      0.65      0.65        39\n",
      " weighted avg       0.66      0.67      0.66        39\n",
      "\n",
      "accuracy = 0.5384615384615384\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.60      0.60        15\n",
      "       missed       0.45      0.45      0.45        11\n",
      "\n",
      "    micro avg       0.54      0.54      0.54        26\n",
      "    macro avg       0.53      0.53      0.53        26\n",
      " weighted avg       0.54      0.54      0.54        26\n",
      "\n",
      "(64,)\n",
      "[-0.31541716  0.62743689  0.02860387  0.05432268 -0.3478403   0.00269222\n",
      " -0.03995771  0.15720222  0.06827267 -0.57977528  0.32737216  0.23641755\n",
      "  0.25286817 -0.47329835 -0.19903454  0.07679641 -0.27790338  0.14095376\n",
      "  0.21575517 -0.32059235 -0.01736158 -0.62310343 -0.08301513 -0.01335363\n",
      "  0.0887751   0.12060689  0.03561483  0.37930595  0.10693276  0.0956682\n",
      "  0.54678039  0.03276138  0.05496576 -0.32240218  0.12946954 -0.01621869\n",
      "  0.37762017  0.05827318  0.3932421  -0.34604666 -0.39593673  0.06956239\n",
      " -0.20801754  0.20981125 -0.25278679 -0.15042448  0.0263552  -0.05880972\n",
      "  0.20611205 -0.27307732  0.06630603 -0.40876297  0.05727851  0.37085662\n",
      " -0.43752697 -0.12861604 -0.42175031  0.11788744 -0.06431722  0.39787873\n",
      " -0.03692865 -0.29943009 -0.21731043  0.03731387]\n",
      "502616\n",
      "training: 31 testing: 21\n",
      "correctsource    21\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    15\n",
      "missed            6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.2  0.6  0.6  0.75 1.   0.75 0.5 ]\n",
      "Accuracy: 0.6129032258064516\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.71      0.71      0.71        21\n",
      "       missed       0.40      0.40      0.40        10\n",
      "\n",
      "    micro avg       0.61      0.61      0.61        31\n",
      "    macro avg       0.56      0.56      0.56        31\n",
      " weighted avg       0.61      0.61      0.61        31\n",
      "\n",
      "accuracy = 0.5714285714285714\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.75      0.60      0.67        15\n",
      "       missed       0.33      0.50      0.40         6\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        21\n",
      "    macro avg       0.54      0.55      0.53        21\n",
      " weighted avg       0.63      0.57      0.59        21\n",
      "\n",
      "(64,)\n",
      "[ 0.09259659  0.19281128  0.12801979  0.23766127  0.01103704  0.08590015\n",
      "  0.00982713 -0.08615093 -0.09623072  0.09334188 -0.19777803 -0.31789935\n",
      "  0.032982   -0.11542388 -0.02282223 -0.0405877   0.0230282  -0.04317209\n",
      "  0.03539933 -0.07535102 -0.12072043 -0.01582704 -0.0081254  -0.08930409\n",
      "  0.10931969 -0.13360708 -0.18883785 -0.50426017  0.01423451 -0.02483417\n",
      "  0.14362561 -0.22171853  0.26526908  0.05562418 -0.29286751  0.10919673\n",
      "  0.04078771 -0.01395271  0.09108819  0.0366033   0.26379433  0.04022151\n",
      " -0.00275519 -0.17078019 -0.09780204  0.04494209  0.2522915  -0.00863582\n",
      "  0.08752415 -0.17810804  0.1731898   0.13209783  0.1988719  -0.03793187\n",
      "  0.28065172 -0.18606122 -0.28378717 -0.12249967 -0.02948485 -0.21300688\n",
      " -0.0071828   0.0378852   0.03968068  0.05116051]\n",
      "567214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 29 testing: 20\n",
      "correctsource    17\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    12\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.8        0.2        0.6        0.75       0.5        0.33333333\n",
      " 0.66666667]\n",
      "Accuracy: 0.5517241379310345\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.61      0.65      0.63        17\n",
      "       missed       0.45      0.42      0.43        12\n",
      "\n",
      "    micro avg       0.55      0.55      0.55        29\n",
      "    macro avg       0.53      0.53      0.53        29\n",
      " weighted avg       0.55      0.55      0.55        29\n",
      "\n",
      "accuracy = 0.65\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.83      0.74        12\n",
      "       missed       0.60      0.38      0.46         8\n",
      "\n",
      "    micro avg       0.65      0.65      0.65        20\n",
      "    macro avg       0.63      0.60      0.60        20\n",
      " weighted avg       0.64      0.65      0.63        20\n",
      "\n",
      "(64,)\n",
      "[-0.27260199  0.04383156  0.18670958 -0.18225086  0.04110907 -0.04552213\n",
      " -0.27611937  0.01177949 -0.09999487 -0.20460991 -0.21182736 -0.12487413\n",
      "  0.08381591  0.25552753  0.08751258  0.11694995  0.05439344  0.02393265\n",
      " -0.15753175  0.29457634  0.02433558  0.07602143  0.13167041  0.1349526\n",
      " -0.08867673 -0.3024449  -0.00415074  0.18626065  0.05001774  0.12657465\n",
      " -0.10135082  0.15202621  0.17647562  0.33190651  0.17159632  0.02875179\n",
      "  0.23511745  0.02070068 -0.04885376  0.21816815 -0.31258659 -0.24674762\n",
      " -0.01119868  0.16297733  0.22426209  0.06331724 -0.15018926 -0.1823286\n",
      " -0.41261389 -0.45246544 -0.19984386  0.01816171  0.36661758  0.18835059\n",
      " -0.16004237 -0.02889069 -0.11422195 -0.08940884 -0.29312579  0.17675759\n",
      "  0.31247553  0.06727441  0.11255552  0.30885751]\n",
      "597569\n",
      "training: 39 testing: 27\n",
      "correctsource    28\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    20\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.66666667 0.33333333 0.5        0.6        0.6\n",
      " 0.8       ]\n",
      "Accuracy: 0.5641025641025641\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.70      0.68      0.69        28\n",
      "       missed       0.25      0.27      0.26        11\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        39\n",
      "    macro avg       0.48      0.48      0.48        39\n",
      " weighted avg       0.58      0.56      0.57        39\n",
      "\n",
      "accuracy = 0.5925925925925926\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.74      0.70      0.72        20\n",
      "       missed       0.25      0.29      0.27         7\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        27\n",
      "    macro avg       0.49      0.49      0.49        27\n",
      " weighted avg       0.61      0.59      0.60        27\n",
      "\n",
      "(64,)\n",
      "[ 0.08473856  0.40278356 -0.00752442  0.02167182 -0.37836607  0.0705533\n",
      " -0.30381236  0.01986965 -0.1042939  -0.13964412 -0.1352404  -0.26286383\n",
      " -0.12375185  0.10949793  0.00592532  0.06968421  0.59977939  0.20435881\n",
      " -0.03258316 -0.26535036  0.02725876  0.21383551  0.25411929 -0.12260005\n",
      "  0.29541981  0.02585602  0.18766512  0.0174321   0.02980946 -0.19748889\n",
      " -0.02353252 -0.03026029  0.30191726  0.3801019   0.15606369 -0.22574708\n",
      " -0.48422009  0.12988233  0.11400977 -0.35088695 -0.07075098 -0.2554163\n",
      " -0.08423648  0.028252    0.06480298 -0.04334945  0.0981468   0.3340269\n",
      "  0.08867724 -0.07292559 -0.00946416 -0.0148802   0.10466839 -0.28102591\n",
      "  0.44237116 -0.28353715  0.10987225  0.0012778  -0.32166188 -0.10914434\n",
      " -0.14025297  0.22046992  0.3688993  -0.42112557]\n",
      "652850\n",
      "training: 34 testing: 23\n",
      "missed           21\n",
      "correctsource    13\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           15\n",
      "correctsource     8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.4 0.6 0.6 0.8 0.6 0.6 0.5]\n",
      "Accuracy: 0.5882352941176471\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.45      0.38      0.42        13\n",
      "       missed       0.65      0.71      0.68        21\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        34\n",
      "    macro avg       0.55      0.55      0.55        34\n",
      " weighted avg       0.58      0.59      0.58        34\n",
      "\n",
      "accuracy = 0.4782608695652174\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.25      0.25      0.25         8\n",
      "       missed       0.60      0.60      0.60        15\n",
      "\n",
      "    micro avg       0.48      0.48      0.48        23\n",
      "    macro avg       0.42      0.42      0.42        23\n",
      " weighted avg       0.48      0.48      0.48        23\n",
      "\n",
      "(64,)\n",
      "[ 1.50392265e-01  2.02955783e-01  1.20955766e-01 -1.41160427e-03\n",
      " -2.67927977e-01 -5.50726783e-02  6.39822484e-02  8.31306272e-02\n",
      " -9.61110493e-02  1.77676396e-01  2.52801611e-01  1.77889071e-01\n",
      " -7.29985811e-02 -1.49532273e-01  4.04013025e-01  1.64204912e-01\n",
      " -9.46308432e-02  8.55830453e-02  1.65607564e-01 -3.17555245e-01\n",
      " -8.96121292e-02  1.35597356e-01  4.62611847e-02  1.38228466e-01\n",
      "  4.99627747e-03 -8.02179694e-02  1.70631324e-02 -2.42281618e-02\n",
      " -5.94421603e-02  2.49055595e-01  1.40841902e-01 -4.79009165e-02\n",
      " -3.09409239e-01  2.96228060e-02  1.35316066e-01  1.14835456e-01\n",
      "  9.01874102e-02 -2.06988109e-01  2.88490359e-01  1.83784829e-01\n",
      "  8.26121694e-02 -2.83429462e-02 -1.50046467e-02  3.89068101e-01\n",
      " -1.38270878e-01  1.95383323e-01  7.53168511e-02 -9.58433200e-03\n",
      " -3.93880230e-01 -1.20268935e-01  4.73854664e-02 -1.99367914e-01\n",
      "  3.47749963e-05 -1.99573205e-01  1.65498837e-02  2.08064519e-01\n",
      " -2.14870871e-01  2.03690403e-01  2.64446179e-02  1.53805839e-01\n",
      " -4.79098465e-02 -3.01839669e-01 -2.37375555e-01  2.58706492e-01]\n",
      "677561\n",
      "training: 32 testing: 22\n",
      "correctsource    20\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6 0.2 0.6 0.6 0.4 1.  0. ]\n",
      "Accuracy: 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.61      0.55      0.58        20\n",
      "       missed       0.36      0.42      0.38        12\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        32\n",
      "    macro avg       0.48      0.48      0.48        32\n",
      " weighted avg       0.52      0.50      0.51        32\n",
      "\n",
      "accuracy = 0.5454545454545454\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.71      0.67        14\n",
      "       missed       0.33      0.25      0.29         8\n",
      "\n",
      "    micro avg       0.55      0.55      0.55        22\n",
      "    macro avg       0.48      0.48      0.48        22\n",
      " weighted avg       0.52      0.55      0.53        22\n",
      "\n",
      "(64,)\n",
      "[-0.34521219  0.01766163 -0.14263537 -0.07068915  0.1511171  -0.28389567\n",
      "  0.35587029 -0.08150578  0.2532671   0.15248627 -0.24797608 -0.01787047\n",
      "  0.07182477 -0.26572683  0.18231649  0.28570091 -0.14738584  0.13737075\n",
      "  0.08969855  0.03764438 -0.42268909  0.13844239 -0.1018983   0.06352129\n",
      "  0.28510861  0.25313513 -0.15889346 -0.06333478 -0.32727954  0.31176318\n",
      "  0.1333944  -0.51808488 -0.22448721 -0.12167496  0.08028666  0.0317845\n",
      "  0.18373987 -0.21145863  0.3062711   0.12144415 -0.0692151  -0.09731556\n",
      " -0.18258788  0.06702791  0.05235535  0.06923023 -0.01271673 -0.19180343\n",
      " -0.02628073 -0.05029442  0.02590518  0.17031138 -0.01585289  0.06346851\n",
      "  0.08710095 -0.49283072  0.13305281  0.40345457  0.15108314  0.05740465\n",
      "  0.14975207 -0.18238389 -0.0398918  -0.17923596]\n",
      "711830\n",
      "training: 36 testing: 25\n",
      "missed           21\n",
      "correctsource    15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           14\n",
      "correctsource    11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.4        0.6        0.6        0.4        0.6\n",
      " 0.6       ]\n",
      "Accuracy: 0.5555555555555556\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.47      0.53      0.50        15\n",
      "       missed       0.63      0.57      0.60        21\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        36\n",
      "    macro avg       0.55      0.55      0.55        36\n",
      " weighted avg       0.56      0.56      0.56        36\n",
      "\n",
      "accuracy = 0.56\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.36      0.42        11\n",
      "       missed       0.59      0.71      0.65        14\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        25\n",
      "    macro avg       0.54      0.54      0.53        25\n",
      " weighted avg       0.55      0.56      0.55        25\n",
      "\n",
      "(64,)\n",
      "[ 0.28473028 -0.42112956 -0.33349491 -0.55454584 -0.09874533  0.22518164\n",
      "  0.34035979 -0.27550009 -0.15758849 -0.02313167 -0.20195254 -0.00806838\n",
      "  0.40347449 -0.28190607  0.06101453  0.15360943 -0.29643824 -0.00280176\n",
      " -0.34999524 -0.04737245 -0.02063994  0.22137524  0.15784086  0.01355839\n",
      "  0.56308304 -0.06961791  0.44378543 -0.04581973 -0.34093018  0.34694744\n",
      "  0.11600808  0.09129664 -0.07913156  0.62768938  0.04413908  0.05190105\n",
      " -0.39114659 -0.13968766 -0.0894183  -0.60863618  0.1568608   0.09166632\n",
      "  0.36250023 -0.08126658  0.14425479  0.01350078 -0.12246374 -0.09494907\n",
      "  0.15204851  0.21313768 -0.65005163  0.24160961  0.37940947  0.49394395\n",
      "  0.16059331  0.25038277 -0.20743841  0.08922397 -0.24145236 -0.02099795\n",
      " -0.41056458  0.31966157  0.23842883  0.28791731]\n",
      "729722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 40 testing: 28\n",
      "correctsource    26\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    18\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.5        1.         0.66666667 1.         1.\n",
      " 0.8       ]\n",
      "Accuracy: 0.8\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.88      0.81      0.84        26\n",
      "       missed       0.69      0.79      0.73        14\n",
      "\n",
      "    micro avg       0.80      0.80      0.80        40\n",
      "    macro avg       0.78      0.80      0.79        40\n",
      " weighted avg       0.81      0.80      0.80        40\n",
      "\n",
      "accuracy = 0.6428571428571429\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.79      0.61      0.69        18\n",
      "       missed       0.50      0.70      0.58        10\n",
      "\n",
      "    micro avg       0.64      0.64      0.64        28\n",
      "    macro avg       0.64      0.66      0.64        28\n",
      " weighted avg       0.68      0.64      0.65        28\n",
      "\n",
      "(64,)\n",
      "[ 0.10998976 -0.15386138 -0.13346191  0.44857986  0.14057907  0.01668244\n",
      "  0.02595333 -0.25042733 -0.05787805  0.17587679 -0.00903218  0.12012188\n",
      "  0.25529729  0.41646583  0.01222764  0.07127264 -0.10742238 -0.14481433\n",
      " -0.3263627  -0.0715609   0.04264468 -0.12450442 -0.01009954  0.12348868\n",
      " -0.28519015 -0.39236577 -0.026105   -0.00945844  0.17792686 -0.35246993\n",
      "  0.40981139  0.14280337  0.25038452 -0.04760306  0.17754206 -0.07270735\n",
      " -0.10933748  0.03628562  0.27457658 -0.03642656  0.07663941 -0.12792911\n",
      " -0.16111306 -0.03796766 -0.01429336  0.01666958  0.00716064  0.09227133\n",
      "  0.31204632 -0.03014796 -0.07994204  0.20545954 -0.01906151 -0.10352604\n",
      " -0.00799982 -0.05937681 -0.31337146 -0.28036214 -0.06185047 -0.03272273\n",
      " -0.13260638  0.20091313 -0.16574314 -0.04156597]\n",
      "739694\n",
      "training: 32 testing: 22\n",
      "correctsource    20\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6        0.8        0.4        1.         0.2        0.5\n",
      " 0.33333333]\n",
      "Accuracy: 0.5625\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.60      0.63        20\n",
      "       missed       0.43      0.50      0.46        12\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        32\n",
      "    macro avg       0.55      0.55      0.55        32\n",
      " weighted avg       0.58      0.56      0.57        32\n",
      "\n",
      "accuracy = 0.7727272727272727\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.91      0.71      0.80        14\n",
      "       missed       0.64      0.88      0.74         8\n",
      "\n",
      "    micro avg       0.77      0.77      0.77        22\n",
      "    macro avg       0.77      0.79      0.77        22\n",
      " weighted avg       0.81      0.77      0.78        22\n",
      "\n",
      "(64,)\n",
      "[ 0.00221806 -0.10780823 -0.08907762 -0.17252777 -0.01014106 -0.26085192\n",
      " -0.40752077  0.02936651 -0.30064707  0.37172621 -0.05817218  0.21773961\n",
      "  0.02728436  0.26166046 -0.27497543 -0.50943029  0.08988152 -0.33146301\n",
      "  0.02051558 -0.28360954  0.24280569  0.29980352  0.10186601  0.2202061\n",
      " -0.38187335 -0.20699324  0.07080903 -0.08875347  0.24455754  0.16240693\n",
      " -0.05231781 -0.26533989 -0.06705803 -0.4633905   0.33328306  0.08410364\n",
      "  0.39899182  0.04904992  0.01493558 -0.44349743 -0.23107605  0.21085912\n",
      "  0.21359257 -0.0019982  -0.05153306  0.03994802 -0.19743259  0.07482651\n",
      " -0.2766585   0.39560016  0.07252907 -0.15116817 -0.19971399  0.01811472\n",
      "  0.00270268  0.08969344  0.24762062  0.10943065  0.06581235  0.30647842\n",
      "  0.05139411  0.06864038 -0.05293211  0.00275539]\n",
      "748676\n",
      "training: 36 testing: 24\n",
      "missed           26\n",
      "correctsource    10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           18\n",
      "correctsource     6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.33333333 0.33333333 0.2        0.6        0.5\n",
      " 0.75      ]\n",
      "Accuracy: 0.4166666666666667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.13      0.20      0.16        10\n",
      "       missed       0.62      0.50      0.55        26\n",
      "\n",
      "    micro avg       0.42      0.42      0.42        36\n",
      "    macro avg       0.38      0.35      0.36        36\n",
      " weighted avg       0.48      0.42      0.44        36\n",
      "\n",
      "accuracy = 0.5416666666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.14      0.17      0.15         6\n",
      "       missed       0.71      0.67      0.69        18\n",
      "\n",
      "    micro avg       0.54      0.54      0.54        24\n",
      "    macro avg       0.42      0.42      0.42        24\n",
      " weighted avg       0.57      0.54      0.55        24\n",
      "\n",
      "(64,)\n",
      "[ 0.13060276  0.20359339 -0.19979471 -0.08725992  0.61413173  0.38535102\n",
      "  0.13631954 -0.09157228  0.20518486 -0.41710795 -0.17986629 -0.1908888\n",
      "  0.01290491  0.61743892 -0.63284394  0.31215444  0.58189182 -0.00689624\n",
      "  0.02445796  0.14134649  0.7220403   0.47548787  0.44191354  0.14398589\n",
      "  0.17637838 -0.16600588  0.0357729  -0.73779522 -0.30962636  0.35044816\n",
      " -0.09160296 -0.46900774 -0.22362093  0.20180886  0.00610184  0.11002211\n",
      "  0.33487586 -0.1536206   0.04261351  0.35581642  0.41919179 -0.00938711\n",
      " -0.27142581 -0.72419366 -0.18621694 -0.17271404  0.2688972  -0.13593524\n",
      "  0.09474873  0.47607695 -0.33225907 -0.31119132  0.12588717 -0.19668803\n",
      " -0.05065974  0.45384897 -0.14368497  0.02138996 -0.29403448 -0.16393589\n",
      "  0.38239201 -0.53698636 -0.12208847 -0.70906327]\n",
      "778749\n",
      "training: 33 testing: 22\n",
      "missed           19\n",
      "correctsource    14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           13\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6 0.6 0.6 0.6 0.6 0.5 0.5]\n",
      "Accuracy: 0.5757575757575758\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.71      0.59        14\n",
      "       missed       0.69      0.47      0.56        19\n",
      "\n",
      "    micro avg       0.58      0.58      0.58        33\n",
      "    macro avg       0.60      0.59      0.58        33\n",
      " weighted avg       0.61      0.58      0.57        33\n",
      "\n",
      "accuracy = 0.5454545454545454\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.45      0.56      0.50         9\n",
      "       missed       0.64      0.54      0.58        13\n",
      "\n",
      "    micro avg       0.55      0.55      0.55        22\n",
      "    macro avg       0.55      0.55      0.54        22\n",
      " weighted avg       0.56      0.55      0.55        22\n",
      "\n",
      "(64,)\n",
      "[ 0.09604739  0.02356189  0.0700473   0.17662448  0.05130515  0.03432235\n",
      " -0.00533099 -0.3216747   0.02081015 -0.02772598 -0.05595106  0.14613902\n",
      " -0.34257213  0.03940206  0.02120928 -0.21801923 -0.20857782  0.10163425\n",
      "  0.46688915 -0.12679939 -0.2032256   0.07909387 -0.31377607  0.14805058\n",
      "  0.10343189 -0.14276278 -0.18687482  0.42505012  0.08039814 -0.19489767\n",
      " -0.01408169 -0.17603952 -0.24656621  0.06515338  0.05247565 -0.03395666\n",
      "  0.29872283 -0.07015024  0.07941229 -0.02045488  0.25016495  0.33231743\n",
      " -0.23984893 -0.02149052 -0.08668087 -0.04477563  0.26841847 -0.06211649\n",
      "  0.31973929 -0.35500983  0.00248069 -0.00605451  0.30992629  0.07402809\n",
      "  0.02307674  0.04980547 -0.12973903  0.31560181  0.01222133 -0.16416547\n",
      "  0.2284089   0.02696036 -0.06752091  0.17018807]\n",
      "783781\n",
      "training: 34 testing: 23\n",
      "correctsource    24\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.66666667 0.66666667 0.75       0.75       0.75\n",
      " 1.        ]\n",
      "Accuracy: 0.7352941176470589\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.80      0.83      0.82        24\n",
      "       missed       0.56      0.50      0.53        10\n",
      "\n",
      "    micro avg       0.74      0.74      0.74        34\n",
      "    macro avg       0.68      0.67      0.67        34\n",
      " weighted avg       0.73      0.74      0.73        34\n",
      "\n",
      "accuracy = 0.6956521739130435\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.71      0.94      0.81        16\n",
      "       missed       0.50      0.14      0.22         7\n",
      "\n",
      "    micro avg       0.70      0.70      0.70        23\n",
      "    macro avg       0.61      0.54      0.52        23\n",
      " weighted avg       0.65      0.70      0.63        23\n",
      "\n",
      "(64,)\n",
      "[-0.15123317  0.05256545 -0.19530731  0.12514743  0.13723478 -0.10146463\n",
      "  0.05306985 -0.07406555 -0.13119006 -0.13924321  0.05479002  0.0312849\n",
      " -0.30046577 -0.06140344  0.09335951 -0.14498713 -0.09957126  0.04029819\n",
      " -0.29324669 -0.25956602 -0.01504499 -0.05347142 -0.26314648  0.07989897\n",
      " -0.1682247  -0.19231513 -0.09883968 -0.38352476  0.07449005  0.02618701\n",
      " -0.16782786  0.19000601  0.28920775  0.3131627  -0.15472239 -0.03192328\n",
      " -0.08837411 -0.12196143  0.02829764 -0.31330464 -0.08115833 -0.05538297\n",
      "  0.16771405  0.16830635  0.15626667  0.00899808 -0.04007448  0.26395192\n",
      "  0.02676893  0.09712385  0.05396414  0.31681388  0.29440858 -0.12026386\n",
      "  0.21706573  0.02964843 -0.13020641  0.0921265  -0.24867114 -0.10384359\n",
      " -0.11489829  0.00806001 -0.45796106 -0.03161823]\n",
      "884343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 32 testing: 22\n",
      "correctsource    17\n",
      "missed           15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    12\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.6        0.6        0.75       0.         0.5\n",
      " 0.5       ]\n",
      "Accuracy: 0.53125\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.55      0.65      0.59        17\n",
      "       missed       0.50      0.40      0.44        15\n",
      "\n",
      "    micro avg       0.53      0.53      0.53        32\n",
      "    macro avg       0.53      0.52      0.52        32\n",
      " weighted avg       0.53      0.53      0.52        32\n",
      "\n",
      "accuracy = 0.3181818181818182\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.36      0.33      0.35        12\n",
      "       missed       0.27      0.30      0.29        10\n",
      "\n",
      "    micro avg       0.32      0.32      0.32        22\n",
      "    macro avg       0.32      0.32      0.32        22\n",
      " weighted avg       0.32      0.32      0.32        22\n",
      "\n",
      "(64,)\n",
      "[ 0.2950498   0.00547954  0.13772528 -0.04188824  0.47470556 -0.16745075\n",
      " -0.10473933 -0.17580904  0.2600308   0.14690948  0.03061892  0.17221274\n",
      " -0.50898976  0.14281571  0.20480071  0.1921701   0.37063225 -0.06785541\n",
      " -0.21808234  0.46962555 -0.15202716  0.1741114  -0.12754367 -0.31738243\n",
      "  0.00328674  0.13156347 -0.12731288 -0.11256726  0.19849493  0.21653595\n",
      " -0.41886111 -0.06308711  0.16837871  0.36337867  0.07283272 -0.12585501\n",
      "  0.31617182  0.28698883  0.12042796  0.17900808 -0.00381849 -0.10476108\n",
      " -0.08234682  0.05432927 -0.11838839 -0.21397889  0.12557454 -0.11229923\n",
      "  0.17374988 -0.02249843 -0.15874948  0.04637448  0.19746607  0.04625803\n",
      " -0.14816509 -0.02601704 -0.11988864  0.61584523  0.07673998 -0.0140808\n",
      " -0.00799098  0.01957575  0.22279273  0.17724495]\n",
      "886007\n",
      "training: 39 testing: 26\n",
      "correctsource    27\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    18\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.5        0.66666667 0.66666667 0.66666667 0.4\n",
      " 0.5       ]\n",
      "Accuracy: 0.5897435897435898\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.70      0.70      0.70        27\n",
      "       missed       0.33      0.33      0.33        12\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        39\n",
      "    macro avg       0.52      0.52      0.52        39\n",
      " weighted avg       0.59      0.59      0.59        39\n",
      "\n",
      "accuracy = 0.5769230769230769\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.73      0.61      0.67        18\n",
      "       missed       0.36      0.50      0.42         8\n",
      "\n",
      "    micro avg       0.58      0.58      0.58        26\n",
      "    macro avg       0.55      0.56      0.54        26\n",
      " weighted avg       0.62      0.58      0.59        26\n",
      "\n",
      "(64,)\n",
      "[ 3.09965987e-01 -1.66933613e-01 -3.24456946e-01 -3.10607608e-01\n",
      "  2.90423394e-01  1.38111905e-01 -1.70216249e-03 -1.56997559e-01\n",
      "  1.33166123e-02 -7.01454769e-02 -2.32508070e-01 -1.14956500e-01\n",
      " -2.45350793e-01 -8.29438650e-02 -2.81053868e-01 -3.38010199e-02\n",
      "  3.17778413e-01  4.84175794e-01  2.20519109e-01  7.03330155e-01\n",
      "  3.50409606e-01  6.12710332e-02  3.02529120e-01  6.20314577e-02\n",
      " -2.84034053e-02  8.72990195e-02 -1.52000378e-02 -6.98394959e-02\n",
      " -2.60061268e-01 -1.68597161e-01  3.75645631e-01  5.01677045e-01\n",
      "  4.98930769e-01 -3.32119041e-01 -4.06909520e-02 -1.13465388e-01\n",
      " -1.69315420e-01 -1.10884547e-01 -1.02970561e-01 -2.96063979e-01\n",
      " -7.01211033e-04 -2.80131394e-01  2.21264625e-01  1.27966844e-01\n",
      " -3.46732001e-01 -3.27803922e-01 -9.86068409e-02  3.98954441e-01\n",
      "  6.42030325e-03  2.07936868e-02 -2.44008524e-01 -3.13227609e-01\n",
      " -7.71542339e-02  6.93883087e-02 -9.71785476e-03 -5.18571174e-01\n",
      "  2.59258654e-01 -2.37140085e-01  1.31421478e-01  1.19284717e-01\n",
      "  1.10852116e-01  7.35392372e-01 -1.34820312e-01 -7.25802057e-03]\n",
      "936730\n",
      "training: 36 testing: 24\n",
      "missed           18\n",
      "correctsource    18\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           12\n",
      "correctsource    12\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.16666667 0.5        0.16666667 0.5        0.         0.25\n",
      " 0.5       ]\n",
      "Accuracy: 0.3055555555555556\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.33      0.39      0.36        18\n",
      "       missed       0.27      0.22      0.24        18\n",
      "\n",
      "    micro avg       0.31      0.31      0.31        36\n",
      "    macro avg       0.30      0.31      0.30        36\n",
      " weighted avg       0.30      0.31      0.30        36\n",
      "\n",
      "accuracy = 0.5833333333333334\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.57      0.67      0.62        12\n",
      "       missed       0.60      0.50      0.55        12\n",
      "\n",
      "    micro avg       0.58      0.58      0.58        24\n",
      "    macro avg       0.59      0.58      0.58        24\n",
      " weighted avg       0.59      0.58      0.58        24\n",
      "\n",
      "(64,)\n",
      "[ 0.04303753  0.06922361  0.11023628 -0.0028979  -0.1344603   0.0804044\n",
      " -0.4354082  -0.00224251  0.22813473 -1.5941943  -0.01939652 -0.00939224\n",
      " -0.12736192  0.15525227 -0.07095559  0.05082704 -0.42780338  0.19424872\n",
      "  0.04991796  0.07644495  1.02268163  0.02312896  0.1447829   0.1637681\n",
      "  0.03630336 -0.09079113  0.10230181  0.00370138  0.62959423  0.11442827\n",
      " -0.02510496  0.06672125  0.0099325   0.75417378  0.03409279  0.10871713\n",
      "  0.58593841  0.01785746 -0.35355774  0.11764774  0.23848833  0.09405746\n",
      "  0.02464698  0.00844486  0.02932092  0.06463595  0.2100991   0.03057271\n",
      " -0.32521564 -0.01347271  0.01336897 -0.12859277 -0.2221264  -0.06766596\n",
      "  0.07503468  0.1501877   0.1265486   0.0018426  -0.01320474 -0.04027886\n",
      " -0.56811905  0.06652855 -0.02140538  0.035753  ]\n",
      "956130\n",
      "training: 38 testing: 26\n",
      "missed           20\n",
      "correctsource    18\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           13\n",
      "correctsource    13\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.33333333 0.66666667 0.5        0.4        0.2\n",
      " 0.75      ]\n",
      "Accuracy: 0.47368421052631576\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.44      0.44      0.44        18\n",
      "       missed       0.50      0.50      0.50        20\n",
      "\n",
      "    micro avg       0.47      0.47      0.47        38\n",
      "    macro avg       0.47      0.47      0.47        38\n",
      " weighted avg       0.47      0.47      0.47        38\n",
      "\n",
      "accuracy = 0.5769230769230769\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.56      0.69      0.62        13\n",
      "       missed       0.60      0.46      0.52        13\n",
      "\n",
      "    micro avg       0.58      0.58      0.58        26\n",
      "    macro avg       0.58      0.58      0.57        26\n",
      " weighted avg       0.58      0.58      0.57        26\n",
      "\n",
      "(64,)\n",
      "[ 0.03407162  0.09097706  0.43696204  0.17392164  0.25818289  0.08049333\n",
      " -0.51630625 -0.07664594  0.31160213 -0.5050418  -0.42464279  0.05103563\n",
      "  0.11981456  0.44224617 -0.15406025 -0.42469682  0.03381119 -0.48369991\n",
      "  0.07440038  0.05152201  0.05469651 -0.09606331 -0.07371074  0.0066881\n",
      " -0.31274067 -0.38064129  0.01730791 -0.37824036  0.19081544  0.80982085\n",
      "  0.01078795  0.21980304  0.15664089 -0.1565309   0.22282733 -0.21848765\n",
      " -0.30955134 -0.42873311 -0.02801992 -0.2631659  -0.56333573  0.1870948\n",
      " -0.42709812 -0.07040167  0.04058428 -0.05011342 -0.15691219  0.28216544\n",
      "  0.19901363 -0.02947681  0.07254051  0.54383423 -0.3893141   0.26894168\n",
      " -0.17677252 -0.18412566  0.24061034 -0.32539709  0.06240037  0.01630665\n",
      "  0.34271107 -0.42057309  0.20792005  0.15415731]\n",
      "983291\n",
      "training: 40 testing: 28\n",
      "correctsource    29\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    21\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.57142857 0.5        0.5        0.5        0.4        0.2\n",
      " 0.6       ]\n",
      "Accuracy: 0.475\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.55      0.60        29\n",
      "       missed       0.19      0.27      0.22        11\n",
      "\n",
      "    micro avg       0.47      0.47      0.48        40\n",
      "    macro avg       0.43      0.41      0.41        40\n",
      " weighted avg       0.53      0.47      0.50        40\n",
      "\n",
      "accuracy = 0.7142857142857143\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.78      0.86      0.82        21\n",
      "       missed       0.40      0.29      0.33         7\n",
      "\n",
      "    micro avg       0.71      0.71      0.71        28\n",
      "    macro avg       0.59      0.57      0.58        28\n",
      " weighted avg       0.69      0.71      0.70        28\n",
      "\n",
      "(64,)\n",
      "[-0.11645973 -0.10029874 -0.26091018 -0.06974031 -0.32743402  0.0266401\n",
      " -0.13501145  0.04252601  0.2163244  -0.07734949 -0.12744257 -0.27186853\n",
      " -0.00286705 -0.30046605 -0.07677472  0.13169272  0.33925433 -0.03438255\n",
      "  0.24724499  0.12045979 -0.10232041  0.32676183  0.13717779 -0.12162375\n",
      " -0.29080848 -0.16717353  0.46670061 -0.53141564  0.21726906 -0.10217891\n",
      " -0.16697091 -0.0110371   0.18230013  0.03966881  0.10157751  0.17917948\n",
      "  0.2892092   0.29888292 -0.25188138 -0.18860889  0.30865799 -0.30116808\n",
      " -0.1807387  -0.08955619 -0.29601697 -0.07346055 -0.25185027  0.26311549\n",
      " -0.30374352  0.0569733   0.00370381 -0.2767309  -0.34886771  0.31029608\n",
      " -0.01319033 -0.31548804  0.0913869   0.03514861 -0.13028921  0.02255521\n",
      " -0.01555443  0.01338295  0.22915392 -0.36292655]\n",
      "998166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 37 testing: 26\n",
      "correctsource    21\n",
      "missed           16\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.33333333 0.6        0.6        0.4        0.4\n",
      " 0.6       ]\n",
      "Accuracy: 0.4864864864864865\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.55      0.57      0.56        21\n",
      "       missed       0.40      0.38      0.39        16\n",
      "\n",
      "    micro avg       0.49      0.49      0.49        37\n",
      "    macro avg       0.47      0.47      0.47        37\n",
      " weighted avg       0.48      0.49      0.48        37\n",
      "\n",
      "accuracy = 0.5384615384615384\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.56      0.71      0.63        14\n",
      "       missed       0.50      0.33      0.40        12\n",
      "\n",
      "    micro avg       0.54      0.54      0.54        26\n",
      "    macro avg       0.53      0.52      0.51        26\n",
      " weighted avg       0.53      0.54      0.52        26\n",
      "\n",
      "(64,)\n",
      "[ 0.02335004  0.1288877   0.03509756 -0.30890923  0.07584228 -0.23328321\n",
      "  0.15273962  0.17888385  0.63765472  0.09834422  0.1300638  -0.23000994\n",
      "  0.0708201   0.10434539 -0.28217013  0.19424527 -0.22051537  0.13102747\n",
      " -0.05833928  0.08682071  0.21401495  0.18603843  0.29426136  0.13100702\n",
      " -0.1012226   0.12375627  0.42878537 -0.17252921  0.32108624 -0.1768561\n",
      "  0.04441761  0.28989101  0.03899738 -0.00395339 -0.1477     -0.02998565\n",
      "  0.01253576  0.15546172  0.17967377  0.57062829  0.32623571 -0.12198254\n",
      " -0.69020757 -0.15892405  0.24976185  0.26782207 -0.23831628  0.01673089\n",
      " -0.19215003 -0.05854261 -0.00265897  0.12417149 -0.4359114  -0.21970074\n",
      "  0.44540802 -0.27181223 -0.21478729 -0.55186074 -0.14876484  0.06137811\n",
      " -0.24520701  0.01364138  0.33646253  0.02643286]\n",
      "108391\n",
      "training: 34 testing: 24\n",
      "correctsource    25\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    18\n",
      "missed            6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.16666667 0.6        0.8        0.5        0.5\n",
      " 0.5       ]\n",
      "Accuracy: 0.5294117647058824\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.68      0.68      0.68        25\n",
      "       missed       0.11      0.11      0.11         9\n",
      "\n",
      "    micro avg       0.53      0.53      0.53        34\n",
      "    macro avg       0.40      0.40      0.40        34\n",
      " weighted avg       0.53      0.53      0.53        34\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.71      0.56      0.63        18\n",
      "       missed       0.20      0.33      0.25         6\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        24\n",
      "    macro avg       0.46      0.44      0.44        24\n",
      " weighted avg       0.59      0.50      0.53        24\n",
      "\n",
      "(325,)\n",
      "[ 7.53341678e-03 -1.81096792e-02  8.04671590e-03  1.47289986e-02\n",
      "  6.02769237e-02 -5.90577339e-03  1.40159999e-02  0.00000000e+00\n",
      " -4.97093661e-02  8.33072782e-03 -1.73597202e-02 -7.41965251e-03\n",
      " -5.32111716e-03 -1.51956975e-02  1.24945735e-02  2.89235023e-02\n",
      " -6.38462904e-02 -9.54033208e-03  3.12488923e-02  1.56079266e-02\n",
      "  1.98032457e-02  1.03723684e-02 -4.62934611e-02 -5.32244193e-02\n",
      "  5.57870470e-03  2.09522559e-02  7.16101852e-03 -3.82565925e-02\n",
      "  1.71944576e-02 -1.31376227e-02 -1.10828454e-02 -7.36552152e-02\n",
      " -2.72090157e-02 -4.48665589e-02  3.49023440e-02 -1.48801037e-02\n",
      " -3.72754239e-02  4.48108483e-02  4.60974226e-02 -9.16497713e-03\n",
      "  6.81195035e-03  3.74893851e-02 -2.31122037e-02 -4.03257954e-02\n",
      " -6.39692177e-03 -3.34449566e-03  1.37660678e-02  1.86345069e-02\n",
      " -5.78839342e-02  8.21967815e-02 -7.07183324e-02  2.87855248e-02\n",
      "  1.35291112e-02 -1.68897531e-02  1.36224273e-02  1.26220849e-03\n",
      " -2.59348586e-02 -3.23569197e-02  7.81328053e-02 -4.31575497e-02\n",
      " -8.42681285e-02 -1.47170051e-02 -4.93040055e-02  1.67565382e-03\n",
      "  5.81340889e-02 -3.40486321e-02  5.11670951e-02  9.97905756e-03\n",
      " -9.26783921e-03 -7.94870291e-02  3.92547848e-02 -2.69374122e-02\n",
      " -3.31611555e-02 -3.60037336e-02 -5.12581378e-03 -3.25265923e-02\n",
      " -5.54489308e-02 -2.31488032e-02  1.69224601e-02 -1.26741079e-02\n",
      " -3.61725756e-02  3.71917397e-02 -4.37069775e-02  4.77640671e-03\n",
      "  2.29171835e-02 -2.76457217e-04 -2.89377407e-03 -1.60390258e-02\n",
      " -2.72900776e-02  4.97136782e-02 -6.20025003e-02 -7.36640730e-03\n",
      " -2.32792581e-02 -2.62249364e-05  4.48977893e-02 -1.78419646e-02\n",
      "  2.78910203e-02 -4.26488567e-02 -3.18156806e-02  2.07732327e-02\n",
      "  4.13938504e-02  4.29577565e-02  7.16900728e-02 -4.23484724e-02\n",
      "  1.16548055e-02  3.94328314e-02 -1.25225261e-02 -5.74998942e-02\n",
      "  1.36384053e-02 -2.96577954e-03  2.32088395e-02 -4.13497307e-02\n",
      " -5.26228069e-02  2.56536802e-02  3.56533402e-02 -4.34769010e-02\n",
      " -3.00786252e-03  3.70354819e-02 -4.73190225e-04  4.63347771e-02\n",
      "  1.70335951e-02 -4.27805504e-02  1.33679003e-02 -2.01350035e-02\n",
      " -1.14339406e-02  2.60125013e-02  2.69637253e-03  3.32656059e-03\n",
      "  3.05784857e-03  8.24610647e-03 -3.54247141e-03  2.28777186e-02\n",
      " -1.03135681e-02  2.40892165e-02 -1.14026330e-02  4.55205225e-03\n",
      "  1.30394861e-02 -9.33181742e-03  2.86514148e-03 -1.70988058e-02\n",
      " -5.36053557e-03 -5.87375116e-03  2.67456764e-02 -1.12841014e-02\n",
      "  1.97478898e-02 -6.67504346e-03  1.20873398e-02 -1.56852471e-02\n",
      " -1.57180129e-02 -4.90643858e-03  1.54259959e-02  3.62581271e-02\n",
      "  3.11494275e-02  9.31789135e-03  3.72606560e-02 -4.72272833e-03\n",
      " -7.92705315e-03  2.65839122e-02 -9.22577414e-03  3.52450576e-03\n",
      "  3.34106670e-02  2.67944673e-03  3.38200930e-02  1.59617109e-02\n",
      " -3.27615710e-02 -1.20295317e-02 -3.16113248e-02 -1.68272915e-02\n",
      " -1.83263669e-02  6.29132984e-02 -8.25877987e-03  8.32073597e-02\n",
      " -2.67112675e-02  3.74652749e-03 -3.05631056e-02  7.13487160e-02\n",
      "  1.35009540e-02  9.87570217e-03  2.48685818e-03 -3.05074008e-03\n",
      "  3.07039342e-02  9.83578123e-03 -1.92556415e-02  1.77387025e-02\n",
      " -4.53043854e-02 -1.48539151e-02  5.17222966e-02  4.27374520e-02\n",
      " -3.34637036e-02 -1.41686545e-02 -7.51831527e-02 -5.48928224e-02\n",
      " -5.26840567e-02  2.78474541e-02  5.40531296e-03  1.84125008e-02\n",
      "  3.70692376e-02  7.79927658e-03  1.84463943e-02  6.90088266e-03\n",
      "  1.87448175e-02 -4.51505452e-02 -2.19118671e-02  5.19526101e-02\n",
      "  6.17026052e-02  5.90341333e-02  2.27187274e-02  1.55353945e-02\n",
      "  3.76876036e-02  2.22319453e-02 -5.22466233e-02  2.34528602e-03\n",
      "  5.87893506e-03 -2.66566072e-03 -1.22360218e-02  4.72660547e-02\n",
      "  2.92683965e-02 -5.90528924e-02  2.81111939e-02 -3.07226694e-03\n",
      "  1.13068283e-02  5.76663584e-03 -3.48364970e-02  3.24602360e-02\n",
      " -3.47664893e-03  2.44765476e-02 -5.13928087e-02 -4.52793533e-03\n",
      "  1.63514157e-02 -2.83428158e-02 -6.02343936e-02  3.17998059e-02\n",
      " -4.30926960e-02  5.56519512e-04  4.52018448e-02 -1.42013556e-02\n",
      " -6.40080251e-03  1.53219230e-02  7.01418686e-02 -7.01722470e-03\n",
      "  1.60083889e-03  1.61384089e-02  3.70139834e-03  2.47465536e-02\n",
      "  4.18218143e-03  1.57852200e-02 -4.49423321e-02  3.66194182e-02\n",
      " -2.41481493e-02  3.66153720e-02  1.62733658e-02 -1.06236918e-01\n",
      "  2.12231650e-02 -6.77914247e-02  2.71473157e-02 -7.25484507e-03\n",
      " -4.16711165e-02 -1.45243161e-02 -3.33755550e-02  1.63918282e-02\n",
      "  1.44762135e-02  6.77915471e-02 -4.33997099e-02 -1.16413405e-02\n",
      "  1.37156894e-02  1.80034808e-02  8.73020242e-03 -3.32937538e-02\n",
      " -2.48164870e-03  3.97684596e-02  2.10982143e-02 -4.21872075e-02\n",
      " -6.42663242e-02 -5.37866522e-04  3.99526662e-02  2.46933355e-02\n",
      "  1.54453138e-02  1.58995172e-02  6.41443143e-03  3.70380447e-02\n",
      " -3.06492591e-02 -2.67770828e-02 -9.87341840e-03  9.07710815e-02\n",
      "  6.03111228e-02 -7.89789295e-03 -2.41215965e-02  7.22830410e-02\n",
      "  2.16784553e-02 -2.19164028e-02 -1.14847532e-02  3.17323152e-02\n",
      "  2.97327048e-02  4.53296912e-02  6.27705519e-02  3.75678419e-02\n",
      "  2.98437715e-02  4.88870250e-02 -1.01695285e-02 -4.98502654e-02\n",
      "  1.37957604e-02 -2.20065819e-02 -1.79592908e-02  2.53778759e-02\n",
      " -5.30265095e-03 -5.56129782e-02  1.16162939e-02 -1.87051571e-02\n",
      " -3.27721947e-03  9.65847755e-03  7.96616328e-03 -9.23237891e-03\n",
      "  9.21010790e-03  1.37957055e-02  1.97306342e-03  7.06923569e-03\n",
      " -6.85709539e-03 -1.02506780e-02 -1.88776110e-02 -4.82128412e-02\n",
      " -1.83842313e-02 -4.78448589e-03  6.79540702e-02 -3.73548410e-02\n",
      "  4.60570028e-02]\n",
      "122922\n",
      "training: 28 testing: 20\n",
      "correctsource    15\n",
      "missed           13\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           10\n",
      "correctsource    10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.8  0.5  0.5  0.   0.5  0.25 0.  ]\n",
      "Accuracy: 0.39285714285714285\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.44      0.47      0.45        15\n",
      "       missed       0.33      0.31      0.32        13\n",
      "\n",
      "    micro avg       0.39      0.39      0.39        28\n",
      "    macro avg       0.39      0.39      0.39        28\n",
      " weighted avg       0.39      0.39      0.39        28\n",
      "\n",
      "accuracy = 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.60      0.60        10\n",
      "       missed       0.60      0.60      0.60        10\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        20\n",
      "    macro avg       0.60      0.60      0.60        20\n",
      " weighted avg       0.60      0.60      0.60        20\n",
      "\n",
      "(325,)\n",
      "[-0.03755308 -0.0109507  -0.02254675  0.01951741  0.00162711 -0.02148785\n",
      " -0.00379304 -0.01729958 -0.02772575  0.02435683  0.02065658 -0.02981891\n",
      " -0.0190291  -0.00154522  0.01780041 -0.02712959  0.03385742  0.03790343\n",
      " -0.01208057  0.01685635 -0.0340505   0.00087767  0.00630539 -0.0023664\n",
      " -0.02169474  0.00037374 -0.04097912 -0.02264862  0.07331932  0.00328551\n",
      "  0.07178038  0.03028045  0.00208293 -0.02653659  0.01581281 -0.00731453\n",
      " -0.01245122 -0.03556567 -0.01917361  0.08069841  0.00808088  0.0102837\n",
      " -0.02363411 -0.03459611 -0.01611775  0.00120022  0.01080457 -0.03380754\n",
      "  0.00763458 -0.00457764  0.03806739 -0.01507059 -0.02332979  0.04013589\n",
      "  0.03662658 -0.00515678 -0.02927669  0.00497427  0.00543715 -0.02552603\n",
      " -0.0063208  -0.01855072 -0.03713431 -0.00173406  0.01200436  0.02298537\n",
      "  0.03234109  0.04117334 -0.028472   -0.02117024 -0.00712367 -0.03199809\n",
      "  0.00879539 -0.02367218 -0.0384486  -0.03255332  0.03564189  0.01488957\n",
      " -0.04955181  0.033125    0.02960814  0.02381705 -0.03639947 -0.00858415\n",
      "  0.02719275  0.07146983 -0.00663668  0.02957224  0.00905317 -0.01687281\n",
      "  0.02168311  0.00321455  0.01818968 -0.04405318  0.04529621  0.06054047\n",
      " -0.02035896 -0.00422523 -0.00693661  0.00572348  0.03534511 -0.01074908\n",
      " -0.03188663  0.00230983  0.01589339 -0.0053007  -0.01747113  0.02494533\n",
      " -0.02124034  0.0128135  -0.00863262  0.02228545  0.00336388 -0.00290846\n",
      "  0.02225309 -0.02691609  0.0163498  -0.03017806 -0.01213682 -0.0028887\n",
      " -0.02200341  0.01184562  0.01896497 -0.01955508 -0.00747381 -0.04565105\n",
      " -0.01974582 -0.03778311  0.0141464  -0.02213438  0.05592482 -0.01240103\n",
      "  0.00706641 -0.0174128   0.04517424 -0.03913461 -0.03564784  0.00368493\n",
      " -0.02300872  0.03188183 -0.00300664  0.00476322 -0.0227106   0.00061402\n",
      " -0.00366865  0.01271364  0.00057079  0.00389547 -0.00402191  0.02532169\n",
      " -0.00807202  0.01003466 -0.02001214 -0.05118386  0.0256693   0.02777143\n",
      " -0.00188666  0.02314913 -0.01150229 -0.03404015 -0.02223083 -0.05141188\n",
      " -0.03867458  0.02218518  0.00098211 -0.00256317  0.01803712 -0.01805573\n",
      " -0.00214857 -0.00978666  0.0035518  -0.00224542  0.05240531 -0.03267352\n",
      "  0.03361173  0.00722846  0.0184499  -0.01094043 -0.01834447 -0.00434664\n",
      "  0.00482535 -0.01197957  0.04065405 -0.00861688 -0.0160417  -0.01653359\n",
      "  0.05135336 -0.03688948 -0.02014017  0.03050237  0.07730395 -0.00870998\n",
      "  0.0055832  -0.03227875 -0.03198636 -0.02049258 -0.02480032  0.02004345\n",
      "  0.00824182  0.01781439 -0.0149568   0.01915775 -0.00696927  0.03992029\n",
      " -0.04084242 -0.00650944  0.02953819  0.03911756 -0.02137593 -0.01139099\n",
      " -0.00426145 -0.03435835  0.0570619  -0.00276558  0.05448076  0.04715825\n",
      " -0.01143017  0.01518669 -0.00997376  0.02382479 -0.00172935  0.00814001\n",
      "  0.00751184 -0.01937538 -0.02101262  0.01901482 -0.0561046   0.01019095\n",
      "  0.04708081 -0.02337357  0.00769478 -0.0179153   0.01654431  0.02116458\n",
      " -0.04687886 -0.04406065 -0.01013543  0.03577261  0.03460656  0.0342873\n",
      " -0.02734388  0.00010744 -0.0409204   0.00908699 -0.03172703  0.03564537\n",
      " -0.01214434  0.02498568 -0.02529461  0.01101086  0.00757176 -0.00420295\n",
      "  0.01729919 -0.01570175  0.01827792  0.01125805 -0.01099431  0.01160534\n",
      " -0.02451067 -0.02675541  0.01598686  0.00795762  0.01351121 -0.01394163\n",
      " -0.01629092 -0.03381149 -0.01159698  0.00574341  0.03546296  0.02279977\n",
      "  0.0674643   0.01285615 -0.02109698 -0.03439104 -0.00499448 -0.00597016\n",
      " -0.04597628  0.05705556  0.01405257 -0.01744    -0.04550422 -0.00433543\n",
      "  0.0208944   0.00837433  0.0125358   0.01028063  0.00464817 -0.0008208\n",
      "  0.01639076 -0.00181946 -0.04529775  0.01018312 -0.02308839 -0.03701947\n",
      "  0.01331266 -0.02531473 -0.00744212  0.0051182   0.00208519 -0.05349827\n",
      "  0.03362743  0.03123762  0.01863734 -0.02354503  0.02833428 -0.03640761\n",
      " -0.02394503 -0.01198536  0.00350086 -0.00451446 -0.01911951 -0.02904784\n",
      "  0.03603403  0.01588331  0.01069875  0.03566063  0.02094434 -0.01317134\n",
      " -0.05140153  0.0117629  -0.02100329  0.0292338  -0.00345437 -0.05515365\n",
      "  0.01769558]\n",
      "139593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 33 testing: 23\n",
      "missed           24\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           17\n",
      "correctsource     6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.33333333 0.4        0.5        0.25       0.75\n",
      " 0.        ]\n",
      "Accuracy: 0.3939393939393939\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.08      0.11      0.09         9\n",
      "       missed       0.60      0.50      0.55        24\n",
      "\n",
      "    micro avg       0.39      0.39      0.39        33\n",
      "    macro avg       0.34      0.31      0.32        33\n",
      " weighted avg       0.46      0.39      0.42        33\n",
      "\n",
      "accuracy = 0.34782608695652173\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.09      0.17      0.12         6\n",
      "       missed       0.58      0.41      0.48        17\n",
      "\n",
      "    micro avg       0.35      0.35      0.35        23\n",
      "    macro avg       0.34      0.29      0.30        23\n",
      " weighted avg       0.45      0.35      0.39        23\n",
      "\n",
      "(325,)\n",
      "[-4.18124365e-02  8.79805272e-03  1.23304235e-02 -3.22194172e-03\n",
      " -2.35744417e-02 -8.92796177e-03  2.43583882e-02  2.06456193e-02\n",
      "  4.01810905e-02  2.83810227e-02 -1.58287985e-02 -6.87604077e-03\n",
      " -2.01762534e-02  9.00637842e-02  4.53176087e-02  3.71081372e-02\n",
      "  1.66255042e-02 -3.82625382e-02 -3.67332866e-02  5.51159800e-03\n",
      "  1.02194774e-02 -3.71784575e-02  4.07379473e-02 -8.06839312e-03\n",
      " -5.77807061e-03  1.76330043e-02  3.79963707e-02 -1.95290541e-02\n",
      "  2.98088804e-02 -3.41477446e-02 -8.13721381e-03  7.79867935e-02\n",
      " -3.90242719e-03 -5.65068404e-02 -3.57058301e-02 -8.37824432e-02\n",
      "  1.36507830e-02  3.46076535e-02 -6.18507561e-03 -2.64573281e-02\n",
      "  1.71227522e-02  3.32196951e-02  3.40294734e-03  5.74751238e-02\n",
      "  4.12164912e-02 -4.36407079e-03 -1.24587247e-02  9.54787510e-04\n",
      "  7.19159556e-02 -2.40809991e-02  2.46428901e-02  4.71797326e-02\n",
      "  3.61455443e-02 -3.48693391e-02 -6.74611757e-02 -2.64973316e-02\n",
      "  2.60973867e-02 -1.31354817e-02  5.48592757e-04  3.01730105e-02\n",
      "  3.61874288e-02  3.39000925e-02 -1.35073093e-02 -1.67711874e-02\n",
      " -3.96501997e-02 -1.65942278e-02  1.09819524e-01 -1.79108929e-02\n",
      "  1.12338722e-02  5.10825148e-02 -1.05615885e-02  3.11359695e-02\n",
      "  3.18569267e-02 -4.70992673e-02  2.55924118e-02 -6.46280237e-02\n",
      "  5.58414902e-03 -1.84179187e-03  1.98857360e-02  1.79662875e-02\n",
      "  9.30112138e-03  1.46965609e-03 -3.36247724e-02  2.18170122e-02\n",
      " -5.90273732e-02 -1.75157340e-02  1.29822651e-02  2.11040563e-02\n",
      "  3.39588134e-02  1.46121874e-02  1.10596012e-02  6.35752390e-03\n",
      "  1.69885265e-02 -2.17938497e-02 -9.02999539e-02  4.24448069e-02\n",
      "  3.17768358e-02  6.11850871e-02  8.67387796e-03  2.57171147e-03\n",
      " -2.15129232e-02  4.06093054e-04  3.98195125e-02  1.51959752e-02\n",
      " -1.21190160e-02 -1.39357317e-02  1.33688847e-03 -4.50193121e-02\n",
      "  5.56830191e-03  3.28659620e-02  2.40433926e-02 -2.12811626e-02\n",
      "  3.09043630e-02  1.59755529e-02 -9.74212403e-02  9.26681869e-03\n",
      " -2.65242319e-02  1.85413012e-02  8.13191430e-02  1.61808007e-02\n",
      "  2.43456307e-02  1.88411294e-02  4.70645879e-03  7.73944044e-02\n",
      "  1.01858241e-02 -3.18121691e-02 -1.71939268e-02 -7.35057442e-03\n",
      "  2.34228558e-02 -3.99772988e-02  3.35367690e-02  1.28968493e-04\n",
      " -4.12573043e-02  1.08759750e-02 -2.44502984e-02 -1.45231543e-02\n",
      "  4.97028736e-02 -2.48822771e-02  7.43717040e-02  3.22279840e-03\n",
      " -5.78762270e-02 -2.28069998e-03 -7.75919177e-02 -1.08726038e-02\n",
      " -1.21714514e-02  4.32117785e-02  8.28253187e-03  1.94616709e-02\n",
      " -3.11747526e-02 -6.58667028e-02  1.42022679e-02  1.52236206e-02\n",
      "  2.71838202e-02 -3.10022977e-02  2.35945198e-02  8.42428400e-03\n",
      "  1.80383278e-02 -3.39283689e-02 -6.19128928e-03  7.56566295e-02\n",
      "  2.97742469e-02  5.85687842e-02  2.18702431e-02 -3.34363242e-03\n",
      "  4.31623752e-03 -1.35890842e-02  5.94266475e-02 -5.35185888e-03\n",
      "  4.71786005e-02 -1.50791568e-04 -1.82635954e-02 -2.18804798e-03\n",
      " -3.67651247e-02 -2.95249177e-02 -1.38607937e-02  1.49861018e-02\n",
      "  1.79091484e-02 -3.66193659e-02 -5.89633737e-02 -4.41659843e-02\n",
      "  9.60473739e-02  3.29405929e-02  8.33141252e-02 -2.16880770e-02\n",
      "  2.68090363e-03  1.63095534e-02 -2.42786197e-02 -2.41386930e-02\n",
      " -1.54197797e-03 -2.84503205e-02  3.79044159e-02 -4.96159026e-03\n",
      "  5.65437872e-02  9.73848004e-03 -3.22337458e-02 -1.09103359e-02\n",
      " -1.13851559e-02  5.31520951e-02  2.55948235e-02  6.49022242e-02\n",
      "  4.01645522e-02 -2.63735905e-02 -1.85901352e-03  4.45349163e-02\n",
      " -7.30404384e-03 -4.80237214e-02  3.94125462e-03  1.56256471e-02\n",
      "  3.11076848e-02  1.93959855e-03 -2.08657287e-02 -7.25088145e-03\n",
      "  2.97120289e-02 -5.21999485e-02  8.93401184e-02  4.50162344e-02\n",
      "  3.05687951e-02 -1.76783961e-02 -2.20496276e-03 -3.73014828e-02\n",
      " -1.62584443e-02 -4.63783233e-03  3.22745322e-02 -3.73199375e-02\n",
      " -1.41661230e-02 -4.67917210e-02 -3.32392019e-02  3.07776750e-03\n",
      "  2.42995914e-02  4.71148354e-02 -2.32638926e-02  5.24748441e-02\n",
      "  3.53394537e-02 -8.44395463e-03 -5.78644690e-02  4.08682440e-02\n",
      " -1.72254426e-02  2.22419113e-02 -2.11230649e-02  2.07761233e-02\n",
      "  2.42969635e-02 -9.79682651e-04  3.59398590e-02 -2.79152796e-02\n",
      "  1.37683747e-02 -7.81937400e-02  3.70407957e-02 -6.03738737e-02\n",
      " -4.06624851e-02 -5.26370936e-02 -5.64677843e-02 -1.03872833e-02\n",
      " -2.12473672e-02  2.03615650e-02  1.64058586e-02 -3.49379967e-02\n",
      " -1.53195551e-02 -3.88748979e-02  2.51539917e-02 -6.77808301e-03\n",
      "  3.19109907e-02  2.78868484e-02 -8.66891113e-02  6.33781963e-02\n",
      "  1.79669232e-02 -2.50881006e-02 -2.25672021e-02 -2.09778078e-02\n",
      " -1.33034590e-02 -1.15745941e-02 -3.62936328e-03 -2.93368175e-02\n",
      "  1.91060305e-02 -5.03988172e-02 -4.28369793e-03 -1.92255710e-02\n",
      " -9.30554734e-03 -2.04469074e-02  4.87217386e-02 -3.79771955e-02\n",
      " -9.59025630e-03 -8.27276593e-04 -6.08916257e-03  5.26873459e-02\n",
      "  2.53841790e-02 -8.07668784e-03  3.10119445e-03  1.17071612e-03\n",
      "  9.74696459e-03  8.75149363e-02  2.73022218e-02  4.17871269e-02\n",
      "  5.95894454e-02 -5.66873810e-03  8.52530890e-02 -5.21858089e-03\n",
      "  5.96202514e-03 -9.97554915e-03  2.71854018e-02 -1.48804848e-02\n",
      "  2.04327164e-02 -2.69021240e-02 -3.54918480e-02  4.34275229e-02\n",
      " -6.86557162e-02  4.58532605e-02  3.53243498e-02  1.98890508e-02\n",
      "  1.54998417e-02 -3.55033294e-02 -6.25017857e-03 -1.76677477e-02\n",
      " -7.42517917e-02  6.75321223e-03  4.97787313e-02  1.08947508e-02\n",
      "  1.76408327e-02  2.29120482e-02  2.56127709e-03  9.62591286e-05\n",
      " -5.86095668e-03 -4.58754707e-04  1.07403746e-02 -6.97680616e-03\n",
      "  7.13064124e-03]\n",
      "164965\n",
      "training: 39 testing: 27\n",
      "correctsource    24\n",
      "missed           15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    17\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.42857143 0.66666667 1.         0.8        0.4        0.2\n",
      " 0.4       ]\n",
      "Accuracy: 0.5641025641025641\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.63      0.71      0.67        24\n",
      "       missed       0.42      0.33      0.37        15\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        39\n",
      "    macro avg       0.52      0.52      0.52        39\n",
      " weighted avg       0.55      0.56      0.55        39\n",
      "\n",
      "accuracy = 0.5185185185185185\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.71      0.65        17\n",
      "       missed       0.29      0.20      0.24        10\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        27\n",
      "    macro avg       0.44      0.45      0.44        27\n",
      " weighted avg       0.48      0.52      0.50        27\n",
      "\n",
      "(325,)\n",
      "[-3.66213226e-02 -4.79090208e-04 -5.90305706e-02  4.04477290e-02\n",
      "  3.44237734e-02  1.37510361e-02 -2.18333184e-02 -3.28774841e-02\n",
      "  4.35658518e-02 -1.71912981e-02  3.13712728e-02 -8.46737978e-03\n",
      " -2.02129792e-02  1.35965882e-03  2.23617295e-02  9.62797035e-03\n",
      " -2.76903878e-03 -1.00305166e-02  4.66617523e-02 -5.14206868e-02\n",
      " -3.83901032e-03  1.49655290e-02  5.52057825e-02 -1.94154501e-02\n",
      "  3.26011110e-02  8.16797812e-02 -1.17952263e-02  3.86324303e-03\n",
      "  8.27929769e-02 -3.24287500e-02 -3.64307160e-02  3.15040577e-02\n",
      "  4.23784976e-02  3.12500768e-02  2.13495120e-02 -1.69449480e-02\n",
      " -3.64922437e-02  1.82904340e-04  6.01656738e-02 -2.11477855e-02\n",
      " -3.34119107e-02  2.40783124e-02 -3.39022493e-02  5.45545857e-03\n",
      " -2.93447510e-02 -5.26201767e-02  4.56216945e-02  6.47686606e-02\n",
      "  2.00213490e-03 -6.36142176e-02 -2.27471381e-02  2.89813812e-02\n",
      " -2.35710845e-02  2.28976366e-02  3.54787674e-03 -2.52097522e-02\n",
      "  3.04398948e-02  6.47408842e-03 -7.90457272e-03 -1.27443041e-02\n",
      "  3.42862194e-02 -4.49683541e-03 -3.25369098e-03  6.80255702e-02\n",
      "  0.00000000e+00  3.94325102e-02 -1.98870165e-02 -5.95737899e-02\n",
      " -1.04069122e-02 -4.05409270e-02  1.19461100e-02 -3.52596674e-02\n",
      " -1.28165127e-02  2.51199563e-02 -3.83777990e-02 -6.09095100e-03\n",
      "  4.23965230e-02 -2.19365479e-02 -2.90513610e-02  3.72397832e-03\n",
      "  4.09913791e-03  8.13401453e-03 -3.09912908e-02 -2.73990254e-02\n",
      " -1.86283072e-02 -6.01497692e-02  2.10478871e-02  2.09651282e-02\n",
      "  1.10425676e-02 -2.22985832e-02  3.61835002e-02 -8.06671130e-02\n",
      " -7.25103304e-02 -7.02933410e-03 -2.93025027e-02 -4.44296316e-02\n",
      " -1.72555074e-02 -1.59334707e-02  1.58679507e-02 -1.43449136e-02\n",
      " -2.27550132e-02 -3.93095828e-02 -5.74220501e-02 -3.32011056e-02\n",
      " -3.04503199e-03 -2.10389274e-02  4.96536626e-02  4.53861572e-03\n",
      "  1.13314804e-02  3.36860858e-02  3.11399922e-02  3.56353273e-02\n",
      " -1.70334183e-02  2.97697193e-02 -2.63684698e-02 -3.01264786e-03\n",
      " -4.55902343e-02 -5.65547696e-02 -3.91044063e-02  5.51817759e-03\n",
      "  1.75786001e-02 -1.17243293e-02  1.69318285e-02  4.90270977e-02\n",
      "  2.76484117e-02 -1.67677731e-02 -4.42006949e-02  8.55798009e-03\n",
      " -1.90111110e-02  3.48161604e-02 -1.41196102e-02  1.06026330e-02\n",
      "  3.25992040e-02 -2.45942047e-02 -1.75275892e-02 -3.00488058e-03\n",
      " -1.87796846e-02 -1.80982035e-02 -1.84243502e-02 -1.23835759e-02\n",
      " -1.43136039e-03  5.51961957e-02 -9.44725770e-02  1.48696020e-02\n",
      "  2.35034329e-02  4.12412002e-02 -2.35892373e-03  4.00455960e-02\n",
      " -2.22840550e-03  4.63597738e-02  4.66667539e-02 -1.73891403e-02\n",
      "  3.26405632e-02  2.44597678e-02  5.49937657e-02 -1.87513621e-02\n",
      " -5.92718931e-02  1.99008522e-02  5.90000113e-03 -4.30658772e-02\n",
      "  3.18830711e-02  1.42883312e-02 -3.40698887e-02 -2.57047423e-02\n",
      " -4.88031394e-04  9.80448869e-03 -2.20059966e-02 -6.26659973e-02\n",
      " -4.28546916e-02 -4.49300734e-02  5.37127301e-02 -1.26937588e-02\n",
      " -3.94452143e-02  1.73516436e-02  2.28525946e-02  3.54150780e-02\n",
      " -4.55385071e-02 -1.46381808e-02 -5.61440239e-02 -1.97940858e-02\n",
      "  5.79708745e-02 -4.46276293e-02 -2.17611738e-02  2.25556261e-02\n",
      "  2.88376776e-02  4.88565602e-04 -1.50859787e-02 -2.81149711e-02\n",
      " -3.45822732e-03  6.63690127e-03  4.09799903e-02 -3.70579206e-02\n",
      "  1.18216173e-01  3.91087218e-02 -4.09091584e-03  1.28442328e-02\n",
      " -5.17246393e-02 -9.91188137e-03  4.22951442e-02 -1.70092815e-02\n",
      " -3.72986611e-02 -9.02180306e-02  4.61571522e-02 -1.54630118e-02\n",
      " -1.19317749e-02 -6.83158287e-02  5.99422686e-02  7.00322340e-02\n",
      " -7.58464094e-03  9.18733804e-03 -1.45230363e-02  1.10737367e-02\n",
      "  3.52920940e-02  6.92168512e-03 -1.54026152e-02 -2.02330769e-02\n",
      "  1.78871415e-02  2.26562299e-02  2.70057002e-02  1.24227473e-03\n",
      "  1.72072527e-03  2.71048333e-02 -1.30387497e-02 -1.42418563e-03\n",
      " -4.22110046e-02 -3.98177692e-02 -8.86783471e-03 -7.07700984e-03\n",
      "  6.59333851e-03 -1.13273852e-02  3.63552863e-02 -1.70584623e-04\n",
      "  1.45981943e-02  4.21911143e-02 -2.38207968e-02 -3.51813207e-02\n",
      "  5.69697499e-02  3.90399470e-03 -5.89242524e-03 -5.68536741e-03\n",
      "  3.59715327e-02  4.52006154e-04 -6.66267251e-02 -1.58531317e-02\n",
      "  4.97734660e-02 -5.71791280e-02 -1.81636427e-02 -1.31647360e-02\n",
      "  6.69528841e-02  2.68352514e-02  3.10002387e-02  1.05121161e-01\n",
      "  4.98629749e-02 -1.60014817e-02  2.37746964e-02  4.78034651e-02\n",
      " -1.79901700e-02  4.94486826e-02  3.59321485e-02  1.66780107e-02\n",
      " -1.72861391e-02 -6.80053721e-02  1.00148694e-04  2.05576518e-02\n",
      " -5.06755351e-02  2.25716089e-02  1.84788506e-02  2.69768947e-04\n",
      "  1.03842418e-02  9.41421839e-03 -2.10069302e-02 -2.03929043e-02\n",
      "  4.80016036e-02  2.53679649e-02 -4.68801835e-05 -4.01436382e-02\n",
      " -4.46078059e-03 -3.64393213e-02  2.11738708e-02  4.64692166e-02\n",
      "  5.42983003e-03  1.53193176e-02  3.26055155e-02 -2.03402443e-02\n",
      " -1.82679723e-02 -3.96045114e-02 -2.10901715e-02  5.54996693e-03\n",
      " -1.95227534e-02 -2.37887719e-02  8.63917209e-03  8.96626610e-03\n",
      "  1.16561388e-02  1.49157036e-02 -2.85238504e-02 -1.26162302e-02\n",
      "  1.35739085e-02  1.61294164e-02 -3.14494531e-02  1.50476389e-02\n",
      " -7.24118806e-03 -2.83069543e-02  2.96979802e-02 -1.99585779e-02\n",
      " -3.90579516e-03  7.64380437e-03  7.19907812e-02 -4.21379591e-02\n",
      "  1.28760572e-02 -2.49551637e-02 -2.31132759e-03  1.77642334e-03\n",
      " -9.44782202e-03 -1.33187552e-03 -1.65895947e-03  3.63489737e-02\n",
      "  1.79679138e-03  6.41398854e-03 -2.82470842e-02  3.76303999e-02\n",
      "  1.25362758e-02 -2.43456829e-02 -1.44362917e-02 -1.85975267e-02\n",
      " -1.28251963e-02]\n",
      "199801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 20 testing: 14\n",
      "missed           11\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           8\n",
      "correctsource    6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.25       0.5        0.66666667 0.33333333 0.5        0.\n",
      " 0.        ]\n",
      "Accuracy: 0.35\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.30      0.33      0.32         9\n",
      "       missed       0.40      0.36      0.38        11\n",
      "\n",
      "    micro avg       0.35      0.35      0.35        20\n",
      "    macro avg       0.35      0.35      0.35        20\n",
      " weighted avg       0.35      0.35      0.35        20\n",
      "\n",
      "accuracy = 0.6428571428571429\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.33      0.44         6\n",
      "       missed       0.64      0.88      0.74         8\n",
      "\n",
      "    micro avg       0.64      0.64      0.64        14\n",
      "    macro avg       0.65      0.60      0.59        14\n",
      " weighted avg       0.65      0.64      0.61        14\n",
      "\n",
      "(325,)\n",
      "[ 0.01729866 -0.01339637 -0.02459383  0.00351444 -0.04142197 -0.02192359\n",
      "  0.03331457 -0.02532877 -0.01529513 -0.02912564  0.01186424 -0.00809642\n",
      "  0.01183945  0.03061931 -0.06069514 -0.00503405 -0.00027946 -0.01008144\n",
      "  0.00726126 -0.01908426  0.01112581  0.0088498   0.02593415  0.02876714\n",
      " -0.00325945  0.01944667 -0.01819201 -0.03702755  0.03856941 -0.01428755\n",
      "  0.0499476  -0.01169582 -0.00888903 -0.01539041 -0.00568022 -0.0167861\n",
      "  0.00294567 -0.01106036  0.0139308   0.01639621  0.0113518  -0.0163057\n",
      " -0.01660404  0.03032411  0.00271818 -0.02036754  0.01318272 -0.01727212\n",
      "  0.01074928 -0.0124397   0.01765284 -0.04023313 -0.03003701 -0.00087446\n",
      " -0.01376773 -0.00610613 -0.02047925  0.01007623  0.00794547  0.01030084\n",
      " -0.04612482 -0.03391977  0.03781236  0.03874739 -0.0034697  -0.0098829\n",
      "  0.03586217  0.00618026  0.02700599 -0.00683302  0.00733686  0.00432292\n",
      "  0.00930126  0.02885774 -0.02258055 -0.01545042 -0.03886167 -0.02832168\n",
      "  0.0210879   0.0091014  -0.00063012  0.00998928  0.03507197 -0.04376705\n",
      "  0.00634717 -0.01341871  0.01167891  0.0005446  -0.00398236 -0.03454582\n",
      " -0.01332734  0.00542423  0.01298055  0.01950784  0.01294302  0.01145308\n",
      "  0.01066193 -0.035756    0.00891312 -0.03002934  0.00782838 -0.01323948\n",
      "  0.03125878 -0.03338549  0.02711144 -0.00991407  0.03244091 -0.01181333\n",
      "  0.00495845  0.03549212 -0.01092669  0.01814549 -0.01527709  0.01683936\n",
      " -0.03268069 -0.02686263 -0.02055286  0.01921815 -0.01320448  0.01261379\n",
      "  0.02007352 -0.00442375  0.01467529  0.04044431  0.02976446 -0.0298546\n",
      " -0.0015814  -0.00383924 -0.01268928 -0.02467146 -0.02081093 -0.042791\n",
      "  0.01801915  0.03261178  0.00836254  0.01346358  0.0123947  -0.02314662\n",
      "  0.01161245  0.02913764  0.01190277  0.05156886  0.00558736  0.02566094\n",
      "  0.02715885  0.04499362 -0.03269634 -0.00769579  0.02266214  0.01679585\n",
      "  0.02389907  0.02282618  0.0043318   0.0255954   0.00459576 -0.01150983\n",
      "  0.01876816 -0.00482054 -0.00323631  0.01105967  0.02630271  0.00709891\n",
      "  0.01575655  0.00798028 -0.00156994  0.00951427 -0.00178407 -0.0137295\n",
      " -0.01408202 -0.00268071  0.01221833 -0.00613256  0.00485982 -0.01871475\n",
      " -0.00682557 -0.01128559  0.00708657 -0.0189202  -0.01076786 -0.02256005\n",
      " -0.00142647  0.00193464 -0.0064269  -0.02658048 -0.00527121  0.00647425\n",
      "  0.006099   -0.0075841   0.01587877  0.0006762  -0.01857405  0.00911143\n",
      "  0.01708979  0.02338732 -0.00024165  0.01949477  0.01192794  0.00744715\n",
      " -0.00926278 -0.00131409 -0.01732     0.00315489  0.0246989   0.0285326\n",
      " -0.01810388 -0.01489312  0.02665336  0.01215284  0.01990181  0.01075803\n",
      " -0.02787824 -0.00511063  0.00762703  0.00157589 -0.02462429  0.00241694\n",
      " -0.01258014 -0.01185791  0.00968008  0.01604193  0.08379948 -0.01566933\n",
      " -0.01772624  0.0336721   0.0269845  -0.0098464  -0.01339115 -0.00607635\n",
      "  0.00123311 -0.00519721  0.00316553  0.05291736 -0.01184371  0.02478218\n",
      "  0.02769862 -0.00375702  0.03414317  0.00028296  0.0084198   0.01500306\n",
      "  0.02320435  0.00918165  0.01979197 -0.04387566 -0.01231444  0.0037656\n",
      "  0.00533686  0.03272678  0.05363424  0.00028508  0.02288782  0.06057395\n",
      " -0.00416012 -0.0162485   0.00421769  0.04104586 -0.00972882 -0.00528751\n",
      "  0.02104626  0.02427171  0.02206812 -0.00336629  0.01158843 -0.00325346\n",
      "  0.0075033   0.00986275  0.00287527 -0.01696804 -0.00594428 -0.01688873\n",
      "  0.05223386  0.00545246  0.01524831 -0.02408791  0.0232991   0.00864378\n",
      "  0.00049594  0.0055736   0.00296538 -0.02688378  0.01105395  0.0240146\n",
      " -0.02555321 -0.00922511 -0.02441422 -0.00726724  0.00235302 -0.03602435\n",
      " -0.00992588  0.02517986  0.04744514  0.01661673 -0.00685142  0.02794041\n",
      " -0.02312076 -0.00414361 -0.04394546 -0.0164827  -0.0010333  -0.01030784\n",
      " -0.02824187  0.02282856 -0.00998749  0.0188123  -0.01598799  0.00258136\n",
      " -0.00095794  0.0084925   0.01079688 -0.01524882  0.01413075 -0.01859408\n",
      "  0.01425693  0.02229891  0.03261227 -0.00246408  0.01086123 -0.02944293\n",
      "  0.00262185 -0.01425215  0.00101376 -0.01648131 -0.01611872 -0.01522784\n",
      "  0.01088118]\n",
      "247659\n",
      "training: 34 testing: 23\n",
      "missed           25\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           17\n",
      "correctsource     6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.66666667 0.4        0.6        0.5        0.5\n",
      " 0.25      ]\n",
      "Accuracy: 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.00      0.00      0.00         9\n",
      "       missed       0.65      0.68      0.67        25\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        34\n",
      "    macro avg       0.33      0.34      0.33        34\n",
      " weighted avg       0.48      0.50      0.49        34\n",
      "\n",
      "accuracy = 0.6521739130434783\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.25      0.17      0.20         6\n",
      "       missed       0.74      0.82      0.78        17\n",
      "\n",
      "    micro avg       0.65      0.65      0.65        23\n",
      "    macro avg       0.49      0.50      0.49        23\n",
      " weighted avg       0.61      0.65      0.63        23\n",
      "\n",
      "(325,)\n",
      "[-3.11672337e-02  2.33509944e-02  3.06279507e-02 -2.36773322e-02\n",
      " -1.91488681e-02 -3.23683712e-02 -4.30992480e-02  5.76192516e-03\n",
      "  4.28361576e-02 -3.27838995e-02  3.27193198e-02 -2.77557813e-02\n",
      "  3.93125798e-02  1.00361679e-02 -3.68272789e-03  5.97229997e-03\n",
      "  3.29156654e-02  3.53869121e-02 -1.77615063e-03 -4.60624895e-03\n",
      " -1.35267210e-02  3.06703811e-02 -1.25450412e-02 -7.23057844e-02\n",
      " -6.56327289e-02 -9.39139933e-03 -4.19873785e-02 -2.90726414e-02\n",
      " -1.16588852e-03 -1.92326980e-03 -2.08099609e-03  1.04742588e-02\n",
      " -6.38860723e-02 -4.46889039e-02 -6.14653544e-02  7.31725187e-03\n",
      " -1.66030072e-02  5.15331803e-03  4.16540220e-02  5.49508695e-02\n",
      " -2.13019580e-02 -5.67730747e-02  2.80703995e-02  2.65713818e-02\n",
      "  3.09889357e-03  2.44269182e-02 -8.23616550e-02 -3.15410757e-02\n",
      " -1.25699982e-02 -3.41746775e-02  3.26423720e-02 -7.61582986e-02\n",
      " -7.01030504e-02 -4.98689341e-03 -2.79914865e-02  2.14881284e-03\n",
      " -3.88091303e-02  7.67189903e-03  5.07706986e-02 -4.80178490e-02\n",
      " -4.50593629e-02 -1.15674822e-02 -4.23138082e-03  3.01880721e-02\n",
      "  0.00000000e+00 -2.38647820e-02  5.49690919e-02  2.15951548e-03\n",
      "  8.83998738e-03 -1.92684365e-04  1.63548044e-02 -5.27264548e-02\n",
      " -1.04189433e-02  5.35725179e-03  4.63380533e-03  4.48169372e-02\n",
      " -3.27047521e-02  1.92317295e-03  2.63526817e-02  8.62927128e-03\n",
      "  4.11774007e-02 -1.99860307e-02  2.65477904e-02 -2.93348837e-02\n",
      "  5.38680468e-02  1.08002064e-02  3.51801253e-02 -1.12275573e-02\n",
      " -1.73700233e-02 -5.41923509e-03 -2.45531298e-02 -2.29060013e-02\n",
      " -2.32835399e-02 -2.71230503e-02 -3.68774785e-03 -2.69417109e-02\n",
      " -5.41465576e-02  5.36451794e-02 -9.08909926e-03 -1.09410017e-02\n",
      " -6.09711407e-02  4.78915157e-02  3.62435685e-02 -1.72678086e-03\n",
      " -1.24277341e-02  1.28434229e-02 -2.72585862e-03 -1.51203965e-02\n",
      "  4.41077145e-03 -1.84786158e-03 -6.44384170e-02 -3.63929025e-02\n",
      "  3.08511954e-02 -6.48108089e-05  2.35369679e-02  2.11874651e-03\n",
      "  5.83748900e-03  1.68721409e-02 -2.54459985e-02 -1.45423994e-02\n",
      "  1.06361292e-02 -2.10845533e-02 -2.52147475e-02  3.86616732e-03\n",
      "  6.03549750e-03  1.56010132e-02  2.20964241e-02 -4.08127124e-02\n",
      " -5.03844045e-02  1.22106268e-02  2.48868534e-02 -7.35308624e-03\n",
      "  3.35869561e-02  4.46518646e-02 -5.81267885e-02  1.34650949e-02\n",
      "  3.02518751e-02  3.91171333e-02 -3.41536362e-03  1.42731118e-02\n",
      " -4.04965681e-03 -3.24709554e-02 -4.89856650e-02 -5.05540707e-03\n",
      "  3.07913379e-03 -5.05624719e-02 -2.98927860e-02  1.68323662e-02\n",
      " -1.35224282e-02  3.62669443e-02  7.60711441e-02  1.83390860e-02\n",
      " -2.82882288e-02 -2.06209623e-02 -3.23053640e-02 -3.98753964e-02\n",
      " -3.79818199e-02 -6.90166031e-02  6.77595817e-03 -1.12787742e-02\n",
      " -5.95611384e-03 -2.13315829e-02 -3.30533812e-02 -4.49393813e-02\n",
      "  1.72552762e-02 -5.76142613e-03  3.70248954e-02 -8.68984700e-03\n",
      " -4.28887560e-02  4.45019800e-02 -2.95580641e-02 -1.70188111e-03\n",
      "  5.17379726e-03  1.66213859e-02 -1.20687327e-02 -2.96332416e-02\n",
      "  7.61555953e-03  1.43249249e-02 -4.58001487e-02 -1.35729030e-03\n",
      " -2.34486546e-02 -4.88536035e-02  5.07441678e-03 -3.52643622e-02\n",
      " -5.29194024e-02 -1.88342957e-03 -5.72731145e-02 -1.23265393e-02\n",
      "  4.89986228e-02  6.92634553e-03 -7.96221985e-04  2.51593941e-02\n",
      "  3.65910499e-02  3.07609666e-02  6.37111550e-02 -2.49500960e-03\n",
      " -1.70439961e-02 -8.43702580e-03  9.91552645e-03 -1.40556931e-02\n",
      "  1.49558572e-02  2.84487415e-02 -1.50228982e-02  9.64943448e-03\n",
      "  2.53643956e-03 -2.83814005e-02 -2.27979104e-03 -1.69238454e-02\n",
      "  2.65393807e-02  8.15339681e-03  8.61871538e-03 -1.41549486e-02\n",
      "  6.54928847e-02 -2.15139696e-02 -2.18744397e-03 -1.19985351e-02\n",
      "  1.29440762e-02  3.30514795e-02  2.36463737e-02  3.03717585e-02\n",
      "  5.07559865e-02 -1.97283456e-02 -2.15124119e-02 -7.68753794e-03\n",
      "  3.96881625e-02 -2.64772248e-02  1.81036464e-02 -3.55453893e-02\n",
      " -2.59550671e-02 -2.03840617e-02 -5.90411550e-03 -3.88648466e-02\n",
      " -1.29417266e-02  1.46553183e-02  2.20339429e-02 -4.52127241e-02\n",
      " -1.32161136e-02 -2.02892903e-02 -2.90961656e-02 -1.69088673e-02\n",
      " -1.35535242e-02  2.45292227e-02 -5.21954217e-02 -4.22558882e-02\n",
      "  4.76651353e-02 -6.67549640e-02  3.29570377e-02  2.65821153e-02\n",
      " -4.74676451e-02  3.25338677e-02 -4.43107042e-04  1.56273647e-02\n",
      " -1.99872698e-03 -3.38923932e-02  1.98404187e-02  1.96229302e-02\n",
      " -6.81306068e-02  2.07695653e-02 -2.98768248e-02  2.28150602e-02\n",
      " -2.69125226e-02 -7.87719437e-03 -6.26645984e-02 -8.67457927e-02\n",
      " -4.11350555e-02  8.21488470e-03 -6.41515314e-03 -1.79430118e-03\n",
      " -3.91509675e-02  1.19201456e-02  1.70072485e-02  6.29009906e-03\n",
      "  3.21568038e-02 -1.76542995e-02 -2.34955567e-02 -2.48587134e-02\n",
      "  1.36461926e-02 -1.12943982e-02  8.56468192e-04 -3.28716573e-02\n",
      " -3.17772082e-02  4.15839893e-02 -3.18199193e-02 -2.16052853e-04\n",
      "  4.65552209e-02 -3.55738824e-02 -5.90852095e-02 -2.27111288e-02\n",
      " -1.96069268e-02  3.06874109e-02  7.77730398e-03  1.82433016e-02\n",
      "  1.32961804e-02 -3.90569482e-03  1.82859189e-02 -2.03659041e-02\n",
      "  4.14857542e-03 -1.48656242e-02 -5.28494789e-02  2.09662063e-02\n",
      " -3.89082769e-02 -4.00326662e-02  2.60301612e-02  4.19650060e-02\n",
      " -6.01488951e-03 -1.32591326e-02  1.09876686e-02  1.28163523e-02\n",
      "  2.25682228e-02 -1.44580863e-02  3.43935871e-03 -2.53761005e-02\n",
      " -2.78824346e-02  1.84932196e-03 -3.81324233e-02 -1.14484080e-02\n",
      " -1.16320250e-02  1.02015451e-02 -4.05376047e-03 -1.16144321e-02\n",
      " -2.36939298e-02 -2.16888908e-02 -7.55569625e-03 -6.64445981e-02\n",
      "  2.28653155e-02]\n",
      "255499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 39 testing: 27\n",
      "correctsource    23\n",
      "missed           16\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.85714286 0.71428571 0.4        0.8        0.8        0.2\n",
      " 0.6       ]\n",
      "Accuracy: 0.6410256410256411\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.68      0.74      0.71        23\n",
      "       missed       0.57      0.50      0.53        16\n",
      "\n",
      "    micro avg       0.64      0.64      0.64        39\n",
      "    macro avg       0.63      0.62      0.62        39\n",
      " weighted avg       0.64      0.64      0.64        39\n",
      "\n",
      "accuracy = 0.5185185185185185\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.59      0.62      0.61        16\n",
      "       missed       0.40      0.36      0.38        11\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        27\n",
      "    macro avg       0.49      0.49      0.49        27\n",
      " weighted avg       0.51      0.52      0.51        27\n",
      "\n",
      "(325,)\n",
      "[ 2.36268151e-02  2.88795866e-02 -1.00314805e-03 -1.74492032e-02\n",
      "  4.52175195e-02 -7.87107771e-02  1.63756724e-02 -7.73647138e-03\n",
      " -6.07934796e-02  2.15155048e-02 -2.90662572e-02 -7.46020247e-02\n",
      " -1.32556578e-03 -1.52632895e-02 -1.94831770e-02 -3.52862464e-04\n",
      " -1.92265197e-02 -3.50279556e-02  5.06324628e-01 -4.94729876e-04\n",
      " -1.39857146e-02  2.42023908e-02 -2.30315846e-02  2.09548854e-02\n",
      " -2.77401530e-02  2.88581929e-03 -2.33500821e-03 -9.19222539e-03\n",
      " -2.83913344e-01  6.01212352e-01 -6.09819872e-03  1.37447995e-01\n",
      "  4.19563710e-02 -5.68581561e-03  1.83438133e-02  3.01934757e-02\n",
      "  4.57814456e-02 -5.35453160e-02  1.01357127e-02  1.85642631e-01\n",
      " -6.37777460e-02  7.77258276e-01  2.92307482e-03  4.52791653e-02\n",
      " -1.08121746e-02  1.30212233e-02 -4.31928538e-02  2.22081354e-02\n",
      " -5.01692329e-02 -1.93544304e-02  2.31208329e-02  1.97788510e-02\n",
      " -2.23544174e-02  4.77802086e-02 -1.83384635e-01  2.13769373e-01\n",
      " -3.24171709e-02 -1.13746246e-01 -1.70459660e-01  1.48198306e-02\n",
      " -5.13327483e-02 -6.61116671e-02 -8.06882866e-03  3.40876693e-03\n",
      " -2.03348821e-02 -3.98862698e-04 -3.73366645e-04  1.38564260e-02\n",
      "  1.15662829e-02 -2.25923153e-02 -3.65301038e-02  1.99844227e-02\n",
      "  1.99734130e-02 -1.99729944e-01  9.35605955e-03 -8.37611408e-01\n",
      "  1.77817767e-02 -3.95649434e-02 -6.55107932e-02 -2.03466449e-02\n",
      "  1.17817604e-02 -4.15653790e-03 -1.11529254e-02 -5.52133371e-03\n",
      "  2.99721037e-02  3.93843033e-04  1.21349779e-02  2.68329326e-02\n",
      " -3.80545831e-01 -4.72700150e-02 -1.36263888e-02  2.24606750e-02\n",
      " -1.44748932e-02  9.03061033e-03 -4.15280561e-03  1.10181092e-03\n",
      "  1.78616120e-02  1.03914441e-02  4.29553255e-01  7.53407672e-02\n",
      " -1.39096753e-03 -1.80335266e-02  5.21092289e-02  3.62943684e-02\n",
      "  3.23759542e-03  3.32757973e-02  9.12240447e-03 -4.10632685e-02\n",
      "  5.51807233e-03  8.58225464e-05 -5.97669317e-02  2.23684245e-02\n",
      "  3.49834546e-02 -6.17172965e-03 -8.97582882e-02  7.46043759e-01\n",
      "  5.26595704e-02  2.75523563e-02 -3.33390377e-02  2.37085446e-02\n",
      " -3.06865009e-03  4.39874710e-01  4.67005295e-03  9.04724769e-03\n",
      "  6.18526082e-02  3.90064190e-02  1.19824815e-01  2.66947610e-02\n",
      "  9.40910610e-03 -7.48366741e-02 -8.03595827e-03 -1.49736350e-02\n",
      " -1.04525302e-02 -1.26800154e-01 -1.01652280e-01 -1.54076476e-02\n",
      "  2.59265349e-02 -3.21779092e-03  2.00312864e-04  4.48069991e-02\n",
      "  3.17414621e-03  3.21393560e-02  2.28928049e-02 -7.08726642e-02\n",
      "  2.11406783e-03  5.01729843e-02  1.49323250e-02  5.66555250e-03\n",
      "  5.53545809e-02  2.22703022e-02 -2.13336074e-02  4.27464536e-03\n",
      " -3.45580085e-02 -1.64988061e-02 -8.41221283e-02 -1.08444817e+00\n",
      " -4.03407986e-03  1.60365526e-02 -2.22268441e-02  2.92881074e-01\n",
      " -2.61918809e-02  3.30169414e-03  1.54795312e-02  1.25098971e-02\n",
      " -4.79963316e-03 -8.25211704e-02 -9.99293772e-02 -1.11547031e-01\n",
      "  1.49125948e-03 -8.49593798e-03  1.89861495e-01  2.67378775e-02\n",
      "  3.84081149e-04 -1.45144828e-03  7.08385071e-01 -3.65462855e-02\n",
      "  5.18292746e-02 -3.28969361e-03 -2.19794573e-02 -3.38418703e-02\n",
      " -1.27052759e-02 -3.98403131e-03 -8.14627672e-01 -1.36987136e-02\n",
      "  1.85946810e-02  3.52310559e-03 -9.57055769e-03 -2.96621829e-03\n",
      " -4.96703423e-03 -1.17292611e-02  3.31256036e-02 -8.93934487e-03\n",
      "  2.98965070e-02  5.98177491e-01 -5.02467621e-02 -3.79062380e-01\n",
      "  2.57241282e-02 -4.61691974e-04  5.74979144e-02  1.01150194e-02\n",
      "  2.48519152e-02 -7.61431924e-02 -1.81780664e-03 -1.42545311e-02\n",
      "  3.00272913e-02 -6.16870154e-02  1.73384562e-02 -2.55138673e-01\n",
      " -1.22464342e-02  4.11018899e-02  1.53895670e-02  1.92990921e-02\n",
      "  5.44548080e-01  3.04158523e-02  1.03004690e-02  2.91150229e-02\n",
      " -1.38823098e-02 -1.05018362e-02  2.10478663e-02  4.02786174e-02\n",
      " -2.45585814e-02 -8.51652195e-02  5.52078234e-02 -1.94123211e-02\n",
      " -6.16957966e-01  3.51964805e-02 -1.03435680e-02  1.31831070e-02\n",
      "  1.57748337e-02  3.36445847e-02 -4.21394915e-02 -2.91395220e-02\n",
      " -2.73229484e-01 -1.65818241e-02 -1.59089450e-02 -1.16598132e-01\n",
      " -5.49072525e-02 -6.27385970e-03  1.26611974e-01  2.54189405e-02\n",
      "  4.14263972e-02 -2.32442212e-01  5.24748601e-02 -2.05867440e-03\n",
      "  7.89512202e-02  3.24531685e-02  3.05641582e-02  1.32815825e-02\n",
      " -7.79304164e-02 -4.41279623e-01  7.67301189e-02  8.10623361e-03\n",
      " -2.79881549e-02  1.25738847e-02  3.93429586e-02  3.41444824e-02\n",
      "  2.65455397e-02 -4.90744778e-02 -3.13810061e-02  4.59598301e-02\n",
      " -2.48761367e-02 -2.83760772e-01 -2.33757727e-02  2.50153270e-01\n",
      " -1.30134721e-02 -6.10747429e-03  9.28281795e-03  1.03516265e-02\n",
      "  9.05702505e-03 -3.06889138e-02  6.07618855e-03  3.58898594e-02\n",
      " -5.25357182e-02  2.72550958e-03  1.84211043e-02  7.08734933e-03\n",
      "  3.82538666e-03  9.39020513e-02 -9.82004003e-02  8.19006388e-02\n",
      "  9.03601704e-03  3.65744499e-01  1.82431607e-02 -1.47667747e-02\n",
      "  4.76455818e-02  2.25387297e-02  8.56723233e-04 -1.79341133e-02\n",
      " -1.21721563e-02 -1.69547331e-02  1.92110592e-02 -7.01562027e-03\n",
      "  3.50929470e-02 -6.45402117e-02 -1.99631433e-03 -2.13683153e-02\n",
      "  1.88009391e-02  1.94639574e-02  1.03887702e-01 -1.76723427e-02\n",
      " -1.85142018e-01 -5.73998422e-02 -5.55214077e-02 -1.51928134e-02\n",
      "  6.16223692e-03 -3.67972949e-03  9.70584905e-03  3.13698128e-02\n",
      " -4.60348701e-01 -2.16303329e-02  1.09029929e-02  2.84721008e-02\n",
      "  4.32593103e-02  8.54228213e-03  8.38465291e-02  1.10211463e-02\n",
      " -1.55545023e-02 -6.15881201e-03 -1.43793495e-02 -2.74016114e-01\n",
      "  4.04738872e-02  2.69709455e-02 -6.22117130e-02  3.82172152e-02\n",
      "  3.73280465e-02]\n",
      "258618\n",
      "training: 36 testing: 25\n",
      "correctsource    20\n",
      "missed           16\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.83333333 0.33333333 0.2        0.6        0.8        0.8\n",
      " 0.75      ]\n",
      "Accuracy: 0.6111111111111112\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.65      0.65      0.65        20\n",
      "       missed       0.56      0.56      0.56        16\n",
      "\n",
      "    micro avg       0.61      0.61      0.61        36\n",
      "    macro avg       0.61      0.61      0.61        36\n",
      " weighted avg       0.61      0.61      0.61        36\n",
      "\n",
      "accuracy = 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.71      0.67        14\n",
      "       missed       0.56      0.45      0.50        11\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        25\n",
      "    macro avg       0.59      0.58      0.58        25\n",
      " weighted avg       0.59      0.60      0.59        25\n",
      "\n",
      "(325,)\n",
      "[ 2.63516092e-02 -2.08256086e-03  4.02496400e-02 -7.94906541e-03\n",
      "  4.69269203e-02 -2.44230308e-02 -1.46732427e-02  4.88784547e-02\n",
      "  9.40249075e-03 -6.07429148e-03  6.92492501e-02 -5.04649421e-02\n",
      " -5.58212519e-02  8.43002311e-03  3.13447452e-02  5.16925932e-03\n",
      " -2.40880288e-02 -2.81285170e-02  4.89686508e-02 -4.63540656e-02\n",
      " -1.22092987e-02 -1.38019371e-02 -7.20812609e-02  2.45295323e-02\n",
      "  1.16736583e-02  7.48895775e-02  3.30554902e-02 -6.52285037e-03\n",
      " -6.62327578e-02 -2.14244910e-02 -1.46482953e-02 -1.72039227e-03\n",
      "  8.05305985e-03 -1.10978749e-02  5.73460120e-02  4.96088668e-02\n",
      " -4.32488578e-03  1.03502651e-03 -1.46996501e-02  7.90192328e-03\n",
      " -1.76819507e-02  1.58950617e-02  5.05872607e-02 -2.38588674e-02\n",
      " -5.39945860e-03  4.77916021e-04 -6.57077079e-02  1.17818273e-03\n",
      " -7.01873614e-03  1.84957720e-02 -2.73080320e-02  5.02401998e-02\n",
      "  2.11735484e-02 -2.70149847e-02 -3.82886951e-02  2.56546677e-02\n",
      " -1.21087396e-02  7.12535974e-02 -4.41099262e-02  3.80343916e-02\n",
      " -3.83610588e-02 -5.01686177e-02  1.22244545e-02 -1.32581377e-02\n",
      " -4.64871320e-02  2.37295319e-02 -6.92198040e-03 -4.87431061e-02\n",
      " -1.14776019e-02  4.19603899e-03 -3.86331552e-03 -3.92566575e-02\n",
      " -2.12343184e-02  6.75263426e-02 -8.01601152e-04  8.55801657e-03\n",
      "  1.69705405e-02 -1.49747937e-02 -2.93241028e-02 -1.25624196e-04\n",
      " -3.76192154e-02 -3.18129712e-02 -1.59310670e-02 -7.20342881e-03\n",
      "  1.33382644e-02  8.29669932e-02  5.33627851e-04 -2.08617260e-04\n",
      "  1.43633824e-02 -6.24535648e-02  2.52904360e-02  1.87308158e-02\n",
      "  1.99272958e-02  3.32915900e-02  5.20821836e-02 -1.29251816e-02\n",
      "  4.31048383e-03 -2.59163552e-02 -1.76189467e-02  2.44847825e-02\n",
      "  5.06139104e-02  1.82361222e-02 -6.31636619e-02  1.35580236e-02\n",
      " -1.64499215e-02 -9.78945273e-03 -3.10971162e-02  5.42566176e-03\n",
      "  2.83013869e-02 -3.85130404e-02 -1.83457759e-02 -3.34997428e-02\n",
      " -4.22861582e-03  2.11851011e-02 -1.52551623e-03 -6.23070627e-03\n",
      " -2.77276759e-02 -1.54078843e-03 -1.05497536e-02 -1.88277294e-02\n",
      " -3.35361264e-02  2.21572882e-02 -4.36649649e-02 -2.27601249e-02\n",
      " -8.59476174e-03  7.32447550e-03 -2.43674275e-02  2.97168083e-02\n",
      "  5.94910556e-02 -9.02022991e-03 -4.96742525e-02  1.20710907e-02\n",
      " -2.88412649e-03 -1.04816892e-02 -1.28279671e-02 -2.94450079e-02\n",
      " -7.71171290e-03  2.34546333e-02 -4.86889536e-02 -2.13496164e-02\n",
      "  4.46671432e-02 -3.23581754e-02 -4.73853457e-02 -1.89608910e-03\n",
      "  2.22567907e-02  7.90364911e-03  1.06098608e-02  1.20668118e-02\n",
      "  8.97866905e-03 -1.61655112e-02  3.87089937e-02 -4.36320446e-02\n",
      "  2.84031356e-02 -2.99599274e-02 -4.83841170e-02  1.42850352e-02\n",
      " -1.94586494e-02 -2.80215482e-02  5.43391449e-02  4.43391276e-03\n",
      " -2.16449950e-02 -4.48001854e-03 -1.08236241e-05  2.85432744e-02\n",
      "  2.93464326e-02 -9.75086272e-02  1.83864771e-02 -2.31493782e-02\n",
      " -4.01939674e-02 -4.32861772e-03 -1.44648697e-02 -7.34862652e-03\n",
      "  2.45430249e-03  3.93406176e-03 -3.84285117e-02 -3.91727949e-03\n",
      " -5.77297640e-03 -1.87212020e-02 -3.48295231e-02  1.47962659e-02\n",
      " -1.81323213e-03  2.10389119e-02 -3.93912110e-02  1.77515540e-02\n",
      " -7.95954644e-03 -2.22656394e-03  2.65207027e-02 -1.55733653e-02\n",
      "  4.04097499e-02 -2.91544113e-02 -9.26777787e-03  3.47053470e-02\n",
      " -3.60818179e-02 -1.21654264e-02 -5.72839925e-03 -3.42701815e-03\n",
      " -3.36135377e-02  1.16913178e-02 -4.50311500e-02  1.82298360e-02\n",
      " -7.79973098e-03  3.55310847e-02  4.34231133e-03  1.86769498e-02\n",
      " -5.32888031e-02  2.25183799e-02 -2.47633084e-02  1.98820999e-02\n",
      " -7.67861657e-03  2.76425685e-02  6.86623540e-02 -3.89855850e-02\n",
      "  1.22550078e-02  2.85531041e-02  5.58967178e-03 -4.97921649e-02\n",
      " -2.64142002e-02 -6.76154704e-03 -1.34045766e-02 -6.95959468e-03\n",
      "  2.22981712e-03 -5.91722057e-02  4.42782279e-02 -2.22310112e-02\n",
      "  1.19824173e-02 -1.78601970e-03 -7.31400660e-03  1.70054250e-02\n",
      " -3.78171412e-02  1.77617125e-02  4.64151419e-03 -4.97168355e-02\n",
      " -1.01635930e-02 -1.35243480e-02  8.72443211e-03 -5.05144216e-02\n",
      " -1.23909369e-02  1.28038921e-02  5.88052232e-03  3.70335459e-02\n",
      " -1.01967928e-02  1.69585247e-02 -3.43830387e-02 -2.02428235e-02\n",
      "  2.52603004e-02  2.21988842e-02  6.35384792e-03 -1.07714936e-02\n",
      " -5.89692396e-03  6.24122523e-02  3.20701235e-02 -6.75164387e-03\n",
      "  1.11741563e-02 -1.20824208e-02 -1.63307906e-02  7.37045278e-02\n",
      " -3.02159968e-02  6.91163446e-03  8.54650442e-03 -2.45334500e-02\n",
      " -5.99472926e-02  1.30340200e-03  8.18934599e-03  9.58299096e-03\n",
      " -9.76013877e-03  2.43448396e-03  9.42521164e-03  8.14802156e-02\n",
      " -1.08132476e-02  7.94637646e-03  2.31279810e-02 -4.27604809e-02\n",
      "  6.81848100e-02  7.15026789e-02  2.43050785e-02 -5.98966576e-03\n",
      " -1.02733037e-02 -2.50163394e-02  7.19919756e-03  5.08605773e-02\n",
      "  1.78770716e-02  4.52031609e-02 -1.15129689e-02  1.34049370e-02\n",
      "  3.75941222e-02  2.27493791e-03 -1.75531200e-02  3.65845742e-04\n",
      " -2.35892898e-03  1.42447151e-02 -1.64643708e-02 -5.01949114e-03\n",
      "  2.90938214e-02 -3.21988834e-02 -1.68806992e-03 -5.67887573e-03\n",
      " -1.36624981e-02  2.24844755e-03  1.73747023e-02  1.69312570e-02\n",
      "  7.26627547e-03  3.07819523e-02  6.14657314e-02  1.53809742e-02\n",
      " -1.87630981e-02 -2.85889371e-02  1.91052233e-02  3.34947550e-02\n",
      " -1.43005848e-02 -5.48607869e-03 -7.98351502e-03  5.72536700e-03\n",
      "  6.12287591e-02 -2.82573469e-02  6.41889617e-02 -1.49347591e-02\n",
      " -1.19622446e-02 -1.46847095e-02  3.26568649e-02 -3.92264135e-02\n",
      " -2.99366117e-02 -6.38277054e-03  2.14340972e-02  1.13841851e-02\n",
      "  3.41861035e-02]\n",
      "271596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 testing: 25\n",
      "correctsource    22\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5 0.8 0.6 0.4 0.6 0.4 0. ]\n",
      "Accuracy: 0.4722222222222222\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.57      0.59      0.58        22\n",
      "       missed       0.31      0.29      0.30        14\n",
      "\n",
      "    micro avg       0.47      0.47      0.47        36\n",
      "    macro avg       0.44      0.44      0.44        36\n",
      " weighted avg       0.47      0.47      0.47        36\n",
      "\n",
      "accuracy = 0.52\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.64      0.56      0.60        16\n",
      "       missed       0.36      0.44      0.40         9\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        25\n",
      "    macro avg       0.50      0.50      0.50        25\n",
      " weighted avg       0.54      0.52      0.53        25\n",
      "\n",
      "(325,)\n",
      "[ 0.02360272  0.06754789  0.00061199 -0.01262677  0.025216   -0.00928811\n",
      " -0.01824695 -0.01896374  0.0452313   0.00209557  0.01825519  0.03337732\n",
      " -0.03081427  0.01877798  0.01479323  0.01744037  0.00918388 -0.00934068\n",
      "  0.01549421 -0.05154279  0.04131446 -0.010229    0.01562321 -0.04474577\n",
      "  0.02960056 -0.00712374 -0.00591567  0.00358556  0.0418359  -0.01833789\n",
      " -0.01893987  0.00569731 -0.0211715   0.00036624 -0.00573005 -0.01794865\n",
      "  0.00823529  0.03283172 -0.02257975 -0.00391591  0.03052388 -0.0187295\n",
      " -0.01059504  0.02336143  0.02609561  0.01196172 -0.00985526  0.05387538\n",
      "  0.00859445  0.03425079  0.03465342  0.00142694  0.06673059 -0.0126745\n",
      " -0.02266703 -0.00589289 -0.01562914  0.00929709 -0.02087749  0.01056048\n",
      "  0.00319333 -0.02916406  0.01721937  0.05507639  0.04719613  0.02584391\n",
      " -0.04255182  0.0485984  -0.00233872  0.0018318   0.00705301 -0.0022836\n",
      " -0.01313038 -0.02696342 -0.02634118  0.00553776 -0.06388809 -0.05190538\n",
      " -0.02344628 -0.00576912 -0.01784826  0.02532453  0.01077198 -0.00462222\n",
      " -0.01738218  0.03089099  0.00746027 -0.02055815  0.00418618  0.01766078\n",
      "  0.02864937  0.0113628  -0.02383729  0.00792383  0.00992615  0.00790531\n",
      " -0.0042031   0.00661982 -0.03099571  0.02175897  0.00625278  0.00631733\n",
      " -0.01156593 -0.0373002  -0.00313393  0.00246321 -0.00300093 -0.00303276\n",
      "  0.02394296 -0.03747604  0.00798302  0.00452112 -0.02163086 -0.0404143\n",
      "  0.06925928 -0.05176433 -0.03040351 -0.02599946  0.03121718  0.01386545\n",
      "  0.03120443 -0.02629734  0.00034249  0.01981518  0.02499909  0.01978463\n",
      " -0.03353233  0.00299071 -0.00024933 -0.09635537  0.03100475 -0.04782979\n",
      "  0.00745338  0.01261125  0.04741619 -0.02957307 -0.01670884 -0.00170753\n",
      " -0.0066939  -0.00362921  0.02587757 -0.07132056  0.00497797 -0.03911395\n",
      "  0.0025087  -0.00708264  0.0521776  -0.02565152 -0.01818226 -0.03229056\n",
      " -0.01284311  0.04025742  0.03779127 -0.02066469  0.00661647  0.01589988\n",
      " -0.02313007 -0.00767571 -0.01194534 -0.0177829  -0.00363139 -0.00221956\n",
      "  0.02405191 -0.02715033 -0.0437073  -0.00326418 -0.01801616 -0.00432347\n",
      " -0.00895021  0.03657679 -0.00422899  0.00847536  0.01237815 -0.00907309\n",
      "  0.00789302  0.02564476 -0.02369631  0.00543334  0.03516901  0.00230677\n",
      "  0.0117456  -0.03862736 -0.03354052  0.07211197 -0.00948288 -0.0061606\n",
      "  0.0257177  -0.00974601 -0.04632941  0.02114957 -0.05723658  0.03896097\n",
      "  0.0373228   0.00105507  0.01814418 -0.01682359 -0.04233934 -0.07080804\n",
      " -0.02046693  0.01039073 -0.03338087  0.03190827  0.00730647  0.04221492\n",
      "  0.04546109  0.03288726  0.02172979  0.04308191 -0.03271346  0.01747282\n",
      " -0.00922425  0.01580927 -0.00029823  0.03173973  0.00176374  0.04680069\n",
      "  0.0148346   0.02420205 -0.06644249 -0.0339212  -0.05609955 -0.03183764\n",
      " -0.01730117 -0.00037098 -0.00193158  0.0457035  -0.01181886 -0.01699835\n",
      " -0.03202142  0.00751489  0.02175576  0.01560444  0.01739615 -0.01381176\n",
      " -0.01033666  0.02024857  0.02205959 -0.0058431   0.00550018 -0.08446233\n",
      " -0.04279636 -0.01689259 -0.0128702   0.03274907 -0.01894315  0.00398532\n",
      "  0.03409265  0.05398528 -0.01124172 -0.01976483  0.02389171  0.01024647\n",
      "  0.00672499 -0.02649382 -0.04818797  0.00436819  0.04027997  0.0250351\n",
      "  0.02881514 -0.06227249  0.0481385   0.04422665  0.03380948 -0.01684328\n",
      " -0.00549599  0.07466046  0.02166035  0.03244902  0.01090133 -0.0137412\n",
      "  0.01059953 -0.03443609 -0.02501936  0.01585992  0.01532512  0.00201821\n",
      "  0.02968474  0.00844639  0.04234882  0.0006456   0.01201727 -0.03737814\n",
      " -0.0163894  -0.05417716  0.02149498 -0.00173273 -0.04372044  0.01809812\n",
      " -0.0328131   0.00345499  0.0202291   0.00734998 -0.00567272 -0.00368904\n",
      "  0.01261804  0.01946999 -0.01677855  0.00154285 -0.04373233  0.06380185\n",
      "  0.03152349  0.01777964 -0.04301737  0.01013845  0.04116407 -0.02369617\n",
      " -0.02985467 -0.00486997  0.0138219   0.01668655 -0.01551118  0.02111824\n",
      " -0.00417938  0.00691994 -0.04259009 -0.00906219  0.         -0.01911105\n",
      " -0.02962207  0.01873953 -0.03090923 -0.00369333 -0.05182092 -0.04391481\n",
      " -0.00977236]\n",
      "314409\n",
      "training: 40 testing: 28\n",
      "correctsource    21\n",
      "missed           19\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           14\n",
      "correctsource    14\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.5        0.33333333 0.5        0.5        0.4\n",
      " 0.4       ]\n",
      "Accuracy: 0.425\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.43      0.29      0.34        21\n",
      "       missed       0.42      0.58      0.49        19\n",
      "\n",
      "    micro avg       0.42      0.42      0.42        40\n",
      "    macro avg       0.43      0.43      0.42        40\n",
      " weighted avg       0.43      0.42      0.41        40\n",
      "\n",
      "accuracy = 0.5357142857142857\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.56      0.36      0.43        14\n",
      "       missed       0.53      0.71      0.61        14\n",
      "\n",
      "    micro avg       0.54      0.54      0.54        28\n",
      "    macro avg       0.54      0.54      0.52        28\n",
      " weighted avg       0.54      0.54      0.52        28\n",
      "\n",
      "(325,)\n",
      "[-2.90083947e-02  2.07808193e-02  7.55580463e-03 -2.00985390e-02\n",
      "  3.96477707e-02 -9.55823654e-02  2.27256997e-02  1.52788437e-02\n",
      "  1.73498021e-02 -5.50005371e-02 -2.26130681e-02 -4.23082264e-02\n",
      "  3.27375980e-02 -1.51094662e-02  4.34521280e-02 -2.77974638e-02\n",
      "  1.11545208e-02 -1.73362042e-02 -4.25462871e-03  1.66448842e-02\n",
      "  2.37199378e-02  3.58223612e-04  4.30969425e-02  4.17518136e-02\n",
      "  4.70372357e-03 -1.98680443e-02 -1.98406382e-02  1.89903202e-02\n",
      "  7.28551588e-04  3.43256877e-02  2.96793866e-02 -1.88717705e-03\n",
      "  7.24875123e-02 -4.01138585e-02 -4.31963138e-02 -2.07255063e-03\n",
      "  7.71066060e-02  2.09752874e-02 -1.12561993e-02 -9.70068105e-03\n",
      " -1.82634505e-02 -1.25907047e-02 -1.36722238e-02  1.81823619e-04\n",
      " -8.09158671e-03 -4.50985800e-02 -4.41340393e-03 -2.71852496e-03\n",
      "  4.85775936e-02 -3.96427920e-02 -1.11811626e-01 -8.27231840e-02\n",
      " -2.93146062e-02  1.09859830e-03  7.90024813e-03  9.66519700e-02\n",
      " -3.16593436e-02 -8.46332140e-02  1.21783590e-02 -1.22366375e-02\n",
      " -3.96437686e-03  7.34896962e-02  9.43062623e-03 -1.30677299e-02\n",
      "  6.24987919e-02 -7.90287351e-02  3.68833702e-03  3.08611705e-02\n",
      "  1.07994456e-01 -6.14300277e-03  1.97733920e-02 -1.56919155e-03\n",
      "  2.29567450e-02  1.41437500e-02  4.79024030e-03 -6.32193824e-03\n",
      "  2.22052976e-02  6.28218963e-02 -3.94463524e-02 -5.53650359e-02\n",
      " -4.51206365e-02  1.69000061e-02  2.94005835e-02 -3.01564764e-02\n",
      " -7.72014981e-03 -2.37926325e-02 -2.90761295e-02  2.74712559e-02\n",
      " -4.29562833e-03 -4.17879503e-02 -7.33846526e-03  3.17662056e-02\n",
      " -3.45247512e-03 -2.09099719e-03 -7.10839281e-02  6.37265416e-03\n",
      "  3.45805713e-03  1.90764695e-02 -2.84864392e-02  1.76387301e-02\n",
      "  9.87748048e-03 -5.10270585e-02 -1.60917741e-02 -1.47283806e-02\n",
      " -1.75844441e-02  7.43968303e-04  4.22491338e-02  6.51387431e-03\n",
      " -2.84741563e-02 -5.31843105e-03  2.81362664e-02  2.98018758e-02\n",
      " -4.30199051e-02 -1.16053707e-02 -5.14018769e-02  1.40533211e-02\n",
      " -3.94284413e-02  3.15716673e-03 -2.52471885e-02  3.23892751e-02\n",
      " -3.10585831e-02  9.20751519e-02 -7.25388321e-02 -4.47957397e-02\n",
      " -1.29875910e-02 -9.93779962e-03 -3.37089016e-02 -1.99273344e-02\n",
      " -4.18940860e-02  6.97393277e-03 -5.99360375e-03 -8.79172422e-03\n",
      " -3.66328708e-02 -2.86417062e-02 -6.15879385e-02 -1.77817647e-02\n",
      " -3.83319104e-02 -3.16389539e-02  1.74961112e-02  4.83119972e-02\n",
      "  1.15725530e-02 -1.32207330e-02  6.17083474e-03  2.07847796e-02\n",
      "  2.12246501e-02 -2.16345091e-02 -9.25711132e-03 -6.01790073e-03\n",
      "  6.81717829e-03 -3.24462032e-03 -7.55584098e-02  2.22662920e-02\n",
      "  2.11044617e-02  2.18106822e-02 -3.91023461e-02  9.19810910e-02\n",
      " -2.77283647e-03  1.02991430e-01 -1.06963780e-02  1.69135996e-02\n",
      " -3.14064660e-02  5.78780701e-02 -1.71872750e-02  3.64628991e-02\n",
      " -7.11603418e-02  8.07259239e-02 -7.36561714e-02 -2.35829806e-02\n",
      "  1.25426782e-02 -2.07310905e-02  6.68655883e-03  7.95101880e-03\n",
      " -3.34284299e-02 -7.81388254e-02  6.51307092e-04  1.72564447e-02\n",
      "  4.36230662e-02 -2.19134777e-02  3.11804832e-02 -3.26327951e-02\n",
      " -7.06163033e-03 -6.83587970e-03  1.48887881e-02  8.53410524e-03\n",
      "  1.28817059e-02 -1.43912086e-02  9.39482926e-03 -3.80969624e-03\n",
      "  4.13217604e-02 -7.04107959e-03 -8.41219068e-03 -2.07318741e-02\n",
      "  1.75335433e-03 -9.18021435e-03 -3.11285574e-03  1.82534321e-02\n",
      " -4.72077895e-02 -1.91155443e-02  8.97674076e-02 -6.57405244e-02\n",
      " -2.32991930e-02 -7.33908172e-03  5.97960056e-02  1.33152840e-02\n",
      "  3.70468511e-03 -3.28946642e-02  2.85769166e-02 -5.24641469e-02\n",
      " -1.05756008e-02  7.92675543e-02  7.95535818e-03  4.25671658e-02\n",
      " -2.56714285e-02  9.37192712e-02  3.12460024e-02 -3.07848654e-02\n",
      "  9.11203487e-02 -3.78834752e-02 -2.06415822e-02  4.76540506e-03\n",
      "  5.49184099e-02  1.28970308e-02 -1.07316577e-02 -5.00649937e-04\n",
      " -1.07400072e-03 -3.18999132e-02  6.23515476e-02  8.79514342e-03\n",
      " -3.27636147e-02 -2.20204683e-02 -1.75843723e-03  5.49564486e-03\n",
      " -4.02743988e-02 -5.97924509e-02  1.37799476e-03  7.59324507e-03\n",
      "  1.24517503e-02  4.67161495e-02 -3.54160275e-02 -3.39306710e-02\n",
      "  1.10904923e-02  4.26968862e-03 -1.34156785e-02  2.21187335e-02\n",
      " -1.33692743e-02 -3.97799900e-02 -7.34186649e-02  3.17885680e-02\n",
      "  8.89801033e-02 -1.56082331e-03  8.27052104e-03 -5.89743967e-03\n",
      " -2.13904573e-02  4.51652061e-03 -9.22162199e-02 -1.75098407e-02\n",
      " -2.82176311e-02 -3.60104688e-02 -2.62913942e-02 -2.55134746e-02\n",
      " -3.45636577e-02  7.69435029e-02 -1.02553874e-01  4.94192924e-02\n",
      " -1.53722827e-02  2.06642775e-02 -4.60715814e-02  2.36886202e-02\n",
      "  1.35949561e-02  3.63207997e-03 -2.24567578e-02  1.69750643e-02\n",
      "  1.46259149e-02 -2.22360818e-02 -5.70385838e-02  1.16685930e-02\n",
      "  7.40922673e-02 -2.87129976e-03  6.29000337e-05  3.39590821e-02\n",
      "  4.03903536e-03 -7.38499733e-02 -5.25840428e-02  1.77298035e-02\n",
      "  1.04171226e-02  3.80277557e-02 -6.39045664e-02  6.74331963e-03\n",
      " -1.55863569e-02  9.76298814e-02  2.81298967e-02  2.70167900e-02\n",
      "  1.39748276e-02  5.49451725e-02 -2.90198979e-02  1.58104331e-02\n",
      "  3.76854012e-02  5.64900136e-02  7.53422519e-02  1.05047253e-02\n",
      " -3.58941059e-03  2.79681085e-02 -1.31042227e-02 -3.92851300e-02\n",
      "  7.12763628e-02  1.03223468e-03 -5.45942992e-02  1.74763694e-02\n",
      " -3.88737825e-03  3.52687755e-02 -4.27850725e-02 -8.29743635e-03\n",
      " -2.35054677e-03 -1.45883403e-03 -4.93362105e-02  2.65204329e-02\n",
      "  0.00000000e+00  3.25341059e-02 -1.65781212e-02 -2.67733467e-03\n",
      " -1.67404469e-03 -4.99694794e-02 -2.68593141e-02 -3.35149757e-02\n",
      "  3.52414437e-02]\n",
      "336665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 30 testing: 20\n",
      "correctsource    16\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    11\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.2  0.8  0.75 0.5  0.5  0.5  0.5 ]\n",
      "Accuracy: 0.5333333333333333\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.57      0.50      0.53        16\n",
      "       missed       0.50      0.57      0.53        14\n",
      "\n",
      "    micro avg       0.53      0.53      0.53        30\n",
      "    macro avg       0.54      0.54      0.53        30\n",
      " weighted avg       0.54      0.53      0.53        30\n",
      "\n",
      "accuracy = 0.55\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.55      0.57        11\n",
      "       missed       0.50      0.56      0.53         9\n",
      "\n",
      "    micro avg       0.55      0.55      0.55        20\n",
      "    macro avg       0.55      0.55      0.55        20\n",
      " weighted avg       0.55      0.55      0.55        20\n",
      "\n",
      "(325,)\n",
      "[-2.96824855e-03  7.47424295e-03 -1.65266158e-02  3.92890817e-02\n",
      "  2.89783117e-02  9.73605671e-03 -2.19279261e-02  5.09045453e-03\n",
      "  1.40302686e-03 -3.33730059e-02  8.10305596e-03 -2.49017260e-02\n",
      " -6.01855572e-02  2.63012352e-02  1.00536882e-02 -5.23404699e-02\n",
      " -1.67164028e-02  6.62312943e-03  2.68964255e-02 -4.32125158e-02\n",
      "  5.74227687e-03  1.07232374e-02 -1.72454461e-02  1.05462433e-02\n",
      "  1.04397085e-02  1.40122964e-02 -2.25913412e-02 -4.43246359e-02\n",
      "  1.27611490e-02  4.70055495e-03  4.10752676e-05  2.54640406e-02\n",
      "  2.21021537e-02  3.16434017e-02  1.15144793e-02 -4.50060153e-03\n",
      "  4.72706054e-02  2.03303202e-02  1.24277523e-03  3.27802651e-02\n",
      " -3.66774917e-02 -4.51224235e-02  5.43818365e-03  2.77516231e-03\n",
      "  2.79179693e-02 -1.28380119e-02 -1.36624660e-02  5.55286785e-02\n",
      " -3.35016366e-02  4.33832542e-02  3.57524252e-02 -2.02408953e-04\n",
      "  6.39472565e-03  1.27784600e-02  9.34860688e-03  3.73323384e-02\n",
      " -2.64951149e-02  1.37633897e-02 -4.01557620e-02  7.62065033e-04\n",
      "  2.11781215e-02  2.95862769e-02 -3.43979195e-03  2.62255564e-02\n",
      "  3.46845415e-03  8.55157013e-03  1.39114327e-03  1.98446939e-02\n",
      "  4.73425853e-02 -2.07890974e-02  4.38609001e-02 -9.11804125e-02\n",
      "  5.03030035e-02  3.16399180e-02 -1.73847981e-03 -2.78886875e-03\n",
      "  9.74217488e-03  1.19611647e-02  7.39316830e-04  1.27533504e-02\n",
      "  1.65782349e-03 -7.55379037e-03 -4.25098445e-02 -1.91128520e-02\n",
      "  2.17556843e-02 -4.50986780e-02 -1.98018454e-02 -5.71066530e-04\n",
      " -1.94324630e-03  1.85681208e-03  1.97755175e-02  7.12491983e-03\n",
      "  2.43481508e-02  7.81405753e-03  5.96256216e-03  4.67079047e-03\n",
      "  3.21990204e-03 -3.01186141e-02 -3.89090857e-02  4.06003669e-03\n",
      "  3.21567057e-03 -5.00153387e-02 -1.14682564e-02  4.11783174e-02\n",
      " -3.37926367e-02 -6.46028346e-04  6.80452242e-03 -6.70874257e-02\n",
      "  3.77825854e-02 -1.88247696e-02 -5.48847584e-03  1.18197990e-02\n",
      " -8.05694058e-03 -5.09752257e-03  4.63728250e-02 -2.53707876e-02\n",
      "  5.22763262e-04  1.06494536e-02  7.27432001e-03 -8.50682603e-03\n",
      " -2.93425827e-02  4.44699546e-03  4.17213238e-02  4.99469543e-02\n",
      "  5.83298855e-03 -5.99146026e-02 -9.89277939e-02  1.10119365e-02\n",
      "  8.67294679e-03 -1.34338409e-02 -1.28099937e-02  1.20228464e-02\n",
      " -1.26429721e-02 -2.28399982e-02 -4.74242132e-02 -8.45104418e-03\n",
      "  1.42894353e-02  7.85766035e-03  2.30682184e-03  2.90360953e-02\n",
      "  2.23645903e-03  1.36256923e-02 -2.89327896e-02  2.16215359e-02\n",
      " -7.40999724e-03 -2.20175017e-02 -2.14715390e-04 -2.64438138e-02\n",
      "  2.50510088e-02  2.32060647e-02  9.64197088e-03  3.41755813e-02\n",
      "  5.28569229e-03 -7.57486975e-03 -1.47655094e-02 -1.48345114e-02\n",
      " -1.12571206e-02  1.15684277e-02  3.33698396e-03 -2.93779849e-02\n",
      "  1.41223718e-02 -5.86041632e-03 -1.35766480e-02  4.24523482e-02\n",
      "  2.21737771e-02  2.58657993e-02 -1.55040316e-02 -4.58658286e-02\n",
      "  1.32409355e-02  4.23953678e-03 -1.49579238e-02 -1.22065706e-02\n",
      " -6.69253103e-03  8.00140475e-03  2.55815884e-02  1.47140914e-02\n",
      " -2.46181375e-02 -5.28335228e-03 -4.12041879e-03  2.67876263e-03\n",
      " -2.65472839e-02  4.60387656e-02 -3.20560147e-02  2.32796678e-02\n",
      "  1.46415057e-02  2.02933326e-02 -1.37374101e-02 -2.32565639e-02\n",
      "  1.62512086e-03 -4.58979433e-03  4.57692797e-02  1.67715025e-02\n",
      " -1.70171548e-02  2.35028921e-02 -2.00023435e-02 -5.89687575e-02\n",
      " -2.95070941e-02  2.94995432e-03  1.56443920e-02 -3.89403469e-02\n",
      " -3.90599812e-02 -5.51094642e-04 -3.23116175e-02  2.06723880e-03\n",
      "  2.36679072e-02  2.87257897e-02  3.25634587e-02  2.38826561e-02\n",
      "  2.28415555e-02 -2.98200851e-02  5.53898779e-02 -2.39196064e-02\n",
      "  8.31729177e-03  8.68163122e-03 -1.42363553e-02 -2.65429067e-02\n",
      "  5.56455892e-02 -1.53911856e-02 -4.23795887e-03  3.86926768e-03\n",
      " -3.87712435e-02 -2.05627286e-02  2.14225012e-02  1.02375408e-02\n",
      " -2.59119967e-02  4.96599662e-03  5.14314302e-04 -2.05249129e-02\n",
      "  1.14909732e-02  1.31399482e-02  1.03077104e-02  5.18663002e-02\n",
      " -5.69506698e-03 -2.41236787e-02 -3.18075733e-02 -1.32233284e-02\n",
      "  1.88074561e-02  7.61380589e-03  9.65437734e-04 -1.29929986e-03\n",
      "  1.94386351e-02  1.47648800e-02  1.80885863e-03  1.59544724e-02\n",
      " -1.83629923e-02  4.60030648e-03  5.37057943e-03  1.01025480e-02\n",
      "  8.65349504e-03 -5.60248402e-03  8.46086474e-03 -1.00298237e-02\n",
      "  1.74851537e-03  2.28232875e-02 -3.46342631e-02 -2.37556733e-02\n",
      " -1.89765470e-02 -6.92323335e-03 -4.05197258e-03 -4.09112774e-02\n",
      " -2.68262892e-03  5.91498023e-02  2.07965830e-02  1.10514499e-02\n",
      " -1.58840459e-02 -5.27766581e-03  1.61373276e-02  1.83445145e-02\n",
      " -1.22228716e-02 -3.04916808e-02 -5.48057809e-03  2.85554781e-02\n",
      "  8.23949480e-03  2.86509319e-02  4.76705497e-02 -9.68485693e-03\n",
      "  2.85373825e-02  3.21139160e-02 -3.79926593e-04 -1.53292189e-03\n",
      "  4.86902746e-02  2.24208671e-02  2.33312267e-02 -1.35918319e-02\n",
      "  5.43465958e-03 -1.59649532e-02 -3.24259042e-02  6.04143380e-03\n",
      "  2.11238349e-02 -1.21777027e-02 -2.98953429e-02 -2.32769124e-03\n",
      " -3.92148141e-02 -1.72420859e-02  2.62407450e-02 -2.40668004e-02\n",
      " -1.24482078e-02  1.81304118e-02  1.66130887e-02 -3.14753422e-02\n",
      "  1.52125900e-02 -8.73929352e-03  1.79300550e-02 -5.33838171e-03\n",
      "  2.74225588e-02 -2.12853281e-02 -5.36454749e-03 -4.10466725e-02\n",
      " -7.75928216e-03  1.46750173e-02  1.32188819e-02  7.71820403e-03\n",
      " -3.15209343e-02  4.06522109e-02  5.59581243e-03  2.61067857e-02\n",
      "  5.15010257e-03  1.44968947e-02 -1.99655301e-02 -4.09467371e-02\n",
      " -1.77170150e-02  2.09670859e-02  3.50707249e-02 -4.27252172e-02\n",
      " -3.68163063e-02]\n",
      "337021\n",
      "training: 34 testing: 23\n",
      "correctsource    24\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.83333333 0.66666667 0.66666667 0.5        0.75       0.25\n",
      " 1.        ]\n",
      "Accuracy: 0.6764705882352942\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.76      0.79      0.78        24\n",
      "       missed       0.44      0.40      0.42        10\n",
      "\n",
      "    micro avg       0.68      0.68      0.68        34\n",
      "    macro avg       0.60      0.60      0.60        34\n",
      " weighted avg       0.67      0.68      0.67        34\n",
      "\n",
      "accuracy = 0.5652173913043478\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.69      0.69      0.69        16\n",
      "       missed       0.29      0.29      0.29         7\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        23\n",
      "    macro avg       0.49      0.49      0.49        23\n",
      " weighted avg       0.57      0.57      0.57        23\n",
      "\n",
      "(325,)\n",
      "[-4.35542224e-02  1.49542122e-02  1.65114952e-02 -1.52723370e-02\n",
      "  3.76916711e-02  1.96332670e-03  9.88545600e-04 -1.98108973e-02\n",
      "  3.97263963e-02 -8.04196022e-03  5.01506274e-02  2.31167332e-02\n",
      " -2.36827281e-02 -5.92924424e-02 -4.99979647e-03 -1.14991669e-02\n",
      "  1.22364640e-03  3.40683863e-03 -9.87290118e-03  2.38736271e-02\n",
      "  2.17225297e-02 -1.65629102e-03 -2.15763619e-02  1.10596887e-02\n",
      "  3.70242383e-02 -2.38927598e-02 -4.11260314e-02 -1.49675292e-02\n",
      "  6.23207494e-03 -1.81215818e-02  1.13524407e-02  4.85321236e-03\n",
      "  2.84500545e-02  1.15249347e-02 -8.29257584e-03  4.89253714e-02\n",
      " -1.51952115e-02 -3.36228630e-02  1.52021198e-03 -8.32513910e-03\n",
      "  2.15043046e-02  2.37404789e-02  2.27276891e-04  2.15326008e-02\n",
      "  3.25974302e-02 -2.64638974e-02  2.62176870e-03 -3.19213730e-02\n",
      " -1.89532742e-02  2.21299419e-02 -8.50294404e-03 -2.06484433e-02\n",
      "  1.50356054e-02  2.48795060e-02  1.81927155e-02 -6.76472586e-02\n",
      " -3.19649045e-02  1.82052169e-02  4.62041981e-03  1.18499017e-02\n",
      "  2.24848207e-03 -1.63085168e-02 -3.62338936e-03  4.11633077e-02\n",
      " -6.07428769e-03  4.82389003e-02  6.84354366e-02  3.67548762e-02\n",
      " -3.13368746e-02 -2.11323606e-04 -3.00779411e-03  3.30255457e-02\n",
      " -3.21661532e-02 -1.08086775e-02  1.29327399e-02  3.42493775e-02\n",
      " -1.25487363e-02 -2.42170082e-02  2.74305553e-03  3.44366565e-03\n",
      " -6.11010071e-02  3.72107466e-02 -2.03157244e-02 -3.13629103e-02\n",
      " -1.39162075e-02  3.44429445e-02  1.48161839e-02  3.27409828e-02\n",
      " -4.48718521e-02  2.70891927e-02  1.77351009e-02 -4.49028952e-03\n",
      "  1.78437905e-02  2.45329494e-02  8.44246470e-02 -4.71373821e-02\n",
      "  8.85546070e-03 -3.24997980e-02 -2.58506463e-02 -6.91684538e-03\n",
      "  3.33367366e-02  5.81776707e-02 -3.88156609e-02 -4.41842544e-02\n",
      "  1.24471047e-02 -2.73530195e-02  3.62989746e-02  2.19088545e-02\n",
      "  1.02893563e-02  6.94898017e-03  2.03413017e-02  1.06028826e-05\n",
      " -2.98103940e-02  4.65420915e-02 -3.20772645e-02 -3.90029648e-02\n",
      " -2.91045284e-02 -2.04052105e-03 -1.81241259e-02 -3.30488272e-02\n",
      " -3.48161179e-02  5.53513826e-03 -1.09373984e-02 -1.30199483e-02\n",
      " -3.87436885e-02  2.73767767e-02 -1.02579075e-02  3.48938691e-03\n",
      " -1.91184523e-02 -5.42111005e-02  4.55220757e-03 -7.10810388e-02\n",
      "  2.56448902e-02 -3.13109285e-02  2.17976861e-02 -4.11391687e-04\n",
      " -1.68474171e-02 -2.24376825e-02  8.88128613e-03 -2.56904729e-02\n",
      "  8.50409632e-03  6.14528074e-03  2.92154100e-02 -8.69844699e-03\n",
      "  1.07074987e-03  2.36463662e-02 -2.12590872e-02  3.95032120e-02\n",
      "  4.12308369e-02  2.37683683e-03  6.67732006e-04  4.20426363e-02\n",
      " -1.01322951e-02  8.25111167e-03  4.43827892e-02  2.33127343e-02\n",
      " -4.46974453e-02 -6.84135045e-03  7.31561118e-02 -1.85153097e-02\n",
      "  6.06662896e-03 -1.00309828e-02  1.18222576e-02  2.30328847e-03\n",
      "  1.18210405e-02  2.13642435e-02 -2.27947344e-02  3.99183795e-02\n",
      "  1.19983399e-02 -4.42361194e-02  3.13367963e-02 -1.06503762e-03\n",
      "  1.17036796e-02 -2.91639547e-02 -6.39480507e-02 -1.67117953e-02\n",
      "  2.32477937e-02  6.05803882e-02  3.23819266e-04 -8.21147556e-02\n",
      "  2.20182941e-02 -1.60268516e-03  5.55632933e-04 -5.32001488e-03\n",
      " -1.40405689e-02  3.99800714e-02  1.24752253e-02  7.11846225e-03\n",
      "  8.21820887e-03 -3.11088217e-02 -4.07141513e-02  1.45288085e-02\n",
      " -1.98353214e-03  2.38052686e-02 -2.70691400e-02 -2.29577672e-02\n",
      "  9.52319474e-03 -2.39191233e-02 -2.73205277e-02  2.05830620e-02\n",
      "  1.54083323e-02  5.39956909e-02  1.93910948e-02  1.82584000e-02\n",
      "  3.12904293e-02  4.63543312e-02  1.40407392e-02  3.86686588e-02\n",
      "  1.68768144e-02 -3.37573246e-02 -7.98131615e-03 -8.62426700e-03\n",
      " -8.94708092e-04 -3.84670214e-02  8.48237333e-03  2.71031143e-03\n",
      " -3.89489255e-02 -6.44921409e-04  5.15048807e-02 -4.18512501e-02\n",
      " -5.55362991e-03 -1.51353932e-02  2.35183834e-02 -1.51572112e-02\n",
      "  4.19827284e-03 -7.54354513e-03 -3.62609063e-02  1.43298283e-02\n",
      " -1.61043705e-02  1.39151045e-02  4.80112455e-02 -1.34884382e-03\n",
      "  1.43328468e-02 -2.07674834e-02 -1.73624724e-02  1.71591064e-02\n",
      " -1.22990433e-03  2.39711813e-02  1.54988653e-02 -1.22904220e-02\n",
      " -2.75999666e-02 -1.12425746e-02 -1.83193082e-02  3.27645074e-02\n",
      " -2.53811265e-03  1.56155714e-02  2.42685859e-02  5.42057445e-03\n",
      "  1.94515342e-02  3.05212691e-02 -2.08992882e-02 -1.62426798e-02\n",
      " -7.43548674e-03  2.40162748e-02 -4.73430631e-02  7.16584132e-02\n",
      " -2.75914947e-02  1.41759640e-02  2.90085340e-02  6.39103026e-03\n",
      "  5.33918027e-03 -2.22691089e-02  2.07310814e-02  3.13888440e-02\n",
      "  4.05356459e-02 -5.46535057e-02  9.69154249e-03 -8.72199032e-03\n",
      "  1.17976554e-02  2.06938694e-02  3.49965774e-02 -2.90435522e-02\n",
      "  6.60164881e-02 -1.69267507e-02 -1.24971211e-02 -5.53035618e-02\n",
      "  1.80178967e-02  4.64201145e-02 -7.40356616e-02  4.43162584e-02\n",
      "  7.37471246e-03  2.67304680e-02 -1.29213372e-02  3.09006754e-02\n",
      "  1.19625475e-02  2.07782511e-02 -3.42158871e-02  4.82481986e-02\n",
      "  8.19868833e-03 -4.42940226e-03 -3.81414827e-02 -4.43802929e-02\n",
      " -4.05650908e-02  3.52312218e-03 -1.05710734e-02 -1.08150118e-02\n",
      " -1.00197067e-02 -6.44369479e-03 -4.06496773e-02  1.35421244e-02\n",
      " -1.20369878e-02  2.77746846e-02  6.42335771e-03  2.04512951e-02\n",
      "  2.51718980e-02  2.65011118e-02  1.74609947e-02 -3.39369789e-02\n",
      " -2.33181538e-02  1.50259168e-02  5.50437111e-03  1.28041693e-02\n",
      "  9.00845247e-03 -2.63371063e-02 -4.14972963e-02  3.05522509e-02\n",
      " -9.98316768e-03  7.69933233e-03 -1.42613242e-02 -2.17063126e-02\n",
      "  1.89412787e-02  3.94733793e-02 -3.66654392e-02 -5.18719291e-03\n",
      " -1.64822110e-02]\n",
      "396250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 39 testing: 27\n",
      "missed           21\n",
      "correctsource    18\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           14\n",
      "correctsource    13\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.83333333 0.83333333 1.         0.5        0.8        0.4\n",
      " 0.8       ]\n",
      "Accuracy: 0.7435897435897436\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.75      0.67      0.71        18\n",
      "       missed       0.74      0.81      0.77        21\n",
      "\n",
      "    micro avg       0.74      0.74      0.74        39\n",
      "    macro avg       0.74      0.74      0.74        39\n",
      " weighted avg       0.74      0.74      0.74        39\n",
      "\n",
      "accuracy = 0.5185185185185185\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.46      0.48        13\n",
      "       missed       0.53      0.57      0.55        14\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        27\n",
      "    macro avg       0.52      0.52      0.52        27\n",
      " weighted avg       0.52      0.52      0.52        27\n",
      "\n",
      "(325,)\n",
      "[ 9.72182390e-03  6.44610288e-02  1.03064662e-02  2.31446837e-03\n",
      " -3.20697897e-02 -9.83224308e-03 -4.97459283e-02 -1.54111755e-02\n",
      " -3.88086648e-03  1.53191933e-02  1.37480347e-02 -1.22703310e-03\n",
      " -1.94827092e-02 -3.46098063e-02  4.75926287e-03  5.58741749e-03\n",
      "  4.69210024e-03 -1.22447827e-02  9.15898120e-04 -1.11610378e-02\n",
      "  7.90580331e-03  4.88955037e-03  4.48106203e-02 -4.86754230e-02\n",
      " -3.14149922e-02 -1.93319139e-02  8.07412173e-03 -9.71251055e-03\n",
      "  9.01719987e-03 -9.06757171e-03 -2.44092396e-02  1.37456685e-03\n",
      "  1.13623326e-02  4.46577877e-03  1.11065415e-02  1.11801964e-02\n",
      "  4.05107656e-03 -9.53757200e-04 -2.65033632e-03  2.53123074e-02\n",
      "  8.82574333e-03 -5.11561223e-02  2.60748031e-02  2.11381731e-02\n",
      "  4.93320327e-03 -1.71003260e-02 -5.95363723e-02 -3.79121191e-02\n",
      " -2.75663266e-02 -3.26780585e-02 -1.27590632e-02 -3.41488708e-02\n",
      " -2.50814382e-02  5.39696145e-03 -1.45268033e-02  7.40121763e-03\n",
      " -1.25240076e-02  4.65153815e-02 -1.52758919e-02  7.56441171e-03\n",
      " -2.24060330e-03 -5.09416046e-03 -4.57955740e-02  1.79367686e-02\n",
      "  3.30670758e-02  5.48924561e-02 -2.48365506e-02  9.44265551e-03\n",
      " -1.45672233e-03 -1.94044529e-02  2.57217574e-02  1.39676715e-02\n",
      " -2.64847993e-02  6.43631738e-03 -1.71158658e-02 -1.18243137e-02\n",
      "  1.99917021e-02  1.22998816e-02  4.06226329e-02  7.11665167e-03\n",
      "  1.54521355e-02  1.42745522e-02  4.07455504e-02  1.96614529e-02\n",
      "  1.85222560e-02 -2.02202955e-02 -2.75298942e-02 -1.16645481e-02\n",
      " -1.45383657e-02 -3.29221926e-03  2.87370309e-02 -8.62886931e-03\n",
      " -6.83045786e-03 -1.98292557e-02 -2.95150506e-02 -1.59660254e-02\n",
      "  2.18601683e-02  1.53825439e-02 -1.12418982e-03 -1.75087551e-02\n",
      " -2.96689779e-03 -5.40265761e-02  1.65105371e-02 -6.89716575e-04\n",
      " -3.57300452e-03  1.38089367e-02 -2.25045779e-02 -7.20349518e-03\n",
      "  1.20673961e-02 -2.61943204e-02 -4.30279679e-02  4.16530025e-03\n",
      "  1.30047455e-02 -3.21224221e-02  3.58432538e-02 -1.61008910e-02\n",
      "  1.51761480e-02  3.22069314e-02  9.06635148e-03 -8.89848898e-03\n",
      "  4.38838504e-02  2.39471459e-02  1.28745226e-04  1.25711540e-02\n",
      "  3.68561064e-02  1.73901698e-03  6.02224939e-03 -1.17859053e-02\n",
      " -8.80758514e-03 -1.41591354e-02  2.25548052e-02 -5.34968337e-03\n",
      " -3.46516463e-02  6.99708525e-03 -5.46933996e-02  1.27041585e-02\n",
      " -3.34771720e-03 -2.92897767e-02  1.84789308e-02 -5.78355783e-03\n",
      "  7.66284608e-02 -2.31712427e-03 -1.57152904e-02  2.04408239e-03\n",
      "  1.78184199e-02 -2.13967843e-02 -8.73225025e-03  4.24061785e-02\n",
      " -5.35542896e-03 -1.65589653e-03  2.64074961e-02 -5.37078068e-03\n",
      "  1.20181221e-02 -1.53956312e-02 -3.47203434e-03  3.73588857e-03\n",
      " -7.80814654e-04 -6.07761700e-03  7.88511706e-03 -3.01422750e-02\n",
      "  1.12515236e-02  3.92273985e-03 -4.01273850e-02  2.24760148e-02\n",
      "  5.50473555e-03 -4.13020819e-02 -7.82410438e-02 -1.90395725e-02\n",
      " -8.93581657e-03 -2.60090386e-02 -7.11647470e-03  1.07962501e-03\n",
      "  8.43389279e-03  1.43185160e-04 -3.52641537e-03  4.20399888e-02\n",
      "  5.49147711e-03 -3.03698783e-02 -3.21002356e-02  4.11892931e-02\n",
      "  1.78535363e-02 -1.00548581e-02 -2.96999128e-02 -3.18057753e-02\n",
      " -2.63037896e-02 -1.20884261e-02  1.02583363e-02 -6.42587629e-03\n",
      " -9.87254366e-03 -1.50044749e-02 -4.16951073e-03  1.59811155e-02\n",
      "  5.92345358e-02 -1.78086120e-02  2.59931278e-02  1.91737482e-02\n",
      " -3.54769629e-02  2.15115963e-02  1.22675378e-02  7.80316172e-03\n",
      "  1.08669043e-02 -1.17217401e-02 -5.61577857e-02 -3.45563049e-02\n",
      " -6.03621465e-05 -1.93952390e-02  4.42252281e-02  2.16535491e-02\n",
      "  1.47680884e-02  5.25401597e-02  2.42307034e-02 -7.70899284e-03\n",
      "  4.83909926e-02 -2.94177077e-03 -4.39442189e-03  3.23805634e-02\n",
      " -2.33196462e-02  1.50540447e-02  1.09411293e-02 -3.11860922e-04\n",
      "  1.05533382e-02 -1.29319995e-02  2.71809471e-02  4.23579695e-02\n",
      " -2.19415620e-02  1.61662757e-03 -1.15724700e-02 -4.41988235e-03\n",
      " -3.48116190e-04 -1.59448984e-02 -2.13750543e-02 -2.86108350e-02\n",
      " -8.02745329e-04 -3.45883787e-02  2.94659487e-02 -2.38611685e-02\n",
      "  4.13540098e-02  1.82564390e-02 -1.72493005e-02  3.65267132e-02\n",
      "  3.12222854e-02 -2.61806495e-03 -6.16653360e-02 -1.42394087e-02\n",
      "  1.12562493e-02  8.64027243e-03  1.05763819e-02 -2.60353160e-02\n",
      " -2.90556018e-02  1.43644068e-02  1.23910541e-03  8.61055888e-03\n",
      " -2.96528083e-02 -3.09477170e-02  2.69143366e-02 -1.25805179e-02\n",
      "  3.53585807e-03 -1.54841247e-02  1.39005536e-02  6.23414085e-02\n",
      "  2.72931073e-03 -2.79136870e-02  1.58069731e-02 -2.68369836e-02\n",
      " -1.05130324e-03  5.48635974e-02 -2.87716644e-03 -6.90372136e-03\n",
      " -3.70663583e-02 -6.49486225e-03  1.80883686e-02 -9.13989682e-03\n",
      " -8.40002114e-03 -2.91500182e-02 -3.00408018e-03 -1.29071508e-02\n",
      "  9.14039481e-03  4.30554252e-02 -2.40370351e-02 -2.60805803e-04\n",
      "  4.73267255e-02  1.59492800e-02  2.08511851e-02 -3.29949369e-02\n",
      " -1.97738969e-02  6.89682868e-04 -1.53571416e-02 -2.22917042e-02\n",
      " -6.80583621e-03 -8.11461808e-03 -8.40897937e-03 -6.19830181e-03\n",
      "  7.18922517e-03 -2.20779470e-02  2.96751702e-02  1.76772240e-02\n",
      " -1.69777300e-02  2.17364428e-02 -2.64241187e-02  1.92962560e-02\n",
      " -3.89613503e-02  2.92300954e-02  2.81709854e-02  7.60380961e-03\n",
      " -2.99014801e-02 -2.16490133e-02 -1.46154126e-02 -1.75022658e-02\n",
      "  5.87840973e-02  4.94812754e-03  1.74780921e-03 -1.19686947e-02\n",
      " -5.44143604e-03  1.15699866e-02  8.81223751e-03  3.06328888e-02\n",
      "  8.04670377e-03 -2.91459491e-02 -4.24751264e-02  5.65458083e-02\n",
      " -2.86147560e-02  2.72334161e-02 -5.21955883e-02 -1.76280999e-03\n",
      " -2.35821605e-02]\n",
      "403131\n",
      "training: 42 testing: 28\n",
      "correctsource    23\n",
      "missed           19\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.71428571 0.71428571 0.5        0.66666667 0.83333333 0.6\n",
      " 0.8       ]\n",
      "Accuracy: 0.6904761904761905\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.73      0.70      0.71        23\n",
      "       missed       0.65      0.68      0.67        19\n",
      "\n",
      "    micro avg       0.69      0.69      0.69        42\n",
      "    macro avg       0.69      0.69      0.69        42\n",
      " weighted avg       0.69      0.69      0.69        42\n",
      "\n",
      "accuracy = 0.5714285714285714\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.61      0.69      0.65        16\n",
      "       missed       0.50      0.42      0.45        12\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        28\n",
      "    macro avg       0.56      0.55      0.55        28\n",
      " weighted avg       0.56      0.57      0.56        28\n",
      "\n",
      "(325,)\n",
      "[ 0.02985919  0.01270557  0.00789072  0.01418172 -0.03551823 -0.00681416\n",
      " -0.04763002  0.03092961  0.0149958  -0.00430511  0.01997512 -0.047426\n",
      "  0.01332388 -0.0360297  -0.02650273  0.00247423  0.04776297 -0.00400275\n",
      "  0.02151761 -0.00540738  0.00649636 -0.04140969  0.02409391 -0.03408504\n",
      " -0.00682158  0.02001839  0.01349485  0.01406504 -0.01561325  0.00676002\n",
      " -0.00089706  0.02391241  0.01474494  0.05227063  0.02637357  0.00229617\n",
      "  0.02274279  0.0051274   0.02257079  0.05457954 -0.02371291 -0.02883108\n",
      "  0.00837951  0.01225896  0.01809278  0.01259278 -0.01127666 -0.00029998\n",
      " -0.00863703  0.0154861   0.03376363 -0.01793927 -0.01253258  0.00218717\n",
      " -0.08295575 -0.02547035  0.02683489  0.01243801  0.02210982  0.01834706\n",
      " -0.00280969  0.01549534 -0.01922181  0.00680875 -0.02820396  0.00682131\n",
      " -0.03890668 -0.01231796 -0.00721679 -0.04887634  0.03134372 -0.04161249\n",
      "  0.00841857  0.03077693  0.00321434 -0.0224085   0.0044267  -0.03297409\n",
      " -0.03897388  0.04558825 -0.00344912 -0.03033597 -0.01550398  0.00449859\n",
      " -0.01506586 -0.0117398  -0.00157442  0.01847664  0.00059714 -0.02141678\n",
      "  0.02045277  0.00176102  0.02794574 -0.05366339 -0.05292043 -0.01717753\n",
      " -0.03160322  0.01457656 -0.02521923 -0.03060678 -0.00452451  0.00722969\n",
      "  0.01274568  0.00251374 -0.01800039  0.00611963 -0.03320886  0.00635783\n",
      "  0.01796874 -0.02978388 -0.02470212  0.02309563  0.01567092  0.01826731\n",
      " -0.02774924  0.03397136  0.00413013  0.00276752 -0.02286554 -0.00989752\n",
      "  0.0217692   0.01311616  0.01744195 -0.00380326  0.02706456 -0.01526026\n",
      "  0.01225702 -0.03668404 -0.0701411  -0.02194463  0.04372858 -0.06101344\n",
      "  0.0005845   0.00435645  0.00547287  0.00364385  0.02345037  0.03524441\n",
      " -0.05322203  0.00245199 -0.01652413  0.00324862  0.02549938  0.01634193\n",
      " -0.02794776 -0.01382007 -0.05469113 -0.01732011  0.02501038  0.00463242\n",
      " -0.01325622 -0.01422269  0.01522207  0.02515995 -0.02682168  0.01071071\n",
      " -0.02252056 -0.00515628 -0.00140517 -0.03082675 -0.05010591  0.00698862\n",
      "  0.00087033 -0.0008092   0.00614317 -0.05275422 -0.05664268 -0.00980181\n",
      "  0.00819035 -0.01824665  0.00022155 -0.01028523  0.01804916  0.01217482\n",
      " -0.01135076 -0.0223651  -0.00509821 -0.01946239  0.00410366 -0.0544069\n",
      "  0.01634824  0.02189132  0.01304535 -0.03257816 -0.01566594 -0.01239937\n",
      " -0.00172846  0.03344679 -0.03072083 -0.0159938  -0.00989122 -0.02264807\n",
      "  0.01468323  0.03030132 -0.00919907 -0.04131189 -0.03318713  0.01913482\n",
      "  0.00788586  0.01302644 -0.01704974  0.01908831 -0.02794808  0.0164785\n",
      "  0.03112313 -0.00082209 -0.02037425  0.0099369  -0.01784341 -0.0192378\n",
      " -0.0246906  -0.02617235 -0.01637247 -0.0175541   0.04493358 -0.00394276\n",
      " -0.02172488  0.01741109 -0.01276722  0.01503457 -0.00808849 -0.01581402\n",
      "  0.02985595  0.00742651 -0.00063516 -0.01694955 -0.03062672 -0.01295868\n",
      " -0.01132431 -0.00381193 -0.01375841 -0.03333907  0.04923829 -0.04616888\n",
      "  0.02697146 -0.03836112  0.02929913 -0.01267686  0.00443113  0.02204597\n",
      "  0.02174283  0.0230684  -0.03022466  0.00050887  0.01515327  0.02036291\n",
      "  0.04183148 -0.04357771  0.01257079  0.00256429  0.00655479  0.00192805\n",
      " -0.01009425 -0.00931073 -0.00572168 -0.018508   -0.00379129 -0.0214443\n",
      " -0.01385233  0.00192428 -0.01335153  0.0060836  -0.00151008 -0.00497714\n",
      " -0.0369893   0.01467603  0.00203303 -0.00094698  0.00206125  0.02163675\n",
      " -0.03184329  0.017693    0.00427237  0.00653428 -0.03210487 -0.01142773\n",
      "  0.01305484  0.01573037 -0.01995541  0.00836462  0.00893012  0.02910321\n",
      " -0.01132709 -0.03485737  0.00753903  0.01803848 -0.00040153 -0.01088799\n",
      "  0.01809895 -0.03229261  0.02496404 -0.00048423  0.02836929  0.00055571\n",
      " -0.0541533  -0.01291064  0.02888604 -0.00806766  0.00813114  0.02308286\n",
      " -0.02957549  0.00502908  0.02516748  0.00122666  0.02550013  0.00181491\n",
      "  0.03214634 -0.04143936  0.02712672  0.00134675  0.00540069  0.03784988\n",
      " -0.00378552 -0.02875638  0.01903363  0.00390232  0.00296948  0.00540727\n",
      " -0.00830068  0.00061948  0.03811537  0.0079878  -0.03652696  0.00420751\n",
      "  0.03071235]\n",
      "408506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 30 testing: 21\n",
      "missed           16\n",
      "correctsource    14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           11\n",
      "correctsource    10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6  0.4  0.75 0.75 0.5  0.75 0.5 ]\n",
      "Accuracy: 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.56      0.64      0.60        14\n",
      "       missed       0.64      0.56      0.60        16\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        30\n",
      "    macro avg       0.60      0.60      0.60        30\n",
      " weighted avg       0.61      0.60      0.60        30\n",
      "\n",
      "accuracy = 0.6666666666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.60      0.63        10\n",
      "       missed       0.67      0.73      0.70        11\n",
      "\n",
      "    micro avg       0.67      0.67      0.67        21\n",
      "    macro avg       0.67      0.66      0.66        21\n",
      " weighted avg       0.67      0.67      0.67        21\n",
      "\n",
      "(325,)\n",
      "[ 6.41518276e-03  3.31969853e-02  2.14651632e-03  3.95125692e-03\n",
      " -1.71712946e-02 -2.18262450e-02 -1.06941613e-02 -6.19562440e-02\n",
      " -2.11321546e-02 -7.83562603e-03  3.67265611e-02 -1.98133398e-02\n",
      " -3.92764977e-02 -1.76157811e-02  4.04824102e-02 -8.46255813e-03\n",
      " -2.49786937e-02  7.90984233e-03 -3.21734109e-03  2.50164545e-02\n",
      "  1.71417178e-02 -9.58038411e-03  1.83420838e-02 -2.06509649e-02\n",
      " -1.36541377e-03 -1.17704211e-02 -1.92277872e-02 -4.29543443e-03\n",
      " -1.34703551e-02 -3.65722109e-03  1.98770981e-02  1.90507683e-02\n",
      "  3.17218285e-03  1.24778576e-02 -9.88802875e-03 -1.41973740e-02\n",
      " -4.96899586e-02 -3.39639050e-02 -2.11532773e-02 -5.44531048e-03\n",
      "  9.90838037e-03  2.69597402e-02  1.19314867e-02 -3.11734045e-03\n",
      "  1.23850813e-02 -2.03047056e-04 -1.01784139e-03 -3.54288728e-02\n",
      " -1.67853650e-02  2.57076744e-02  7.44220889e-03  3.92414002e-02\n",
      "  2.13048046e-03 -2.82339839e-02 -1.32103180e-02  4.32736851e-02\n",
      "  1.90099162e-03  2.66155258e-02  8.99442407e-03  1.26670731e-03\n",
      "  1.84497417e-02 -2.06319982e-02  2.71509342e-02  5.44154281e-03\n",
      " -1.13607501e-02 -4.32113069e-03 -1.34627103e-02  1.45192214e-02\n",
      " -4.63837484e-02 -2.08858553e-02  1.24946599e-03  2.65824893e-03\n",
      " -3.08143006e-02 -7.76189831e-03 -3.50956944e-02 -3.17440709e-02\n",
      " -9.54752092e-04  5.07461790e-03 -3.10335002e-02  1.20226518e-02\n",
      "  1.36980475e-02 -3.19620043e-02 -9.40330388e-03  1.30320532e-02\n",
      "  1.05960467e-02 -1.81031912e-02  2.25096183e-02 -2.43256838e-02\n",
      " -3.69267552e-02 -1.89124244e-02 -2.05701073e-02 -5.24853995e-03\n",
      "  1.15725105e-02  3.37586466e-03 -3.77487544e-02  1.31341286e-02\n",
      " -1.38769777e-02 -5.99168930e-03  2.01993641e-02 -3.77996667e-02\n",
      " -4.97698561e-03 -5.89803184e-03  4.08279318e-04  5.76648019e-03\n",
      "  5.21236796e-03  5.40763850e-03 -6.84251834e-03 -1.63123400e-02\n",
      " -2.04713743e-04  1.02908223e-02 -9.63023453e-03  4.34720766e-02\n",
      "  3.59783645e-02  4.76771349e-02 -5.80728292e-03  3.88973714e-02\n",
      "  4.37459822e-02  3.51813765e-02 -3.03486751e-02  9.95518004e-03\n",
      "  1.06336979e-02  1.35472579e-02 -2.37692367e-02  7.65511447e-03\n",
      " -8.67632974e-03  4.78171093e-03 -1.16758456e-02 -1.81935229e-02\n",
      " -1.50069062e-02 -9.73140125e-03 -2.30634522e-02  3.57938684e-03\n",
      " -2.36470997e-02 -2.10210523e-02 -1.71571012e-02  1.03395889e-02\n",
      " -3.35283844e-02 -7.22739507e-03 -1.94667312e-02  2.93008203e-02\n",
      "  6.03918188e-02  3.67011255e-02  5.64467634e-03 -3.58363759e-02\n",
      " -4.64834811e-02  1.15788985e-02  2.07742105e-02  1.59161265e-03\n",
      "  4.83893710e-02 -6.73656433e-03 -2.04915727e-02  2.22309593e-03\n",
      "  2.72971005e-02  3.65276256e-02  1.76364220e-02 -4.74973362e-02\n",
      " -3.08331559e-02  6.00418228e-03 -1.96798684e-02  1.02304416e-03\n",
      " -2.54793101e-02  2.75488461e-02 -3.12737371e-03 -1.45486716e-02\n",
      "  1.68012124e-02  7.16051924e-04  2.81483266e-02 -2.80486349e-02\n",
      " -2.21402954e-02  3.14878826e-03  8.21396469e-03  2.90563196e-02\n",
      "  1.76403322e-02  1.13992282e-02  4.01686142e-02  2.42400209e-02\n",
      " -1.14891195e-02 -1.43215486e-02  6.08981053e-04 -1.30437860e-02\n",
      "  5.65415916e-02 -3.70077833e-02  6.49264431e-03 -3.80461286e-02\n",
      "  2.66092983e-02 -4.17388154e-02  1.34591267e-02 -1.51263795e-02\n",
      " -7.22909735e-03 -9.35560945e-03 -3.41454793e-04  4.27754617e-03\n",
      "  2.41889342e-03  9.35255311e-03 -4.93948620e-02  1.85179607e-02\n",
      " -1.94148045e-02 -2.22413981e-02 -3.04630710e-02 -3.06099816e-02\n",
      " -9.67295487e-03 -2.47441672e-02  1.91416465e-02  2.60397246e-02\n",
      " -1.76449892e-02  1.32007758e-02 -3.33435777e-02 -3.13379079e-03\n",
      " -5.74315439e-03  1.38469552e-02 -3.14447286e-02  1.91930199e-03\n",
      "  1.43471618e-03  6.02136663e-03 -3.88121293e-03 -8.63239958e-03\n",
      " -3.61798634e-02  3.27961932e-02 -2.59457610e-02 -3.89073374e-02\n",
      " -6.59701358e-02  4.55751993e-03 -3.66423257e-03 -2.04733805e-02\n",
      " -1.95440098e-02 -1.61035542e-02  2.35918100e-03 -9.44707464e-03\n",
      "  7.89018984e-03 -2.21220852e-02 -1.95111663e-02 -7.69501329e-06\n",
      "  6.99432437e-03  3.34276279e-02  1.70296532e-02 -1.20591424e-02\n",
      "  8.20780664e-03 -2.98630895e-02 -1.87533782e-02 -7.87250107e-03\n",
      " -1.44343300e-02 -1.81192751e-02 -1.91762031e-02  5.66139524e-02\n",
      " -1.24178666e-02  7.28606814e-03 -3.04520682e-03 -2.98342919e-02\n",
      "  3.05982230e-02  1.09197587e-02  5.01452097e-02 -1.55764862e-02\n",
      "  4.35458693e-02 -3.51471332e-02  5.92148236e-02 -2.50726414e-02\n",
      "  2.53604835e-02  1.27666396e-02  2.67742706e-02 -1.56425424e-02\n",
      "  1.11744068e-02 -5.27699259e-02  2.52666132e-02  3.43367339e-03\n",
      "  2.82650881e-03  2.33572360e-02  1.64232341e-02 -1.42402000e-02\n",
      " -1.03289676e-04 -1.35850047e-02  3.36150680e-03 -3.19339733e-02\n",
      " -3.13923905e-02 -3.22995238e-02  1.82664542e-02  2.33916717e-02\n",
      " -1.01750169e-02  6.76165858e-03 -4.31580267e-02 -1.83659227e-02\n",
      "  3.23419410e-03  4.85791900e-02 -1.36764193e-03 -1.65855634e-02\n",
      "  6.74379947e-03  4.34816934e-02  2.09948389e-02  2.46498950e-02\n",
      "  1.93164112e-02 -4.87521658e-02 -4.55820027e-02  2.15880506e-03\n",
      " -3.20767629e-02  3.13281660e-03  1.80120627e-02  2.60886405e-02\n",
      "  2.94044098e-02 -2.62772040e-02 -1.14255107e-02  2.89394965e-02\n",
      " -1.16688961e-02  2.49707674e-02  4.24485527e-02  3.37788829e-03\n",
      " -3.65307016e-03  2.27912901e-02  1.13589337e-02  1.03858678e-02\n",
      " -2.78462923e-02 -2.59541623e-02 -5.22379757e-04 -1.20036957e-02\n",
      " -6.09177054e-04  1.70233054e-03  8.18202557e-03 -4.69046664e-03\n",
      " -3.28657265e-02  1.37015386e-02  5.10167205e-02 -1.26839815e-03\n",
      "  1.78935130e-02 -1.46609750e-02 -2.73162318e-02 -3.18949172e-02\n",
      " -1.64649513e-02]\n",
      "413474\n",
      "training: 37 testing: 25\n",
      "correctsource    25\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    17\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.5        0.66666667 0.83333333 0.6        0.75\n",
      " 1.        ]\n",
      "Accuracy: 0.7027027027027027\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.75      0.84      0.79        25\n",
      "       missed       0.56      0.42      0.48        12\n",
      "\n",
      "    micro avg       0.70      0.70      0.70        37\n",
      "    macro avg       0.65      0.63      0.63        37\n",
      " weighted avg       0.69      0.70      0.69        37\n",
      "\n",
      "accuracy = 0.84\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.84      0.94      0.89        17\n",
      "       missed       0.83      0.62      0.71         8\n",
      "\n",
      "    micro avg       0.84      0.84      0.84        25\n",
      "    macro avg       0.84      0.78      0.80        25\n",
      " weighted avg       0.84      0.84      0.83        25\n",
      "\n",
      "(325,)\n",
      "[ 1.38431377e-02 -4.52474185e-02 -2.95234104e-02  4.14011688e-02\n",
      " -4.07667826e-02 -4.89550338e-02  1.02379487e-02  8.00819751e-02\n",
      " -3.83075546e-02  6.51074172e-03  2.81656375e-02  1.61092231e-03\n",
      " -2.32173003e-02  5.16793926e-03  2.00453890e-03 -1.17259716e-02\n",
      "  4.70295855e-02 -1.94382422e-02  6.36152207e-03 -3.73009967e-03\n",
      " -2.72096744e-02  3.81300445e-02 -4.14962150e-02 -4.42719446e-02\n",
      "  4.23034766e-03  2.36199853e-03 -3.97865069e-02 -2.61821572e-02\n",
      "  1.68207634e-02  1.69447922e-02  2.98999729e-02 -2.95149353e-02\n",
      "  1.22826868e-02  2.94628744e-02  1.35128074e-02  2.51054038e-02\n",
      " -4.54711503e-02  2.49755742e-02 -1.12282744e-01 -1.97283361e-02\n",
      "  1.87954306e-02  3.29564400e-02 -3.84154583e-02  2.83244904e-02\n",
      " -1.36377868e-02  4.02924432e-02  1.04231445e-02 -4.55258838e-03\n",
      "  1.94997662e-02 -5.43153780e-02 -6.91928559e-03  4.07395544e-02\n",
      "  1.11605568e-02 -6.67445951e-03  1.77856218e-02 -3.19264640e-02\n",
      "  3.60408312e-02 -3.59878572e-02  2.09123418e-02  3.94657935e-03\n",
      " -5.77678547e-03  2.05006236e-02 -4.91380077e-02 -1.17946457e-02\n",
      "  3.05159586e-02 -1.84886779e-02  1.20199215e-03  3.32972991e-02\n",
      " -1.64312905e-02 -2.92342947e-02 -1.96494309e-02 -1.46159953e-02\n",
      " -4.06589591e-02  5.66996195e-02 -2.91796565e-02 -3.99545109e-02\n",
      "  1.36435422e-02 -1.35619372e-02  8.58758073e-03  4.69534967e-02\n",
      " -6.33011762e-03  5.00161497e-02 -1.99032112e-02  4.28271567e-02\n",
      " -7.02884647e-02  1.24831808e-03 -1.68036378e-02  1.06983860e-02\n",
      "  1.49579685e-02  1.14202764e-02  1.95182546e-02  1.02163683e-02\n",
      " -1.13438543e-02 -1.39238888e-02  1.00534158e-02  5.25976704e-02\n",
      "  2.90725999e-02  6.56555912e-02  4.08125592e-02  7.67073924e-02\n",
      " -1.74919019e-04 -1.15455421e-02 -2.96213501e-02  5.75676285e-02\n",
      "  3.76920950e-02  3.51486261e-02  5.41872583e-03  7.91758319e-03\n",
      "  1.45533909e-02 -3.14018151e-03 -1.56108594e-03  1.54511273e-02\n",
      " -2.77415670e-02 -1.99840946e-02  3.32820196e-02 -7.69593797e-03\n",
      "  3.54114543e-02  8.25448289e-03 -1.65696013e-02 -4.92714314e-02\n",
      " -4.11463343e-02  5.51754244e-02 -1.94283910e-02  7.50940075e-02\n",
      "  1.11732575e-02 -3.68465602e-02  6.01917939e-03  8.35483595e-03\n",
      " -2.97102959e-03  4.90907079e-03 -4.61707775e-03  2.74818774e-05\n",
      "  7.46445746e-02 -2.91801061e-02  9.45274027e-03 -1.65059197e-03\n",
      " -4.48461197e-03  5.00428212e-02 -1.20720950e-02 -2.96056219e-02\n",
      " -3.33549228e-03 -3.07862389e-02  7.43531941e-04  2.22129855e-02\n",
      "  2.02721259e-02  2.16747962e-02  4.85325821e-02  2.51717513e-02\n",
      " -2.55609906e-02  2.27536171e-02  3.15978697e-02 -2.57990425e-02\n",
      "  2.75494338e-02 -3.19004724e-02  1.49906411e-02 -3.45355754e-02\n",
      "  5.54833145e-02 -2.60832267e-02  3.98420813e-03 -4.65226714e-02\n",
      " -5.62348232e-02 -2.94586884e-02 -2.66950118e-02  1.31152691e-02\n",
      " -3.23436311e-03  1.99540898e-02 -2.70173494e-02 -3.52714042e-02\n",
      "  3.72593134e-02 -6.79107806e-03 -3.87804944e-02 -2.14742182e-02\n",
      "  2.59415476e-02  8.23259624e-03  1.12232022e-02  2.94473927e-02\n",
      "  2.72164355e-02  5.12548306e-03 -4.69663205e-02  1.33586001e-02\n",
      " -3.34655544e-02 -1.95727563e-02 -1.23247767e-02  6.00100395e-02\n",
      " -1.16929655e-02  2.79756005e-03 -4.99689273e-03 -7.92938103e-03\n",
      " -4.75130461e-03  2.45678771e-02  2.53360843e-02 -2.82097849e-02\n",
      " -5.24290688e-03  3.81671573e-02  6.33862284e-02 -2.27373076e-03\n",
      "  2.19452337e-02  4.08340824e-02 -3.99686096e-02 -9.89872789e-03\n",
      "  1.45553981e-02  2.30099358e-02 -2.40095472e-02  1.62976126e-02\n",
      " -1.76511355e-02 -6.14708193e-02  5.15033168e-02  4.21786733e-02\n",
      " -1.25893977e-02 -3.59342539e-02 -2.26880345e-02 -8.40617587e-03\n",
      "  4.59021697e-03 -1.50393235e-02 -4.42085856e-03  4.46603451e-02\n",
      " -1.99361285e-02  1.23029529e-02 -3.92944433e-02  3.62428167e-03\n",
      " -1.20444150e-02 -9.02623928e-04 -7.41839590e-02 -1.81231866e-02\n",
      " -4.32146743e-02  1.28439082e-02  2.67857886e-02  3.53240308e-03\n",
      "  1.02349162e-02 -2.48361009e-02 -3.74907023e-02 -6.23668662e-02\n",
      "  7.73536762e-03  5.99625907e-02  4.96947933e-03 -5.87656969e-03\n",
      "  3.23838976e-02  4.56135879e-02 -2.97497205e-02  2.24632790e-02\n",
      "  9.06891926e-03 -1.22600739e-02 -5.46065106e-03 -9.46439308e-02\n",
      " -1.11715398e-02 -1.45151614e-02 -1.55251857e-02  1.32515381e-02\n",
      " -1.57224212e-02 -1.54210473e-02 -2.79772656e-02  2.55380699e-02\n",
      "  1.18891702e-02  3.15656030e-02  2.59982163e-02  4.54162281e-02\n",
      " -1.84648813e-02 -1.91548213e-03 -4.57107151e-02  3.27536704e-02\n",
      "  5.86683689e-02 -1.02532285e-02  1.60589956e-02 -1.91019360e-02\n",
      " -3.79428599e-02 -2.40298801e-02 -1.29694431e-02  3.55895999e-02\n",
      " -3.60950018e-02 -4.75594137e-02 -2.42890455e-02  7.38887131e-03\n",
      " -8.70838226e-03  2.92413919e-02  1.92313629e-02 -3.43914360e-02\n",
      "  1.91404668e-02  1.86376720e-02  4.81224379e-03 -1.46962024e-02\n",
      "  2.01772102e-02 -2.24948516e-02  2.47565132e-02  2.55773378e-02\n",
      "  9.92731537e-03  2.82515865e-02  4.57483473e-02  4.95714945e-03\n",
      " -4.20555467e-02 -4.23961379e-02  1.11985826e-03  4.86892202e-02\n",
      " -1.18490160e-03 -4.23769477e-02  6.62171838e-03  2.67560632e-02\n",
      " -3.72486032e-02 -4.49932895e-02 -1.74454399e-02 -3.48361400e-02\n",
      " -1.20403081e-02  2.18938318e-03 -4.81897892e-02  4.41685606e-02\n",
      " -2.35936603e-03 -1.42538143e-02  3.44641690e-02  2.99445179e-02\n",
      " -3.05036794e-02 -1.29519987e-02 -4.54623485e-02  1.29211110e-02\n",
      "  4.51993191e-02 -5.03572014e-03  7.68843948e-03  4.51500960e-02\n",
      " -3.80253438e-03  1.38685088e-03  3.87504965e-02 -2.64580318e-02\n",
      "  3.64103724e-02 -1.77717267e-03  2.91863886e-02 -2.44048606e-02\n",
      "  2.90114994e-02]\n",
      "437101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 testing: 25\n",
      "correctsource    22\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5 0.6 0.6 0.6 0.8 0.6 0. ]\n",
      "Accuracy: 0.5277777777777778\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.59      0.60        22\n",
      "       missed       0.40      0.43      0.41        14\n",
      "\n",
      "    micro avg       0.53      0.53      0.53        36\n",
      "    macro avg       0.51      0.51      0.51        36\n",
      " weighted avg       0.53      0.53      0.53        36\n",
      "\n",
      "accuracy = 0.52\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.64      0.56      0.60        16\n",
      "       missed       0.36      0.44      0.40         9\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        25\n",
      "    macro avg       0.50      0.50      0.50        25\n",
      " weighted avg       0.54      0.52      0.53        25\n",
      "\n",
      "(325,)\n",
      "[ 1.88905970e-02  1.00224271e-02 -2.64014509e-02 -4.59908209e-03\n",
      " -2.41485801e-02  4.11380061e-02 -8.35510432e-02  2.38340479e-02\n",
      " -2.82004863e-02 -1.04975945e-02 -6.26139725e-04  4.92317306e-02\n",
      "  2.32409608e-02  4.49344606e-02  3.06435229e-02  2.65856225e-02\n",
      "  5.48480796e-03 -2.72483262e-02 -4.20795335e-02  1.11472477e-02\n",
      "  2.18021819e-03  4.96121003e-02 -8.02634810e-03 -4.48594340e-02\n",
      " -2.92608137e-02  5.26828479e-02  1.97645166e-02 -3.79029668e-02\n",
      " -3.97953430e-02  2.43835906e-02  1.03113237e-02  6.94346476e-02\n",
      " -2.28191750e-02 -2.68398826e-03 -2.04466177e-02  2.30076034e-02\n",
      " -2.83024234e-02 -3.03697340e-03 -5.16661442e-02  1.59531795e-02\n",
      "  2.56909153e-02  1.44112889e-02  2.00834825e-02 -1.36574149e-03\n",
      "  3.21617641e-02  2.35998139e-02  8.22887931e-02 -2.09371154e-03\n",
      " -1.14775885e-02  1.34748003e-02  4.38288868e-02  1.33953667e-02\n",
      "  5.24372953e-03  1.76591422e-02 -2.23657560e-02 -5.40662623e-02\n",
      "  2.06141646e-03  4.16287854e-02  2.00856906e-03 -2.09207879e-02\n",
      "  3.41838765e-03 -1.47238092e-02  8.33815109e-02  1.68263945e-03\n",
      " -5.54315491e-03  1.42591987e-02 -5.53557032e-02 -4.56442465e-03\n",
      "  4.13344217e-02 -1.40603226e-02 -3.55948312e-02  2.99221201e-02\n",
      "  1.33658009e-02  3.56824577e-02 -4.82109032e-03 -1.04157190e-02\n",
      "  6.67270959e-04  4.31125586e-04  1.34128991e-03 -2.34706669e-02\n",
      " -2.60158444e-02  4.33208569e-03  4.55474561e-02  1.98839290e-02\n",
      " -8.67008484e-03  3.07243854e-02  4.11023316e-03  3.09124423e-02\n",
      " -4.75440898e-03  9.81174611e-03 -6.17840325e-02 -6.43393391e-03\n",
      " -9.93079225e-02  5.03890378e-03  9.15241939e-05 -6.19359790e-02\n",
      " -4.13934137e-03  3.01952344e-02 -8.66808805e-03 -1.58445210e-02\n",
      " -4.92645933e-03 -2.20341188e-02  1.78946375e-02  1.89729254e-02\n",
      " -1.40521884e-02  1.41086522e-02 -8.58723671e-02 -1.01295225e-02\n",
      "  3.66632495e-02  1.11993946e-02  3.72860996e-02 -3.50406324e-03\n",
      " -1.08376102e-02  5.66353013e-02  2.50525890e-02  9.64744313e-03\n",
      " -8.23967156e-03 -5.18741046e-02  4.23909884e-02  1.70785450e-02\n",
      " -8.67234138e-03  7.30873859e-03 -1.13852720e-02  6.22360705e-03\n",
      "  2.21425872e-02  1.55871282e-02 -1.84269594e-02  5.24819160e-02\n",
      " -4.65766114e-02 -4.61334260e-03 -2.89447209e-02  3.78564583e-02\n",
      "  5.14026757e-02 -1.61929560e-02  2.10910796e-02  2.59408160e-02\n",
      "  2.24118893e-02  3.38860079e-02  1.13746822e-02  3.74434862e-02\n",
      " -6.23988051e-03  6.74794358e-02 -6.66892403e-03  1.67597293e-02\n",
      " -4.26008018e-02  1.15694114e-03  3.80995047e-02 -2.37072555e-02\n",
      "  2.97836640e-02  3.96573022e-02 -6.42134541e-02 -1.28840108e-01\n",
      "  6.71919419e-02  2.58033019e-02  2.84876024e-02  4.84478967e-02\n",
      "  1.15081641e-02 -7.57839758e-03 -7.60777340e-02  1.21439168e-02\n",
      " -1.02260985e-01 -1.62259173e-02 -1.62326902e-02  5.05283154e-02\n",
      "  2.85652965e-02 -2.73830265e-02  2.50159936e-02  4.04785059e-03\n",
      " -2.00800257e-02 -3.31705415e-02 -2.40433931e-03 -2.07430998e-02\n",
      "  6.29431584e-03  3.58561358e-02 -1.29140860e-02  2.66887618e-02\n",
      "  5.00020079e-03 -5.49271033e-05  3.80732677e-03 -1.36779809e-02\n",
      "  7.77016671e-02  1.78474151e-02  3.88742088e-02  8.96980027e-02\n",
      "  1.00611731e-02  1.30804102e-02  2.50183963e-02 -1.00420234e-02\n",
      "  3.42045131e-02 -1.12148948e-02  4.80645954e-02  1.99802784e-02\n",
      " -1.30731751e-02  5.13654743e-03  4.90511829e-03  6.60502137e-02\n",
      " -5.44686350e-04 -4.21616987e-03 -2.47463341e-02  5.57394137e-02\n",
      "  5.22199203e-02  5.94873127e-03 -4.02871666e-02  4.33706382e-03\n",
      " -3.02095023e-02 -2.28394515e-02 -7.99909075e-03 -3.23158228e-02\n",
      " -6.91243951e-02 -7.68839827e-03  4.37538607e-02 -2.28902846e-02\n",
      "  1.59503810e-02 -8.20835700e-02 -6.25424071e-02  1.98297676e-02\n",
      " -9.15996968e-03  3.72104840e-02 -4.40390620e-02  4.65102797e-02\n",
      " -1.12788115e-03  3.16239863e-02 -4.68542889e-02  2.75449595e-02\n",
      " -5.27795441e-02  4.25050593e-02 -2.07448086e-02  4.82588297e-02\n",
      "  2.73503775e-02 -1.55470292e-02  1.96165199e-02  1.88006884e-02\n",
      "  1.31936348e-02 -2.52614415e-02  5.84788437e-02  1.27638842e-02\n",
      "  2.21209280e-02 -1.67129203e-02 -4.30733014e-02 -5.44745487e-03\n",
      " -1.02567174e-02  6.72924950e-03  9.07554240e-03  3.07350036e-02\n",
      "  4.33487080e-02  4.22895324e-02  5.19292081e-03  1.16591288e-02\n",
      "  3.29378042e-02  3.15559468e-02  1.07643456e-02  4.04808615e-03\n",
      " -6.06823076e-02 -4.36040391e-02 -1.32051588e-02 -2.03441380e-02\n",
      " -8.41760424e-04 -1.77020735e-02 -6.46120217e-02  3.57061201e-03\n",
      " -1.12128714e-02 -2.86355351e-02 -2.88238315e-02 -3.05235581e-02\n",
      "  4.38356893e-02  2.71598959e-02  7.18758913e-03 -3.81399036e-02\n",
      "  1.27730653e-02  1.36026817e-02  1.81847275e-02 -3.55939701e-03\n",
      " -7.79081764e-04  1.90509364e-02 -7.04230390e-02 -4.43211315e-02\n",
      "  3.96323985e-02  2.78518524e-02 -2.46793996e-02 -3.26663940e-02\n",
      " -3.74773401e-02 -2.76320411e-02  1.30007718e-02 -3.22914478e-02\n",
      " -4.59887918e-02 -2.75111089e-02 -5.78769078e-02 -1.10893153e-02\n",
      " -4.57564553e-02  1.01540592e-02  1.20927383e-02  1.61178525e-02\n",
      "  1.18056594e-02 -5.92077135e-04  7.75283741e-02 -7.65874924e-03\n",
      "  1.39518019e-03 -2.26362636e-02 -3.69523543e-02  8.30803624e-03\n",
      " -3.76086632e-02  4.00959267e-02  8.40390812e-04 -1.31622570e-02\n",
      " -3.93771205e-03 -3.58762033e-02 -1.92507847e-02 -3.98993372e-02\n",
      "  1.32595016e-03 -2.70401203e-02 -5.86895674e-02 -4.25720205e-02\n",
      " -9.92363005e-03  4.25563312e-02  3.37786166e-02  1.72390874e-02\n",
      "  4.19804107e-02 -2.37596696e-02 -8.43583999e-02  3.45538092e-02\n",
      "  6.10610037e-02  4.59425964e-02 -1.69631860e-03  3.67644213e-02\n",
      " -4.15656485e-02]\n",
      "439776\n",
      "training: 44 testing: 30\n",
      "correctsource    33\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    23\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.42857143 0.57142857 0.42857143 0.42857143 0.5        0.6\n",
      " 0.6       ]\n",
      "Accuracy: 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.69      0.61      0.65        33\n",
      "       missed       0.13      0.18      0.15        11\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        44\n",
      "    macro avg       0.41      0.39      0.40        44\n",
      " weighted avg       0.55      0.50      0.52        44\n",
      "\n",
      "accuracy = 0.6666666666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.84      0.70      0.76        23\n",
      "       missed       0.36      0.57      0.44         7\n",
      "\n",
      "    micro avg       0.67      0.67      0.67        30\n",
      "    macro avg       0.60      0.63      0.60        30\n",
      " weighted avg       0.73      0.67      0.69        30\n",
      "\n",
      "(325,)\n",
      "[ 0.02578186  0.04716591 -0.01662487  0.01172078 -0.08285493 -0.08941646\n",
      "  0.04295547  0.0578433  -0.04510772  0.06914506  0.0401235  -0.03429548\n",
      "  0.03676625 -0.07877446 -0.01928199  0.03401533 -0.01582742  0.02509353\n",
      " -0.0299103   0.00763504  0.01407666 -0.02470323  0.00852308  0.00611463\n",
      "  0.00286778 -0.05593442  0.08591274 -0.02377368 -0.06963888  0.07168571\n",
      "  0.07493103  0.01137943 -0.0761906   0.0187605   0.01802197 -0.01227618\n",
      "  0.03998689 -0.01121376 -0.03655721  0.01374128 -0.0231633  -0.0324012\n",
      "  0.07775242 -0.00205208  0.06150778  0.02971497 -0.06595329 -0.01468\n",
      " -0.03098001 -0.04365041 -0.00256001 -0.03590752  0.06026085  0.00528004\n",
      " -0.0053787   0.02255501  0.08407903  0.03773154 -0.11301465 -0.02701897\n",
      " -0.05444804  0.02649324 -0.06111183 -0.02148239  0.0510184  -0.02540621\n",
      "  0.03956601  0.0125684  -0.03635229  0.01790098 -0.04241694 -0.00676795\n",
      " -0.00605451  0.01459231  0.01633199 -0.01759347 -0.0747064  -0.01494298\n",
      " -0.01588091 -0.01804186  0.03318413  0.01597074  0.02993373 -0.06434122\n",
      "  0.05491528 -0.08758757 -0.01049293 -0.05539951 -0.01786405  0.00137463\n",
      "  0.04890166 -0.02011916 -0.00361773 -0.06321242 -0.04210998  0.06078882\n",
      "  0.00101508  0.00972523  0.01680651  0.0001915  -0.01758883 -0.04918447\n",
      " -0.00738686 -0.01188426 -0.05360336 -0.0157219  -0.00430883 -0.00226235\n",
      " -0.01471202 -0.00664443  0.03712248 -0.00692847 -0.03803677  0.03843721\n",
      " -0.01056742 -0.01550264  0.02164672 -0.07035058  0.02951237  0.05240443\n",
      "  0.00278635  0.00420993 -0.06472851 -0.00662573 -0.03860375  0.02163376\n",
      "  0.01777962 -0.00637167  0.01857002 -0.00964555 -0.02038934 -0.01990395\n",
      "  0.0107825   0.019319    0.0152564   0.05084814 -0.01766125  0.00521746\n",
      "  0.00014543  0.02485159  0.00802166 -0.03127115 -0.0210023  -0.03410546\n",
      " -0.01088809 -0.05443832 -0.0148776   0.01996159 -0.0043744   0.00628817\n",
      "  0.01790677 -0.01638047 -0.00619924  0.0151813   0.00207049 -0.12825353\n",
      "  0.07527099  0.01696961  0.03041259  0.04956792  0.01376546  0.03659208\n",
      "  0.06249783 -0.01056188  0.01450499 -0.00771165 -0.01463678  0.00727981\n",
      " -0.0071298   0.01734195  0.03453051  0.06394505  0.05772773 -0.0258843\n",
      " -0.02508729  0.07234652  0.11446557  0.02114803  0.02534742  0.02034787\n",
      "  0.0325827  -0.04289162  0.00590022  0.03646173  0.02978871  0.02311667\n",
      "  0.02444081 -0.04277888 -0.0569664  -0.0220276  -0.03223601 -0.00331339\n",
      "  0.09103572 -0.03845033 -0.01105123 -0.01486695 -0.00999965  0.03481049\n",
      "  0.04584486 -0.01885745  0.01850932 -0.00639611  0.02893565  0.01964108\n",
      "  0.00793323 -0.01147017 -0.04902742  0.02137959  0.01029156 -0.00835487\n",
      " -0.05847295 -0.01203739  0.00730239  0.03448942  0.04048606  0.07552138\n",
      "  0.01800425 -0.03059196  0.02692454  0.02000703 -0.01098233 -0.04256662\n",
      " -0.02880842  0.00929499  0.0973745   0.00571749 -0.01292199  0.0103963\n",
      "  0.03177211  0.0403348  -0.01459496 -0.11009466  0.03735013  0.00945078\n",
      " -0.03799768 -0.02971959  0.04139438 -0.05949304  0.02370051 -0.00663381\n",
      "  0.06911438 -0.09203864 -0.05507762  0.08153863 -0.07763716  0.02800767\n",
      "  0.03422501  0.019188   -0.01797975 -0.11265726  0.03717797  0.02991249\n",
      " -0.02724424  0.07594387  0.03707092  0.02028829  0.03647648  0.01445251\n",
      " -0.0273448  -0.01609449 -0.07857457  0.09136236 -0.00546543 -0.00774723\n",
      "  0.00596414 -0.05122441 -0.00536013  0.06287013  0.01249097 -0.07631635\n",
      "  0.00759069  0.04851346 -0.02495804 -0.00633587  0.08070971 -0.03989729\n",
      "  0.12765892  0.00218033  0.01754169 -0.11138535 -0.02483574  0.02085446\n",
      "  0.03183773  0.06849569 -0.00890235 -0.02422837 -0.01477119 -0.01811378\n",
      "  0.01857824 -0.04827923 -0.0366757   0.04602538 -0.02662218  0.00648578\n",
      " -0.06148144  0.04370751  0.03795907 -0.01060768 -0.00433644  0.02161969\n",
      "  0.06066227 -0.07628659  0.02788613  0.01474381 -0.01056757 -0.00719191\n",
      "  0.02534732 -0.0258664   0.04739915 -0.01096915 -0.01313662  0.06836453\n",
      " -0.05048964  0.00953919 -0.01269034  0.06142476 -0.04816027 -0.00882363\n",
      "  0.00789502 -0.03553737  0.03511979  0.01688225 -0.02862216  0.00887812\n",
      "  0.03918728]\n",
      "458807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 testing: 24\n",
      "correctsource    21\n",
      "missed           15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.6        0.6        0.8        0.6        0.6\n",
      " 0.4       ]\n",
      "Accuracy: 0.5555555555555556\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.71      0.65        21\n",
      "       missed       0.45      0.33      0.38        15\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        36\n",
      "    macro avg       0.53      0.52      0.52        36\n",
      " weighted avg       0.54      0.56      0.54        36\n",
      "\n",
      "accuracy = 0.5833333333333334\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.61      0.79      0.69        14\n",
      "       missed       0.50      0.30      0.37        10\n",
      "\n",
      "    micro avg       0.58      0.58      0.58        24\n",
      "    macro avg       0.56      0.54      0.53        24\n",
      " weighted avg       0.56      0.58      0.56        24\n",
      "\n",
      "(325,)\n",
      "[-3.40633060e-04  3.62744226e-02  2.50107169e-02  7.90625725e-03\n",
      "  1.35519495e-02 -5.75475845e-02 -2.53453635e-02  1.62220744e-02\n",
      " -2.68193056e-02  2.04268613e-02  3.94352175e-02  3.63693019e-02\n",
      " -4.32243559e-02  1.16193907e-02  1.95668005e-02  3.91415805e-02\n",
      "  6.40126553e-02  3.02132948e-02  1.77154533e-02  3.45275321e-02\n",
      " -3.97103564e-02  2.18508771e-02 -1.18253018e-02 -4.27155396e-02\n",
      " -2.85902101e-03  7.22347803e-03 -5.72490490e-03 -2.16904954e-03\n",
      " -5.99668050e-03 -3.49828121e-02  1.88579939e-02 -4.37055351e-03\n",
      " -2.47868560e-02 -9.65265977e-04  1.77075207e-03 -4.85339809e-04\n",
      " -2.16683832e-02 -2.70430185e-02 -1.08664319e-02  3.46910596e-04\n",
      " -1.24961877e-02  1.60064683e-02  2.96277397e-02  2.60227317e-02\n",
      " -1.18179852e-02 -2.76430643e-02  3.00214920e-02  3.25570412e-02\n",
      " -2.27287930e-03 -4.60097452e-02 -6.65848102e-03 -1.91725093e-02\n",
      " -3.04579724e-02 -2.55107944e-02 -3.07555939e-02 -4.09948117e-02\n",
      "  7.32904752e-03 -1.74307252e-02  1.55814686e-02 -8.63522252e-03\n",
      "  1.39619861e-02 -6.23731707e-02  1.54732664e-02  2.34318823e-02\n",
      "  2.13182893e-02  8.55188281e-03 -2.85483057e-02 -3.57988136e-02\n",
      "  3.03767686e-03 -2.00883880e-02  1.00750542e-02  2.79736372e-02\n",
      "  5.71744512e-03  1.89712101e-02  1.69772138e-02 -2.72207911e-02\n",
      "  2.31539986e-02 -1.99981519e-03 -1.31388308e-02  1.55730756e-02\n",
      " -1.35687251e-02 -2.86295083e-02 -2.49400661e-02 -4.57948000e-02\n",
      "  2.46953559e-02  4.66654251e-03 -3.23831552e-02  4.23101787e-02\n",
      "  2.83330526e-02  6.77928840e-02  4.45107223e-02  1.38786471e-02\n",
      "  9.31586178e-03 -2.47065664e-02 -2.63017337e-02 -1.17027728e-02\n",
      "  3.41051022e-02  4.33944183e-03 -5.63547405e-03 -2.64398114e-02\n",
      "  5.21545294e-02 -8.21880532e-03 -2.94511176e-02  3.26861776e-02\n",
      "  2.25921980e-02  1.60397087e-03  2.51784714e-02  1.94878016e-02\n",
      "  5.03837727e-03 -1.18797259e-02 -2.70546972e-02  7.66783003e-03\n",
      " -3.85069981e-02 -3.66570833e-03 -3.32145787e-02 -6.78200402e-03\n",
      "  2.50141584e-02 -1.19379827e-02  3.13832209e-02 -5.61588222e-02\n",
      "  4.53356076e-03  3.94685703e-02 -2.23885166e-02 -9.75283362e-06\n",
      " -1.00870192e-02 -5.36459178e-02 -5.34013165e-02  2.10865148e-02\n",
      " -1.67630776e-02 -3.53300997e-02 -2.56585689e-02 -3.90142192e-02\n",
      "  1.06584925e-02 -1.21680037e-02  8.34326323e-04 -1.81482407e-02\n",
      "  3.06049363e-02 -6.85722054e-03 -6.57746168e-02  5.34546424e-02\n",
      " -1.90098691e-02  4.62557183e-03  3.60463574e-02  3.36797835e-02\n",
      " -2.16124573e-02  6.07140327e-02 -4.37251333e-02 -1.16931004e-02\n",
      " -3.93346486e-03  6.52680735e-02 -2.03369117e-02 -3.04422388e-02\n",
      "  4.09025372e-03 -5.37571554e-03 -3.73691199e-03 -4.64777829e-04\n",
      "  8.53908766e-03  1.10947631e-02  1.50308017e-02  2.54706421e-02\n",
      " -1.26632957e-02  3.19679542e-02 -1.27091675e-02  5.40645026e-02\n",
      " -2.53346753e-02 -1.06814783e-02  3.58254854e-03 -1.51851203e-02\n",
      " -9.88331947e-03 -4.16480164e-02  4.23766383e-04 -6.34690523e-03\n",
      "  4.54181519e-02  1.09626946e-02 -1.66978827e-02 -1.82654268e-03\n",
      " -1.81813679e-02 -1.24781613e-03 -1.49218376e-02 -8.44382219e-03\n",
      "  7.07260632e-03 -3.29320743e-02 -3.18612836e-02 -1.56993870e-02\n",
      " -1.89302397e-02 -3.21909752e-02  1.72378924e-02  8.04470069e-03\n",
      " -2.67456851e-02  2.22228903e-02  3.13424168e-02  1.93499032e-02\n",
      "  5.43150040e-02 -1.37663511e-02  1.63806952e-02  4.99986584e-02\n",
      " -2.82761250e-03 -9.02986386e-03  6.38826514e-03 -1.27473201e-03\n",
      "  2.35952184e-02 -1.33466308e-02 -5.69205577e-02 -1.42606532e-02\n",
      " -5.17376383e-02 -2.55429474e-02 -2.05135063e-02  1.63073505e-02\n",
      " -2.33478267e-02  1.71275811e-02 -1.40555829e-03 -5.42283692e-02\n",
      "  1.91937352e-02 -4.13658585e-02  7.73648808e-03  1.04508528e-02\n",
      "  8.91674717e-03  1.67961666e-02 -2.53981473e-02 -1.39876612e-02\n",
      "  1.35956903e-02 -3.40360290e-02 -1.33213906e-02  1.62032874e-02\n",
      " -1.88380261e-02  4.77863877e-02  8.76851609e-02 -1.84882881e-02\n",
      " -1.11254107e-02 -1.76179513e-02 -1.59627880e-02 -1.45344306e-02\n",
      "  3.08508738e-03 -4.60646791e-02  5.44255770e-02  1.67426229e-02\n",
      "  5.66513108e-03  1.42442466e-02 -2.51428967e-02 -1.44326367e-02\n",
      "  2.86673602e-02  2.98477142e-02 -3.22253487e-02  1.05569519e-02\n",
      "  3.49317285e-02 -1.37812131e-02 -9.35658561e-03 -9.55262541e-03\n",
      "  1.87392515e-02  4.34809322e-02 -4.15641605e-02  1.02724276e-02\n",
      "  1.61338881e-03 -2.99959537e-02 -6.68270847e-03  7.07920141e-03\n",
      " -4.63429724e-02  1.44037043e-02 -4.81718154e-03 -2.23695688e-02\n",
      " -9.79458943e-04 -2.40118814e-03  8.39816335e-03 -1.80316184e-02\n",
      " -1.92321069e-02  1.74633521e-02  7.59957287e-03 -1.24774540e-04\n",
      " -2.46987250e-02  2.17278958e-02 -3.80588038e-02 -3.59113722e-02\n",
      " -3.10418331e-02  4.41925337e-02 -2.51498774e-02 -5.95568472e-02\n",
      "  3.93119754e-02 -1.34532025e-03  3.65056853e-02  2.85367596e-02\n",
      " -1.40053131e-02  3.18314890e-02 -3.51860987e-03  2.09574168e-02\n",
      "  3.45011983e-02  1.43720252e-02 -1.66908418e-02  3.28036674e-03\n",
      "  5.04422259e-03  2.10685607e-03  1.97976277e-02 -2.47456863e-02\n",
      "  1.83709947e-02 -2.79544415e-02 -5.03900990e-02  2.09053339e-02\n",
      " -3.27131561e-02 -1.89074794e-02  9.29398776e-03 -1.80388212e-02\n",
      " -1.15667377e-02  2.02816645e-02 -6.41403455e-04  3.70980125e-02\n",
      " -5.16659017e-02  3.44375687e-03 -1.17039370e-02 -2.38560986e-02\n",
      "  1.26979679e-02 -4.05604327e-02 -1.20253599e-03  9.04188670e-03\n",
      "  4.48232599e-02 -4.75287926e-03 -1.33838350e-02 -3.70969385e-03\n",
      " -2.62657260e-02 -3.80831400e-02 -2.75998538e-02 -8.54220863e-04\n",
      "  3.11207021e-02  5.09731018e-02  2.50767898e-02 -2.66702713e-02\n",
      " -1.54629952e-02]\n",
      "459801\n",
      "training: 31 testing: 21\n",
      "correctsource    20\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    13\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.8        0.6        0.6        0.6        0.5        0.\n",
      " 0.66666667]\n",
      "Accuracy: 0.5483870967741935\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.64      0.70      0.67        20\n",
      "       missed       0.33      0.27      0.30        11\n",
      "\n",
      "    micro avg       0.55      0.55      0.55        31\n",
      "    macro avg       0.48      0.49      0.48        31\n",
      " weighted avg       0.53      0.55      0.54        31\n",
      "\n",
      "accuracy = 0.6666666666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.75      0.69      0.72        13\n",
      "       missed       0.56      0.62      0.59         8\n",
      "\n",
      "    micro avg       0.67      0.67      0.67        21\n",
      "    macro avg       0.65      0.66      0.65        21\n",
      " weighted avg       0.68      0.67      0.67        21\n",
      "\n",
      "(325,)\n",
      "[-0.02029229  0.03567654  0.02412136  0.01531188 -0.01177213 -0.01355724\n",
      "  0.02181022 -0.05517598 -0.01966363 -0.0113818  -0.01390255 -0.04354124\n",
      "  0.00989303  0.0024364   0.0103688  -0.01751445  0.03443746  0.01586653\n",
      " -0.05196414  0.01543553 -0.01466673  0.0264806   0.02878357 -0.04486098\n",
      " -0.0137226   0.00550165  0.02399714  0.02624582 -0.00812263  0.05218075\n",
      "  0.02231166  0.02277861 -0.00102249  0.03254025  0.00198133  0.01364166\n",
      " -0.01599467  0.04273908 -0.01972928  0.06592173  0.00351009 -0.01955728\n",
      " -0.01524717  0.02188389  0.0094461   0.01033383 -0.00994472  0.02596006\n",
      " -0.01980733 -0.02288537  0.07122979  0.01012526  0.02577595 -0.03750836\n",
      " -0.0153198   0.00850823  0.02640499 -0.01981303  0.01410216 -0.00377339\n",
      "  0.01488964 -0.00837559 -0.01862294  0.0026354  -0.00943301 -0.01717503\n",
      "  0.0149207  -0.00508    -0.02915386 -0.00401987  0.01150426 -0.0118762\n",
      " -0.00541914 -0.06206522 -0.02467546  0.00146175 -0.00355225 -0.0101251\n",
      "  0.00213713 -0.00130313  0.00934406  0.03779789 -0.01523132  0.07709304\n",
      "  0.00237128  0.03107238 -0.00294922  0.00694477  0.0520006  -0.01378907\n",
      "  0.01729999 -0.02418224 -0.00017944  0.00143382  0.01929475  0.01428389\n",
      " -0.05269045  0.02551556  0.02153391  0.0081895   0.0592152   0.00787648\n",
      " -0.00478914  0.01068399  0.03269243 -0.00757328 -0.02583643 -0.03535486\n",
      " -0.03845033  0.00938489 -0.006959    0.04624871  0.00993238 -0.01008724\n",
      " -0.03466479  0.06017294 -0.00310634  0.01083175 -0.00348941 -0.03669234\n",
      "  0.01376973  0.05397981 -0.03931694 -0.01050741 -0.01808791 -0.04971199\n",
      "  0.0239369  -0.02651682  0.00729914 -0.01341229  0.01003978 -0.04509188\n",
      "  0.02741776 -0.00776387  0.01159369  0.03216337  0.01092051 -0.00594933\n",
      "  0.04185046  0.00555643 -0.02482397  0.0148982  -0.03230782  0.02685713\n",
      "  0.01718541  0.00714097 -0.01458664  0.07160573  0.0299933   0.00466467\n",
      "  0.04459199 -0.00013864 -0.00024059  0.01063086  0.02354385  0.02952335\n",
      "  0.02902892 -0.00108253 -0.03993625 -0.01294322  0.02908961 -0.00632685\n",
      " -0.08298992 -0.04821201  0.01899166  0.00377981  0.01903605 -0.00878613\n",
      " -0.0041433  -0.00615179  0.02054369  0.04028381  0.02305315  0.04808367\n",
      "  0.02300122  0.0337502  -0.01115498 -0.019537   -0.00983621 -0.04038858\n",
      "  0.02851707  0.00586505  0.00495192  0.00132443 -0.00039391 -0.00178259\n",
      "  0.04547323 -0.01302602  0.02002069  0.02371236  0.03654396  0.0236684\n",
      "  0.04039649  0.00680581 -0.00172261 -0.01418977 -0.04257914 -0.00602609\n",
      " -0.0020947   0.06996889  0.0016052  -0.02150841  0.00173185  0.0035619\n",
      "  0.00867949 -0.06204905  0.01231934  0.00782647 -0.00752773  0.03878319\n",
      " -0.01161366 -0.00314908  0.03739483  0.03083619  0.0114418  -0.01752136\n",
      " -0.02264032 -0.01503088 -0.00192217 -0.00274495 -0.00370462 -0.01883561\n",
      "  0.05490559 -0.0044687   0.00733333 -0.02809883 -0.03111584  0.01860593\n",
      " -0.0195866  -0.00272886 -0.00264291  0.01847594 -0.00037107 -0.00366453\n",
      "  0.05690365 -0.00248805  0.02742046  0.02762084 -0.01751055 -0.03762715\n",
      "  0.02337528 -0.02078181 -0.03358263 -0.01192196  0.05389154 -0.01607143\n",
      " -0.01745894  0.02831771  0.02714622  0.02294464 -0.00985323  0.01562303\n",
      "  0.05974238  0.03172168  0.02187812  0.03634447  0.01503017 -0.02495532\n",
      "  0.01430556  0.09041351  0.03518311 -0.0186897   0.07396643  0.00205502\n",
      "  0.0650349   0.01989012 -0.00612679  0.02997336  0.05040805  0.01124882\n",
      "  0.03603439  0.02270813 -0.0106731   0.05247434  0.00756538  0.01220999\n",
      " -0.01385183  0.03872429 -0.0258074   0.02711122  0.0178218   0.00820058\n",
      " -0.03631132 -0.01837826 -0.01452841  0.02195878  0.0289367  -0.00937145\n",
      " -0.02064612 -0.00024423  0.0130983  -0.0304021   0.00598365 -0.02175737\n",
      " -0.01077163 -0.05590765  0.03566115 -0.02929221  0.03172629 -0.00196241\n",
      " -0.0029471   0.05233522 -0.00088139  0.04028323  0.01925226 -0.01435784\n",
      " -0.00744292 -0.0172034   0.00302675  0.00086666 -0.0540984   0.01268721\n",
      "  0.0529947   0.00664119  0.01178744 -0.00050503 -0.00036251 -0.00020104\n",
      " -0.04737386 -0.02080731  0.00909634  0.0469425  -0.04025794 -0.00586422\n",
      "  0.02002444]\n",
      "484204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 39 testing: 26\n",
      "correctsource    22\n",
      "missed           17\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    15\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.57142857 0.5        0.5        0.8        0.8        0.6\n",
      " 0.6       ]\n",
      "Accuracy: 0.6153846153846154\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.64      0.73      0.68        22\n",
      "       missed       0.57      0.47      0.52        17\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        39\n",
      "    macro avg       0.61      0.60      0.60        39\n",
      " weighted avg       0.61      0.62      0.61        39\n",
      "\n",
      "accuracy = 0.6153846153846154\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.61      0.93      0.74        15\n",
      "       missed       0.67      0.18      0.29        11\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        26\n",
      "    macro avg       0.64      0.56      0.51        26\n",
      " weighted avg       0.63      0.62      0.55        26\n",
      "\n",
      "(325,)\n",
      "[-8.11067946e-02  2.95915308e-02  2.36020227e-02  6.90197187e-02\n",
      " -8.75394800e-02 -2.59926616e-02  4.71071174e-02  3.29162925e-02\n",
      " -1.33827008e-04  5.34759609e-02  1.12624964e-01 -6.51736960e-02\n",
      "  3.18574640e-02 -4.55882735e-02 -8.74373182e-03  1.49526558e-02\n",
      "  2.90499401e-02 -3.56390711e-02 -1.81631697e-02 -1.79889092e-02\n",
      "  7.88050496e-02 -5.16019597e-05  5.19317835e-02 -4.00214568e-02\n",
      " -2.88906687e-02 -1.48021306e-03 -1.21122768e-02 -3.28449370e-02\n",
      "  1.52759231e-03  2.21988740e-02 -5.34815108e-02 -1.88412585e-02\n",
      "  1.86694240e-02  1.93702988e-02  4.52055360e-02  1.98652449e-02\n",
      " -3.62357290e-02  5.02776897e-02 -7.32510726e-02 -1.47967710e-02\n",
      "  1.30268928e-02  3.83715390e-02 -1.89741772e-02  5.24344599e-02\n",
      "  4.07075771e-02  3.13660218e-03 -5.16823442e-02 -2.29348177e-03\n",
      "  1.91605690e-02 -6.75561275e-02  1.80001994e-02  3.96932631e-02\n",
      " -7.82611168e-02 -6.92706646e-02 -5.58933421e-03 -1.17348969e-01\n",
      " -6.12171495e-02 -4.68803130e-02 -1.41139202e-02 -4.32248680e-02\n",
      " -4.43891498e-02  1.06224723e-03  4.00256409e-03 -4.80703839e-02\n",
      "  9.72712407e-03  5.74208941e-02  1.37224521e-02  7.51213293e-02\n",
      "  2.70250215e-02  3.94190771e-02 -4.62788315e-02  2.71096033e-02\n",
      " -1.86258838e-02 -4.94886551e-02  8.60345234e-02  2.15795288e-02\n",
      "  7.52889035e-02  9.01786017e-02 -2.56210323e-02  2.21345560e-02\n",
      "  6.17728113e-03  4.80406788e-02 -6.10921816e-02 -3.04907509e-02\n",
      "  9.52530704e-02  8.11418656e-02  1.27709869e-02 -1.77681803e-02\n",
      " -1.98061799e-02  1.58099583e-02  4.32668988e-02 -3.86966114e-02\n",
      " -3.63310113e-02  3.25553045e-02 -1.65479120e-02 -6.51079868e-02\n",
      " -7.60506053e-02  2.59192405e-02 -5.18135150e-02  2.26181639e-02\n",
      "  3.80360136e-02 -7.92714714e-02  1.69924062e-02 -1.12271506e-01\n",
      "  1.21119159e-01  4.97853697e-02 -7.03216127e-02 -1.90993462e-02\n",
      "  1.64118255e-03 -1.38548478e-02  6.39469138e-03  1.63555003e-02\n",
      " -6.78564230e-03 -1.80137184e-02  1.56927101e-02 -4.94620771e-02\n",
      " -6.39155186e-02  2.54995515e-02 -1.12826715e-02 -9.94982178e-03\n",
      "  1.81107605e-02 -3.59359994e-02 -3.27470580e-02 -5.10990410e-02\n",
      " -1.11471705e-02 -1.23882966e-02 -2.15589589e-02 -7.28309969e-02\n",
      "  1.12296377e-02 -5.62933193e-02  5.50102937e-02 -1.78530227e-02\n",
      " -6.31255095e-03  7.61322711e-03 -3.39561017e-03 -4.98851682e-02\n",
      "  1.05585775e-02  1.97096715e-02 -4.87004667e-02 -1.77054794e-02\n",
      " -6.67461841e-02  1.71955592e-02  1.30151081e-03  7.19516717e-03\n",
      "  9.82687499e-02 -1.29971403e-02  9.48065020e-02  4.17747317e-02\n",
      "  8.06417620e-03 -2.43456776e-02 -9.63476319e-02 -2.37621274e-02\n",
      " -6.76463120e-02  8.54650482e-03  3.83037994e-02  2.56492209e-02\n",
      " -8.45662649e-02  1.73831026e-02  8.08917239e-02 -6.90366853e-03\n",
      "  8.31444231e-03  1.00425217e-01  9.47755271e-04  3.89839584e-03\n",
      "  3.95280601e-02 -1.12076121e-02 -4.50423398e-02 -1.53420219e-02\n",
      " -6.19702396e-02  3.15723068e-02 -5.19154545e-03  1.21284014e-03\n",
      " -1.63920871e-02 -3.45169443e-02  7.82900294e-02  1.05977227e-02\n",
      "  1.06086709e-02 -1.08065862e-02  1.84628833e-02 -1.17481715e-02\n",
      " -2.23737506e-03 -6.76505398e-03  3.39757912e-02 -6.14155280e-02\n",
      " -1.11135178e-02  3.27950804e-02  1.64220748e-02 -8.50686948e-03\n",
      "  7.46115179e-02  4.26408171e-02 -2.47651417e-02  7.02305501e-04\n",
      "  4.15812676e-02 -2.30428302e-02  4.94874261e-02 -8.37574639e-02\n",
      "  2.94961192e-02  9.41849062e-03  1.00521069e-02  1.03364948e-01\n",
      " -3.55928044e-02 -7.99235186e-02  6.55871332e-02 -3.20975566e-02\n",
      "  3.03723642e-02 -1.34124448e-02  1.09098044e-02  2.27690839e-02\n",
      " -3.06946615e-03 -2.06388103e-02  1.86293290e-02  3.08450706e-02\n",
      "  1.45603332e-01  6.20843055e-03 -1.08040223e-01 -1.63100631e-02\n",
      " -6.09824955e-02  3.33793845e-03  2.03170507e-02  5.87938066e-03\n",
      " -1.44944074e-02  1.76088834e-02 -8.26761313e-02  1.40172394e-02\n",
      "  1.69142480e-02  1.41255044e-02 -2.75494765e-02 -5.25826635e-02\n",
      " -1.77462714e-02 -9.62149559e-03 -1.13070803e-02  5.10353103e-02\n",
      "  1.06040210e-02  9.42970905e-03  9.49671211e-02  1.34122825e-02\n",
      " -1.82049382e-02  1.33696693e-01 -5.30567136e-02 -5.46786986e-02\n",
      "  3.04709736e-02 -2.50255187e-03  2.60975898e-02  2.53816680e-02\n",
      "  8.73245956e-02 -7.99775824e-03 -4.74276579e-02  7.77735224e-02\n",
      "  8.57209904e-03 -9.13613990e-03 -1.14004528e-02  5.03294197e-02\n",
      " -1.08316723e-01 -3.38499028e-03  1.53325554e-02  1.33784144e-01\n",
      " -4.57574504e-03  5.38359041e-02  1.67477411e-02  7.01462162e-02\n",
      " -5.23310463e-02  4.68688585e-02  2.30268406e-02  8.32879264e-02\n",
      " -1.05818917e-02  2.59312269e-02  1.32521566e-02  8.83567435e-04\n",
      " -4.22361174e-02 -1.90780580e-02  7.15828710e-02 -1.12419406e-01\n",
      "  3.18160213e-02 -1.83384451e-02  2.48909783e-02 -1.78231676e-02\n",
      " -4.14238134e-02  4.52718983e-02 -2.36731065e-03 -4.22720176e-02\n",
      "  4.79587321e-02 -8.93682504e-03 -1.33160220e-02 -6.05027965e-02\n",
      " -4.97519893e-03  1.97517576e-02  9.79481890e-03  9.50232770e-02\n",
      " -9.52434361e-02  7.53777568e-03  1.27771728e-02  2.22584336e-02\n",
      "  3.25160615e-02  2.86618828e-03  4.20195405e-02  3.17839896e-02\n",
      " -4.59354624e-03 -2.21490705e-02 -4.88347121e-02 -5.15656149e-02\n",
      "  5.80329674e-02 -1.27187486e-02  2.38126002e-03 -4.26125886e-02\n",
      "  1.27523705e-02 -2.28591266e-03  2.78974714e-02 -3.48236115e-02\n",
      "  5.21354302e-02  2.03059057e-02 -1.17924837e-02  6.36091743e-03\n",
      "  4.44756286e-02  3.19871662e-02  3.45167612e-02 -3.99573813e-02\n",
      " -4.55041871e-02  6.18450094e-02 -4.98098469e-02  6.32011464e-02\n",
      " -3.39357566e-02  2.58810794e-02 -6.31146946e-02 -5.09464685e-02\n",
      " -2.29223063e-02]\n",
      "502616\n",
      "training: 31 testing: 21\n",
      "correctsource    21\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    15\n",
      "missed            6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.4  0.2  0.6  0.75 0.5  0.5  0.75]\n",
      "Accuracy: 0.5161290322580645\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.65      0.62      0.63        21\n",
      "       missed       0.27      0.30      0.29        10\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        31\n",
      "    macro avg       0.46      0.46      0.46        31\n",
      " weighted avg       0.53      0.52      0.52        31\n",
      "\n",
      "accuracy = 0.7142857142857143\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.76      0.87      0.81        15\n",
      "       missed       0.50      0.33      0.40         6\n",
      "\n",
      "    micro avg       0.71      0.71      0.71        21\n",
      "    macro avg       0.63      0.60      0.61        21\n",
      " weighted avg       0.69      0.71      0.69        21\n",
      "\n",
      "(325,)\n",
      "[ 9.41542747e-03  2.64213678e-02  3.06445944e-02 -8.43816825e-02\n",
      "  2.02795521e-02  5.36595407e-02  5.09149973e-02  2.15110447e-02\n",
      "  2.56960000e-02 -5.99103139e-03  1.49745980e-02 -1.54734969e-02\n",
      "  9.25777278e-03  2.23671368e-02  3.24388532e-02  3.00987295e-02\n",
      " -1.20002207e-02  1.50141559e-02  3.14540290e-02 -3.03109273e-02\n",
      " -2.29902638e-02  6.94422971e-04  6.55957123e-03  1.46230032e-02\n",
      "  5.28055919e-02 -1.53552030e-02  1.67438131e-02 -2.87752615e-02\n",
      "  7.05980634e-03 -3.95830690e-02  2.65678797e-02  2.67754318e-02\n",
      "  5.61938214e-03 -4.09271344e-02 -2.00143804e-03 -1.81301289e-02\n",
      "  8.88379871e-03 -4.51114956e-02 -8.97269249e-03  2.16417030e-02\n",
      " -6.55148324e-02  3.18637777e-02 -5.32120995e-04  3.72669308e-03\n",
      " -9.17212690e-03 -2.65043569e-02  1.37057548e-02 -8.51181595e-03\n",
      " -3.68918204e-02  4.83619988e-03 -5.84099312e-03  1.51402332e-02\n",
      " -4.85264969e-02 -1.44888105e-02  1.05505152e-02  6.25082706e-02\n",
      "  2.32442024e-03  1.49813564e-03  3.06786654e-02 -7.67872180e-02\n",
      " -6.87647105e-03 -8.89287377e-03  7.17051101e-03  1.98913150e-02\n",
      "  4.90660874e-03 -6.95858731e-03  2.76160403e-02 -4.47140102e-02\n",
      " -2.02748567e-02 -1.60023798e-02  1.50109165e-02 -7.89387275e-03\n",
      "  1.47998538e-02 -1.54927794e-02 -1.61109316e-02  2.65023615e-02\n",
      " -1.64809371e-03  1.90461714e-02 -3.14193302e-03 -1.28154490e-02\n",
      " -3.14152755e-03 -7.44778576e-03 -2.72466404e-02  1.92943216e-02\n",
      " -5.12647167e-02  2.67704506e-02  9.41568724e-03  9.07281902e-03\n",
      "  1.90016180e-02  5.14171402e-03  1.88916340e-02 -6.35831752e-02\n",
      "  3.06629218e-02 -1.77427727e-02 -9.12117700e-03  4.52185441e-05\n",
      " -7.18963025e-02 -5.08725552e-03 -4.73491766e-02  2.72435809e-02\n",
      "  5.03458683e-04 -3.16967479e-03 -3.99990918e-02  1.12787232e-02\n",
      " -2.28348363e-02  1.85235152e-02  3.56084395e-03  1.72423244e-02\n",
      " -3.55909081e-02 -2.58052112e-02  8.85343852e-03 -1.74678277e-02\n",
      "  2.51171542e-02 -1.18461522e-02 -2.99395586e-02 -2.68024451e-02\n",
      " -2.47521181e-02 -1.18920178e-02  2.25837620e-02 -3.15065555e-02\n",
      "  1.04907868e-02  2.75080035e-02  1.28335815e-02 -5.71239486e-02\n",
      "  8.40949692e-03  2.10611895e-02 -6.44172142e-03 -1.74578879e-02\n",
      "  3.31176928e-02  9.27871985e-03  3.55921580e-02  1.76384735e-02\n",
      "  6.03680420e-02  9.41174474e-03 -1.18888067e-02  6.66365867e-03\n",
      " -1.21115339e-02  4.13009229e-03  1.48086367e-03 -2.87588608e-02\n",
      "  1.39441429e-02  2.00342335e-02  7.16005310e-03  4.36492533e-02\n",
      " -2.57433202e-02  7.13419309e-03 -3.72464428e-02  7.92187377e-03\n",
      " -5.10222928e-02  4.14791995e-02 -3.17742289e-02  5.86885992e-03\n",
      " -8.22727125e-02  5.11286107e-03  1.28448824e-02 -1.51612558e-02\n",
      " -1.11590635e-02 -7.10509727e-03  2.61887009e-02 -2.12293651e-02\n",
      " -9.99935714e-03 -8.48172261e-04 -1.26356001e-02 -4.78495204e-02\n",
      "  2.21412009e-02 -3.64238768e-02 -8.73019968e-03  1.51356260e-02\n",
      "  6.61027034e-04 -6.82354963e-03 -1.17578918e-02 -7.48698222e-03\n",
      "  5.12085600e-03  1.42762552e-02 -6.84280120e-03  8.80831908e-03\n",
      "  4.85040241e-03 -3.69496218e-03 -2.31129325e-02 -9.31793459e-03\n",
      "  1.44631454e-02  6.36634976e-03 -3.60542238e-02 -8.05398419e-03\n",
      " -2.73758424e-02 -3.87407000e-02  1.17745459e-02  1.94502656e-02\n",
      " -8.77285591e-03 -3.17414713e-02  1.13271401e-02  3.10578973e-03\n",
      " -7.22769070e-03  2.94344985e-02  2.64043027e-02 -1.27100558e-02\n",
      "  1.04244243e-03  1.55636788e-02 -6.99075220e-03 -1.73350340e-02\n",
      " -3.02911790e-02 -2.39455198e-02 -1.57621243e-02  3.03321614e-02\n",
      " -3.05893464e-03 -7.68569638e-03 -1.40632106e-02  2.62563251e-02\n",
      "  3.86887546e-03  3.82646317e-02  9.23083167e-04  4.14084357e-03\n",
      "  6.26788905e-02 -1.63040061e-02  2.98444608e-02 -4.59713639e-02\n",
      " -9.15888192e-03  3.73591140e-02  3.59559005e-03  3.01305885e-03\n",
      " -1.40561430e-04  5.94427600e-03 -6.94562797e-03  2.23689252e-02\n",
      " -4.43762101e-02 -4.36196688e-02  1.39270139e-02  3.41155943e-02\n",
      "  3.35878723e-03 -1.37032329e-02  1.06649385e-02  1.44998086e-02\n",
      "  4.23045245e-03  3.93509867e-02  4.51515214e-02 -3.88888317e-02\n",
      " -2.79643665e-02 -1.47197468e-02 -2.23356187e-02 -1.71535633e-02\n",
      " -5.79161880e-03  3.01330524e-02 -2.66818389e-02 -3.56742649e-02\n",
      "  3.25796692e-02 -7.66950072e-02  3.55964711e-03 -1.40919432e-02\n",
      " -7.63495361e-03  4.01260116e-02 -1.34320289e-02  3.09362704e-03\n",
      "  4.00370920e-03  2.64269400e-02 -2.12084861e-02 -1.46496813e-02\n",
      "  2.73183193e-02  2.10256899e-02  4.02456523e-03 -4.24903697e-04\n",
      "  2.80429636e-02 -4.66844096e-02  2.54407438e-02  7.87052365e-03\n",
      "  4.59129541e-02  3.31282756e-02 -1.74025665e-02 -3.11100823e-02\n",
      "  3.68625851e-02 -5.93481699e-03 -5.92686075e-02  7.06538680e-03\n",
      " -1.41192203e-02  6.14730962e-03 -4.41646364e-02  1.52377952e-02\n",
      " -3.69726558e-02 -1.37494957e-02  9.88093589e-03  3.51706443e-02\n",
      " -2.57221314e-03  2.44023961e-02 -3.98813976e-02  1.50632267e-02\n",
      "  2.63561009e-02 -2.52769406e-03 -1.85607066e-02  4.87159456e-03\n",
      "  1.66350323e-02 -1.14356172e-02 -4.64160407e-03  4.36696038e-02\n",
      " -1.02332626e-02 -3.65778930e-02  7.59695666e-03  4.79264220e-03\n",
      "  4.08627522e-03 -8.03924310e-03 -1.72200492e-02  4.92690343e-02\n",
      " -3.55405638e-02  8.18256798e-03  3.23847269e-02  1.61244395e-02\n",
      " -3.62166394e-02 -3.38740831e-02  1.68599299e-03 -4.34989972e-02\n",
      "  4.32617075e-02  1.74193427e-02 -3.90154885e-02  3.34519111e-03\n",
      "  2.78269012e-02 -6.20810301e-03 -9.66048910e-03 -1.96137357e-02\n",
      "  3.27961028e-02  4.54790860e-04 -1.48543325e-02  3.55868453e-02\n",
      " -1.61640871e-02  5.64504314e-03 -2.77459847e-03 -3.78626280e-03\n",
      " -3.04261865e-02]\n",
      "567214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 29 testing: 20\n",
      "correctsource    17\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    12\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6        0.8        0.4        0.75       0.5        0.66666667\n",
      " 0.66666667]\n",
      "Accuracy: 0.6206896551724138\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.71      0.69        17\n",
      "       missed       0.55      0.50      0.52        12\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        29\n",
      "    macro avg       0.61      0.60      0.60        29\n",
      " weighted avg       0.62      0.62      0.62        29\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.50      0.55        12\n",
      "       missed       0.40      0.50      0.44         8\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        20\n",
      "    macro avg       0.50      0.50      0.49        20\n",
      " weighted avg       0.52      0.50      0.51        20\n",
      "\n",
      "(325,)\n",
      "[-0.01193481 -0.01469755 -0.00358579  0.04093756 -0.0173061  -0.0215276\n",
      "  0.08303776  0.03052533 -0.04501896  0.03338976  0.00740047 -0.04038671\n",
      " -0.00646062 -0.00633867  0.0090701  -0.00367138  0.00618914  0.01306861\n",
      " -0.00072095  0.0250297   0.01682637  0.01308782 -0.00207194  0.00494814\n",
      "  0.0110234   0.01212834 -0.0321816  -0.00312689  0.02832387  0.00983659\n",
      "  0.00753007  0.02416595 -0.0055789   0.01794275 -0.00744296  0.01435476\n",
      " -0.01175614  0.03279184  0.0008755  -0.03205827  0.00683783  0.00945552\n",
      " -0.00599392  0.02528157 -0.02410833  0.01800476  0.03542504 -0.00592549\n",
      " -0.00635751 -0.00250218 -0.03527142 -0.05011827 -0.00284871  0.0052079\n",
      " -0.01556422  0.0477126   0.03021301 -0.00230256 -0.01242355  0.0208344\n",
      " -0.00815801 -0.01385182 -0.00603185 -0.00245067  0.02578717  0.01384753\n",
      "  0.01067791  0.02103999 -0.05962154  0.00715653 -0.01200024 -0.01483379\n",
      " -0.00876021 -0.01680699 -0.01991553  0.02585666 -0.00219504 -0.01070588\n",
      " -0.00698    -0.00678735  0.00512008  0.00555005 -0.05529264  0.0103369\n",
      " -0.08089885  0.03973031 -0.00384654  0.03534982  0.03023369  0.02374888\n",
      " -0.00872665  0.00800318 -0.02525346  0.01878266  0.02092783  0.0010166\n",
      " -0.00845951 -0.00532004  0.01756239  0.01308643 -0.0024327   0.01168286\n",
      " -0.02233158 -0.02914079 -0.02574434 -0.00212403  0.05100726  0.02764142\n",
      " -0.00973093  0.02396673  0.0166895   0.01845334 -0.00062723 -0.006179\n",
      " -0.02053965 -0.02334565  0.00617383 -0.05041951 -0.0186061  -0.00940107\n",
      " -0.02909202 -0.00257854 -0.02495227  0.00548429 -0.00387838 -0.02519087\n",
      " -0.01604393 -0.0197316  -0.02500668 -0.02756791  0.00065239 -0.00234488\n",
      "  0.01721336  0.0097058  -0.00012843 -0.01845831  0.00119865  0.04049138\n",
      " -0.00103729  0.00241906 -0.01366005  0.00105171 -0.03276021  0.01785641\n",
      " -0.00219436  0.02829525 -0.03027701 -0.0318353   0.00817958 -0.00683601\n",
      "  0.02545358 -0.0102023   0.01761587 -0.0119957  -0.01662458 -0.02150607\n",
      " -0.00357431  0.00747417 -0.024525   -0.03695445 -0.06969508 -0.06125661\n",
      " -0.01812324  0.00645742 -0.00781623  0.02259827 -0.01101078  0.0068905\n",
      " -0.01346729 -0.01010535  0.00061768 -0.01761226  0.01346886  0.00213659\n",
      "  0.0037331   0.03062728  0.01712321  0.01239927  0.02244157 -0.02008716\n",
      " -0.00727646 -0.02804772  0.01974462 -0.01277847 -0.0181349   0.0017197\n",
      "  0.00660193 -0.01635136  0.00098967 -0.01122934 -0.01794199  0.0032447\n",
      "  0.01890893 -0.00117004  0.02704334 -0.0765173  -0.00937962  0.02805072\n",
      "  0.00378555 -0.00484635 -0.00240477  0.02646687  0.02706317  0.01577402\n",
      "  0.02904633 -0.01387778  0.00872892  0.03208552 -0.03639312  0.00153558\n",
      " -0.02277061  0.0109934  -0.0037779   0.00379386  0.00106473  0.02912846\n",
      " -0.02105383  0.01569116 -0.02142524  0.01294296  0.00234038  0.01224428\n",
      " -0.02084159 -0.01186801 -0.02142031 -0.01480555  0.01840577 -0.03078921\n",
      " -0.01763764 -0.00296291  0.01222329 -0.02685903  0.03443743 -0.03875432\n",
      "  0.01640018  0.00513169  0.01379646  0.00353727  0.00390292  0.02720178\n",
      "  0.00731276  0.00395862 -0.00563558 -0.00844568 -0.02779324  0.00267659\n",
      "  0.02133913  0.00487513  0.0039078  -0.00242995  0.00486463  0.00215349\n",
      " -0.01931421  0.0274224   0.0144323  -0.00118022  0.00192447 -0.00307233\n",
      " -0.04490473 -0.03624138 -0.02011541  0.00103451  0.0030655  -0.03495387\n",
      " -0.02113809  0.01952702 -0.01067482 -0.01418183 -0.02523264  0.02228152\n",
      "  0.01757388  0.00042692  0.01242209 -0.01440802 -0.01460868 -0.01639671\n",
      "  0.00970279 -0.00790939 -0.02126221  0.01311609  0.01108897 -0.00925276\n",
      " -0.03066286 -0.05419332 -0.00544261  0.01347618  0.01330749  0.0180883\n",
      "  0.01367374 -0.02449533 -0.00146313  0.0227932  -0.00138061 -0.0108581\n",
      " -0.04324789  0.0011284   0.03025401 -0.0081231  -0.00050668  0.01704639\n",
      " -0.01345884  0.02130601  0.02640362 -0.00672821  0.02768756 -0.01234839\n",
      "  0.00795151 -0.00567668 -0.01276262  0.03225575 -0.00224517 -0.03032834\n",
      "  0.03436001 -0.02445723 -0.00618577 -0.00939455 -0.02974227  0.03010283\n",
      "  0.02524066  0.01641152  0.02866131  0.00724459 -0.04154373 -0.03712291\n",
      " -0.00634844]\n",
      "597569\n",
      "training: 39 testing: 27\n",
      "correctsource    28\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    20\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.83333333 0.5        0.66666667 0.83333333 0.4        0.6\n",
      " 0.6       ]\n",
      "Accuracy: 0.6410256410256411\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.75      0.75      0.75        28\n",
      "       missed       0.36      0.36      0.36        11\n",
      "\n",
      "    micro avg       0.64      0.64      0.64        39\n",
      "    macro avg       0.56      0.56      0.56        39\n",
      " weighted avg       0.64      0.64      0.64        39\n",
      "\n",
      "accuracy = 0.5555555555555556\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.70      0.70      0.70        20\n",
      "       missed       0.14      0.14      0.14         7\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        27\n",
      "    macro avg       0.42      0.42      0.42        27\n",
      " weighted avg       0.56      0.56      0.56        27\n",
      "\n",
      "(325,)\n",
      "[-3.64927043e-02  4.68505653e-02 -1.37931574e-02 -1.43085367e-02\n",
      " -1.93069213e-02 -1.83668409e-02  4.05246431e-02 -4.41135763e-03\n",
      "  5.60398941e-02 -6.09441372e-02 -1.45603426e-03 -6.42056439e-02\n",
      "  4.30654621e-02  9.16609973e-03 -2.84503465e-02  1.01504803e-02\n",
      "  1.96465707e-02  1.94480655e-02  5.02734488e-03 -1.68113781e-02\n",
      "  1.41242925e-02 -2.62587827e-02  2.64588696e-02 -8.36904293e-03\n",
      "  8.12787520e-03  8.04700429e-03 -1.99052175e-02  1.29121648e-02\n",
      "  7.85538183e-02 -3.33933227e-02 -2.30278131e-02  4.19047976e-02\n",
      " -5.41307906e-02  5.32664885e-03 -3.74270400e-03 -6.34673596e-03\n",
      " -2.58684602e-02 -5.59967022e-02 -4.82676432e-02 -1.02758175e-02\n",
      " -4.24200692e-03  3.38029555e-02 -5.70534575e-02 -3.03135262e-02\n",
      " -2.95551239e-02 -2.23325592e-02 -2.07716597e-02  1.00710349e-02\n",
      " -6.59393662e-02 -4.66671899e-02 -5.57435296e-03  5.65304365e-02\n",
      " -2.24853705e-02 -3.14798185e-03  9.16705717e-03  2.13470886e-02\n",
      " -4.20332753e-02  3.33157950e-03 -2.93050755e-02  1.86586878e-02\n",
      "  7.18038680e-02  2.34090877e-02 -3.53374122e-02  2.19207013e-02\n",
      "  1.95657302e-03 -8.76663800e-03  3.39161533e-03 -1.06485054e-02\n",
      " -7.85269301e-03 -3.06499846e-02  5.06460341e-02 -5.51954510e-02\n",
      " -4.18696646e-02 -2.49970063e-02 -6.77754087e-03 -4.65748035e-02\n",
      "  2.28447012e-02  3.50869840e-02  2.07616183e-02 -3.48375018e-02\n",
      " -2.92262707e-02  1.72307584e-02  1.16523309e-02  4.20082390e-02\n",
      "  2.02452244e-02 -1.49468652e-02 -4.82694232e-02 -1.07797559e-02\n",
      "  3.48868193e-02  1.80164779e-02  2.20232837e-02  1.04413411e-02\n",
      " -4.60811129e-02  1.25436199e-02  1.18396525e-02 -6.51200088e-02\n",
      "  1.85813330e-02 -6.65420719e-03 -2.10912881e-02  2.42356289e-02\n",
      "  8.94709524e-03  5.80327718e-02  4.23721027e-02 -2.49769738e-02\n",
      " -3.30497260e-02  3.19598360e-02  3.32660342e-02 -1.10176537e-02\n",
      "  1.00433161e-03 -1.01121796e-02 -1.45308303e-03  2.27197476e-02\n",
      "  2.31989084e-03  9.18134874e-03 -5.20454239e-02 -4.97553262e-02\n",
      " -2.39104427e-02  2.16989703e-02 -3.41373882e-03  1.18455116e-02\n",
      "  1.73286658e-02 -5.55912812e-02  3.45287002e-03  1.44940658e-02\n",
      "  1.53156058e-02  1.37767800e-02  8.44061206e-02 -2.96620332e-03\n",
      " -4.12923787e-03  1.62356307e-02  1.91100367e-02  4.12167961e-02\n",
      " -1.54626316e-02  1.88289006e-02 -5.12846755e-03  4.13480580e-02\n",
      "  4.51131459e-02 -2.15584499e-02  9.44512340e-03 -3.30162249e-02\n",
      " -3.16767913e-02  1.96118871e-02 -2.09620724e-03  1.65494471e-02\n",
      "  5.40444464e-03 -2.55310294e-02  3.32397851e-02 -1.90633969e-02\n",
      "  3.54088356e-02 -5.08425930e-03  2.18685391e-02  1.33498130e-02\n",
      " -2.94106129e-02 -2.52724987e-03  1.62392221e-02  3.03504936e-02\n",
      " -2.81564904e-02  1.14363954e-02 -8.70471925e-03  7.44947389e-05\n",
      "  1.99505563e-02  1.96623435e-02 -4.61436025e-03  3.73120976e-02\n",
      " -1.97272527e-02 -6.89229075e-03 -2.56851091e-03 -3.87498878e-02\n",
      " -4.47305951e-02  9.72832897e-03 -1.68403575e-02 -1.99906642e-02\n",
      " -7.71550679e-03  1.01962062e-02  9.32872928e-03  7.86962381e-03\n",
      " -3.61231168e-02 -1.54398627e-03 -1.64586354e-02  1.45078627e-02\n",
      "  1.31239440e-02  1.42008600e-02  1.84079144e-02  2.34601740e-02\n",
      "  2.01700754e-02 -8.51846011e-03 -1.88449149e-02 -4.33826191e-03\n",
      "  1.07336748e-02 -1.56957654e-02  1.56005902e-02 -2.94773420e-02\n",
      " -1.21349944e-03 -3.23051312e-02 -9.18488518e-03 -3.17567784e-02\n",
      " -2.57770506e-02 -5.24074914e-02  4.29593030e-02  2.30651169e-02\n",
      " -2.88959907e-02  4.65000136e-02  4.34330069e-03  9.98078764e-03\n",
      " -1.50248464e-02 -2.70655565e-02 -1.45832152e-02  6.28214829e-02\n",
      "  1.06407125e-02  1.91280244e-02  1.44822368e-02 -5.66370660e-02\n",
      "  2.33516342e-02 -1.10071997e-02 -1.89267866e-02  7.39032527e-03\n",
      "  1.93661066e-02 -8.87102947e-03  1.13894643e-02  1.30426391e-02\n",
      " -2.82157659e-02  4.34248960e-02 -4.09579377e-02 -6.67995237e-03\n",
      "  1.38807057e-02 -5.10435082e-02 -8.37837295e-03  7.04238332e-03\n",
      " -1.15531123e-02 -2.98328178e-02 -9.49338376e-03  3.21771222e-02\n",
      " -1.14979744e-02 -5.00854879e-02  5.27407236e-03 -1.11048270e-02\n",
      "  1.44182582e-02  1.42808722e-02 -1.53564637e-03 -1.93637543e-02\n",
      " -1.85898739e-02  2.92276608e-02 -5.04913660e-02 -1.69752200e-03\n",
      "  2.70288484e-02 -2.63813206e-04 -6.50078810e-03 -3.73253791e-02\n",
      " -1.33038865e-02  1.71950248e-02  8.59768602e-03  2.87031616e-02\n",
      " -4.13900127e-02  2.03834710e-02 -8.02732538e-03 -8.36457266e-03\n",
      " -4.40485210e-02  2.81222401e-02 -3.81992468e-02  1.07095326e-02\n",
      " -2.17111441e-02  1.47081571e-02  2.13046328e-02 -2.62269173e-02\n",
      "  8.77778202e-03  3.77969820e-03  2.10675165e-03  4.67163973e-02\n",
      " -1.76737045e-02  3.58657852e-02  1.03093668e-02 -4.60622297e-02\n",
      " -5.96410410e-03 -4.91972738e-02 -1.04196903e-02 -1.78714096e-02\n",
      "  2.87229448e-02  1.20302153e-02 -6.23340738e-04  3.47084659e-02\n",
      " -6.64123859e-03 -5.06028158e-03 -1.56494240e-02  2.61239157e-02\n",
      " -9.08558643e-03 -2.15581117e-02  3.07278486e-02  2.24576302e-02\n",
      "  1.56562563e-02  1.17733663e-02 -1.56232153e-02 -1.13247143e-02\n",
      "  1.61044674e-02  3.81608865e-03  4.09829110e-02 -2.81013116e-03\n",
      " -1.29580541e-02  2.84989714e-02 -5.65947857e-02 -2.84718862e-02\n",
      " -9.71361989e-03  1.06275924e-02  2.61704964e-02 -2.75463197e-02\n",
      "  1.07066379e-02 -9.34431264e-03 -1.84141816e-02 -3.75818931e-02\n",
      "  2.25731096e-03 -3.78116622e-02 -6.49280435e-04 -2.60202188e-02\n",
      " -9.12765441e-03 -5.23987702e-02 -9.86197404e-04 -5.03579403e-03\n",
      "  1.32597469e-02 -5.33814908e-03  4.45108899e-02  1.44621944e-02\n",
      " -1.84696026e-03 -2.29616251e-02  1.72383732e-02  2.46594356e-02\n",
      "  2.06127862e-03]\n",
      "652850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 34 testing: 23\n",
      "missed           21\n",
      "correctsource    13\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           15\n",
      "correctsource     8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.4  0.2  0.2  0.4  0.6  0.4  0.75]\n",
      "Accuracy: 0.4117647058823529\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.23      0.23      0.23        13\n",
      "       missed       0.52      0.52      0.52        21\n",
      "\n",
      "    micro avg       0.41      0.41      0.41        34\n",
      "    macro avg       0.38      0.38      0.38        34\n",
      " weighted avg       0.41      0.41      0.41        34\n",
      "\n",
      "accuracy = 0.6521739130434783\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.38      0.43         8\n",
      "       missed       0.71      0.80      0.75        15\n",
      "\n",
      "    micro avg       0.65      0.65      0.65        23\n",
      "    macro avg       0.60      0.59      0.59        23\n",
      " weighted avg       0.63      0.65      0.64        23\n",
      "\n",
      "(325,)\n",
      "[-1.33394688e-02 -3.05393068e-02 -3.41580141e-02  1.48013276e-02\n",
      " -4.14949233e-02  6.28699132e-02  2.39669922e-02 -1.68573477e-03\n",
      " -1.73965757e-02  6.78595169e-03  1.35867170e-02 -1.37196689e-02\n",
      " -3.00536431e-02  1.98531766e-02  8.35920460e-03 -3.82723862e-02\n",
      " -1.71126615e-02 -2.09576232e-02 -2.90191987e-02  5.99858960e-02\n",
      " -4.69518673e-02 -8.71347233e-02  1.10086817e-02 -1.84701232e-02\n",
      " -4.75085616e-02 -3.69790230e-02 -1.08498132e-02 -4.31712826e-02\n",
      "  7.72284915e-03  1.61806046e-02 -2.15140504e-02  4.58981974e-02\n",
      "  1.07427464e-01  8.05607721e-03 -4.02083309e-02  1.14861922e-03\n",
      "  8.04803726e-03 -2.25873255e-02 -1.36749837e-02 -2.33689051e-03\n",
      "  2.02057623e-02  3.12247724e-02  4.01678244e-03 -3.86019635e-02\n",
      " -3.26670654e-02  5.36689662e-03  4.24152497e-02 -2.10025557e-02\n",
      "  2.38571061e-02 -5.74900754e-02 -2.88801952e-02  7.78325319e-03\n",
      " -1.09841962e-01  2.93868315e-02  4.36057630e-02 -4.27573622e-03\n",
      " -9.94602063e-03  2.80542975e-02 -9.92113556e-03  6.13992597e-03\n",
      "  4.30591438e-02  3.10879810e-02  1.30443028e-02  5.66780748e-02\n",
      "  7.84322246e-02 -1.08506642e-02  1.71005987e-02  3.86665524e-03\n",
      " -2.78544300e-02  4.96021416e-03  3.86712334e-02 -2.42799741e-02\n",
      "  1.47878359e-02 -2.94434591e-02 -1.09848295e-02  8.10336258e-03\n",
      "  4.35660604e-02  8.57234851e-03  1.66382333e-02  4.13719523e-02\n",
      " -5.83066150e-03  2.65729090e-02  1.84386362e-02  2.53047259e-02\n",
      " -4.77085494e-03  2.04574423e-02 -1.71317404e-02 -4.54536996e-02\n",
      " -3.39210334e-02 -1.50821812e-02 -6.25391518e-03  8.72198654e-02\n",
      " -5.26186401e-03  1.97997799e-03  2.95126732e-02 -1.83218145e-02\n",
      "  1.99453287e-02 -1.77480416e-02 -3.17450333e-02  3.67162486e-02\n",
      " -3.62520305e-02  1.04507640e-01  4.06326821e-02 -1.67318789e-02\n",
      "  3.15265388e-02  8.60951180e-03 -4.46419184e-02 -4.20154430e-02\n",
      "  8.83320984e-03 -2.02307071e-02 -2.43800261e-02  3.28036224e-02\n",
      "  1.11145925e-02  4.18584487e-02  3.11646986e-02  4.81834017e-04\n",
      "  3.11762891e-02  1.67810638e-02 -9.64377867e-03 -7.42204936e-03\n",
      " -4.73978556e-02  2.55658374e-02 -5.50501768e-03  2.28279583e-02\n",
      " -7.73042244e-03 -6.28947486e-02 -4.88243099e-03 -2.63648572e-02\n",
      "  1.60415444e-02  2.03074631e-02 -2.70461857e-03 -1.10707080e-02\n",
      " -1.16970231e-02  2.31403672e-02 -2.10981275e-02 -2.55811001e-02\n",
      " -2.52789094e-02 -1.67069020e-02  3.45058024e-02  3.53958186e-03\n",
      "  1.29667334e-02  4.27334825e-02  2.55420444e-02 -4.26229919e-02\n",
      "  3.08579062e-03 -3.72058799e-02  1.54276899e-02 -7.10338178e-03\n",
      "  8.07770548e-02 -7.54062431e-03 -2.09730057e-02 -7.12430577e-02\n",
      "  4.05649060e-03 -2.00702018e-03  5.20301507e-02 -7.26933219e-03\n",
      " -9.80971242e-03  7.62559176e-02 -2.11348778e-02 -1.29838886e-02\n",
      " -8.08767342e-02 -8.20669501e-02  1.67213632e-02 -2.03568678e-03\n",
      "  2.46810379e-02  3.09197679e-04 -8.70378501e-03  2.10384681e-02\n",
      " -2.04953065e-02  2.84679006e-02  2.40857263e-02 -4.00047183e-02\n",
      " -5.60966595e-03  1.40155128e-02  2.71619106e-02  9.95205592e-03\n",
      "  9.07167976e-03  3.27656420e-03  3.34378598e-02 -2.56550004e-02\n",
      " -2.09326589e-02  1.20858455e-04  2.00570850e-02  5.64307496e-02\n",
      "  1.55093691e-02  3.28411970e-02 -5.70722258e-03  6.75924511e-03\n",
      "  6.85342121e-02 -2.24877114e-02  3.81494999e-02  9.71976852e-03\n",
      " -2.65770905e-03 -8.34071256e-03 -1.90016948e-02  9.72045353e-03\n",
      "  1.88507652e-02  5.29375246e-02 -9.31092906e-03 -2.93284960e-02\n",
      "  3.85649336e-03 -3.45766122e-02  3.52039870e-03  9.02051870e-03\n",
      "  4.59069994e-02  1.31299152e-02 -9.93551698e-03 -2.77709480e-02\n",
      " -4.95069330e-02 -9.08911387e-03 -1.10997518e-02 -2.34476888e-02\n",
      "  5.54101874e-02 -3.18348913e-02 -3.56323572e-02  3.68887005e-02\n",
      "  7.81342376e-03 -1.82557689e-02 -8.18408640e-02 -3.69066178e-03\n",
      " -3.42845631e-03 -9.76194372e-05 -6.11405911e-02 -1.73440541e-02\n",
      " -5.06019878e-02  2.51304024e-02 -9.71530434e-03 -6.00175048e-03\n",
      " -2.63799413e-02  1.84841824e-02  3.37032999e-02  8.79034183e-05\n",
      " -3.25524570e-02  5.55812919e-02 -2.45679328e-02 -4.33696827e-03\n",
      " -5.45738319e-02  7.70480400e-02  5.60494085e-02  4.81623390e-02\n",
      " -3.04728196e-02  1.22064958e-02  4.51311589e-02  1.60793728e-03\n",
      "  5.49061710e-02  1.43181421e-02 -2.74449502e-02 -5.02262198e-03\n",
      "  8.84382328e-02  1.36745889e-02  8.57184683e-03  1.13820835e-02\n",
      "  4.96554296e-03  1.92229309e-03  1.30865038e-02 -5.90113677e-02\n",
      "  2.11216515e-02  2.03034906e-03 -3.28528484e-02  3.89036678e-02\n",
      "  4.45570533e-02 -2.36246193e-02  4.23394308e-03  2.60985746e-02\n",
      "  2.75365814e-02 -3.04637335e-02  3.22205129e-03  4.25095083e-02\n",
      " -2.77035002e-02  1.27111604e-02  4.09671290e-02 -2.85425048e-02\n",
      "  1.34976226e-02  7.68780901e-02  5.43092886e-02  2.04568783e-02\n",
      "  1.80469871e-02  1.55290480e-02 -3.50737221e-02 -3.74293168e-02\n",
      "  1.99295009e-02 -6.21721925e-02 -2.39386735e-02  2.61507516e-03\n",
      " -3.88202754e-02  1.81437186e-03 -2.87166985e-02  1.50228098e-02\n",
      "  3.04190696e-02 -8.19574391e-03  7.44695787e-03 -1.56910691e-02\n",
      " -2.13444624e-02 -2.37916813e-02 -8.42968710e-04  2.41603704e-02\n",
      " -2.39351889e-02 -6.03841017e-02 -5.24636315e-02 -3.62272389e-02\n",
      "  2.94647356e-02 -3.86294462e-02  5.67215885e-02 -8.10398655e-02\n",
      "  1.57314407e-02 -5.95658917e-02  3.68018218e-02  6.13986498e-03\n",
      " -8.10790630e-03  1.62797074e-04 -6.10065284e-03 -1.61869499e-02\n",
      " -5.13969408e-02  6.04447593e-04 -1.16340843e-03 -5.39038735e-05\n",
      " -1.23593543e-02 -7.76885424e-03 -4.69515707e-02 -1.56182270e-02\n",
      " -1.59810157e-02  5.37979208e-02  3.47768653e-02  2.18095376e-02\n",
      " -4.43105178e-02]\n",
      "677561\n",
      "training: 32 testing: 22\n",
      "correctsource    20\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.4        0.4        0.8        0.         0.4        0.5\n",
      " 0.33333333]\n",
      "Accuracy: 0.40625\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.52      0.60      0.56        20\n",
      "       missed       0.11      0.08      0.10        12\n",
      "\n",
      "    micro avg       0.41      0.41      0.41        32\n",
      "    macro avg       0.32      0.34      0.33        32\n",
      " weighted avg       0.37      0.41      0.38        32\n",
      "\n",
      "accuracy = 0.5909090909090909\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.69      0.64      0.67        14\n",
      "       missed       0.44      0.50      0.47         8\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        22\n",
      "    macro avg       0.57      0.57      0.57        22\n",
      " weighted avg       0.60      0.59      0.60        22\n",
      "\n",
      "(325,)\n",
      "[-8.70464460e-03 -2.59682554e-02  2.13958246e-02  1.58164480e-02\n",
      " -4.60606428e-02  2.72672447e-02 -3.98483726e-02  3.43100103e-02\n",
      " -3.01932933e-03  1.09047199e-02 -6.34533205e-02  1.57047021e-05\n",
      " -5.24799550e-02 -4.14714993e-02  3.51350474e-02  9.61178328e-03\n",
      " -4.35111403e-02 -3.22014976e-02  2.36549692e-02  3.84605230e-02\n",
      " -1.90144825e-03  3.08987574e-03  3.68877328e-02 -4.91369993e-02\n",
      " -4.98899720e-04 -3.40182674e-02 -4.52622549e-02 -4.67051519e-02\n",
      "  5.49074610e-02  9.43874553e-03 -2.31835345e-03 -3.18127027e-02\n",
      "  1.54330482e-02  2.87163738e-02  1.51732003e-02  5.73633749e-02\n",
      "  2.28018284e-04  4.62320587e-02 -3.95729714e-02 -1.76710222e-02\n",
      "  1.14150523e-02  2.34885533e-02  4.55081572e-02 -2.04474079e-02\n",
      "  5.77871276e-02 -2.44254915e-03 -1.51359998e-02 -8.38575787e-03\n",
      " -3.14561141e-02 -9.96646085e-03 -2.77041949e-02 -3.24858965e-02\n",
      "  3.51399965e-02  1.10696848e-02 -2.90857851e-02  4.04713500e-03\n",
      "  6.97130392e-02 -1.45779662e-03 -6.05608369e-03 -8.94060226e-03\n",
      " -4.71208819e-02 -3.20434234e-02 -3.01088738e-02 -2.14247618e-02\n",
      "  2.19473355e-02  6.75266548e-02 -2.27317229e-02 -9.01618669e-02\n",
      " -1.77321443e-02  1.00701047e-02  4.60702940e-02 -7.13353222e-02\n",
      " -1.47706089e-02 -1.52989160e-02 -4.46188285e-02 -1.91139555e-02\n",
      "  9.50766250e-03 -5.93310682e-03 -3.23281615e-02 -9.76598556e-03\n",
      "  4.40352460e-03  3.46854750e-02 -3.34827093e-02  2.56013214e-02\n",
      "  1.29798989e-02  8.81077704e-04 -8.12307765e-03 -1.49214200e-03\n",
      " -1.63581927e-02  2.28011565e-02 -1.26815128e-02 -4.54291633e-02\n",
      "  4.35520390e-02 -1.41390882e-02  1.49935971e-02  3.84229362e-02\n",
      " -1.98323992e-02  1.01188369e-02  1.59271645e-02 -2.34380071e-02\n",
      "  4.59235467e-02  1.27547632e-02 -1.42005543e-03 -1.28497286e-02\n",
      "  2.37624419e-02  1.56935919e-02 -1.08440843e-02 -5.60599118e-02\n",
      " -3.97978324e-03  1.50043695e-02  2.58982151e-02 -1.14983605e-03\n",
      " -1.55009990e-02  3.10828805e-02  6.21629222e-02  2.25210433e-03\n",
      " -1.49063284e-02 -5.12548319e-02 -1.29528649e-02  4.59562698e-02\n",
      "  1.13893583e-02 -7.00292367e-02 -4.79903092e-02 -3.49640540e-02\n",
      " -5.66093428e-03 -1.13893998e-02 -7.29916841e-02  1.10739096e-02\n",
      " -6.32683190e-02 -4.50610044e-02 -4.64211857e-02 -2.15012822e-02\n",
      "  1.55036114e-02 -6.29672960e-02 -8.12344661e-02 -7.62141362e-02\n",
      "  1.06714816e-02  3.84712577e-02  8.29085755e-04 -5.60651105e-02\n",
      "  3.81400404e-02 -4.58847137e-02  1.77130135e-03 -2.17859544e-02\n",
      "  6.67470810e-02  1.50797002e-02 -4.52012875e-02  2.60043605e-02\n",
      " -2.24036822e-02  2.46883812e-02 -1.83383179e-02 -2.13392521e-02\n",
      "  3.61674983e-02 -1.47174044e-02  1.15613403e-02 -1.11272606e-02\n",
      " -1.11281044e-03 -1.87789744e-02  2.81452448e-02 -2.44124750e-02\n",
      " -4.72929408e-02 -5.62185500e-02 -2.33578982e-02 -2.45130753e-02\n",
      " -9.07565991e-03 -8.96410749e-03 -7.39582592e-02 -2.58400080e-02\n",
      "  3.17919729e-03  2.74539252e-02 -3.96314726e-02 -5.17900042e-03\n",
      " -8.56828377e-03  2.62876775e-02  2.11024293e-02  1.08138747e-02\n",
      " -3.48460555e-02 -2.59300493e-02  3.68677436e-02 -1.53942349e-02\n",
      "  5.25730042e-03  4.49160260e-03  4.89237873e-03  2.05380434e-02\n",
      "  4.59646817e-02  4.83648116e-02  3.36199493e-02 -4.00668985e-02\n",
      "  3.68373870e-02  4.52039593e-02  2.99520966e-02 -1.98841322e-03\n",
      " -2.17378800e-02  4.95361840e-02  2.61922083e-02 -8.58421436e-03\n",
      "  3.19202677e-03  2.00362348e-02  3.95753513e-02  5.34680688e-02\n",
      "  9.97062195e-03  4.26708064e-02 -9.62672166e-03  4.26065281e-02\n",
      " -3.14389345e-02 -1.24283753e-02  3.68491040e-02 -1.81529746e-02\n",
      " -4.37812054e-02  2.37170225e-02  3.98112288e-03  7.53367089e-04\n",
      "  8.69081529e-03 -1.77870905e-02 -2.21516429e-02  3.47261918e-02\n",
      " -1.44578314e-02 -2.38229248e-02  2.76163013e-03 -1.58197593e-02\n",
      " -3.78185337e-02 -1.07896156e-02 -5.07052739e-02 -4.01245124e-02\n",
      " -2.69588189e-02 -2.28317310e-02  6.19035820e-03  3.48259687e-02\n",
      "  7.67169743e-02 -3.61632072e-02 -1.13812694e-02  1.19207729e-03\n",
      " -3.28605171e-02 -2.47174379e-02  4.90418278e-02  7.89884372e-03\n",
      "  3.64918637e-02 -2.38678510e-03 -1.20656336e-02  7.16753138e-02\n",
      " -4.13245854e-04 -1.50808210e-02  3.69130879e-02  1.53682242e-02\n",
      "  7.99819664e-03 -5.59717714e-02 -1.94995215e-02 -1.17504085e-02\n",
      " -9.72524987e-03 -3.94365833e-02  2.67876546e-02  4.84949403e-02\n",
      " -3.24188627e-02  3.74931440e-02  1.03739690e-02  1.16651883e-02\n",
      " -1.21891539e-02  3.23077899e-02 -1.39457724e-02 -8.24894960e-04\n",
      " -1.65102294e-02  5.47617851e-02  1.32063718e-02  7.41646985e-03\n",
      "  1.70368311e-02 -2.41225249e-03 -5.31541424e-02  6.78195334e-02\n",
      "  2.37511974e-02 -2.76191664e-02  1.74964426e-02  4.08740647e-03\n",
      "  1.30921751e-02 -3.50716093e-03  2.34280550e-02 -5.96750075e-02\n",
      " -1.76304960e-02 -2.62748722e-02 -4.81000688e-02 -1.68481956e-02\n",
      " -4.05971379e-02 -3.82195792e-02  5.55619787e-02 -4.66570714e-02\n",
      "  1.19404050e-02 -5.30369180e-02 -1.06081572e-02  3.00515205e-02\n",
      " -2.17467709e-03 -2.20707505e-02 -3.40246761e-02  4.93296737e-02\n",
      "  2.59506208e-02 -1.40388489e-02  2.36078647e-02  1.92837273e-02\n",
      " -6.76059910e-03 -5.75926001e-03 -5.15343757e-03 -4.09587439e-03\n",
      " -2.96567406e-02  2.94343490e-02 -3.46274998e-02  1.23610476e-02\n",
      "  1.44362364e-03 -7.65990539e-03  1.55262857e-02  5.34729649e-03\n",
      "  2.06662592e-02  6.19420829e-04  9.83193512e-03 -3.27048115e-03\n",
      "  2.28006379e-02 -3.58389416e-02  2.79393232e-02  4.33052055e-02\n",
      " -1.72919540e-02 -9.67427413e-03 -1.19087831e-02  2.24520776e-02\n",
      " -5.26385684e-02  2.10280927e-02 -5.67250254e-02  1.98317509e-03\n",
      " -7.59241686e-03]\n",
      "711830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 testing: 25\n",
      "missed           21\n",
      "correctsource    15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           14\n",
      "correctsource    11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.83333333 0.2        0.6        0.2        0.         0.6\n",
      " 0.4       ]\n",
      "Accuracy: 0.4166666666666667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.29      0.27      0.28        15\n",
      "       missed       0.50      0.52      0.51        21\n",
      "\n",
      "    micro avg       0.42      0.42      0.42        36\n",
      "    macro avg       0.39      0.40      0.39        36\n",
      " weighted avg       0.41      0.42      0.41        36\n",
      "\n",
      "accuracy = 0.52\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.44      0.36      0.40        11\n",
      "       missed       0.56      0.64      0.60        14\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        25\n",
      "    macro avg       0.50      0.50      0.50        25\n",
      " weighted avg       0.51      0.52      0.51        25\n",
      "\n",
      "(325,)\n",
      "[ 0.1072169   0.0232843  -0.05926223 -0.0673653  -0.02977385  0.05651716\n",
      " -0.04852246  0.01328133 -0.00643435  0.00943804 -0.00050977  0.00389913\n",
      " -0.0254115   0.11265405  0.00509166  0.00870011 -0.04476131  0.01596244\n",
      " -0.04726738  0.00412055 -0.02122882  0.00766897 -0.03679969 -0.0419608\n",
      " -0.02834571 -0.05117075  0.06270104  0.06270899 -0.0386111   0.02212395\n",
      "  0.02749784  0.03565539 -0.00598832 -0.00239257  0.05815794 -0.01507056\n",
      " -0.00852266 -0.01509229 -0.09398754 -0.01411005 -0.00921896  0.00768123\n",
      "  0.02074042  0.02414211  0.00712676 -0.01179656  0.02629153 -0.05262276\n",
      " -0.01597514  0.01816948  0.0185022  -0.06292258  0.00136652  0.03173245\n",
      " -0.0294393   0.03562791 -0.01227996 -0.00819962 -0.00256947  0.0156951\n",
      " -0.01979871  0.05378332 -0.00661256  0.01567119 -0.03098539 -0.01026446\n",
      " -0.03675474 -0.04486933  0.0334333   0.01304366  0.04624821 -0.01480464\n",
      "  0.01551731  0.04405655  0.04309436 -0.01131324 -0.02082334 -0.02055453\n",
      " -0.0215916  -0.0016103   0.01504251 -0.03920439  0.0307198   0.06568017\n",
      "  0.00733769 -0.0521049  -0.0962486  -0.01314621 -0.01334315 -0.07977669\n",
      "  0.01133145  0.00151896 -0.05924115  0.02968135 -0.01849147 -0.01553814\n",
      " -0.07679448 -0.00049958  0.02463877 -0.01346144 -0.00734064 -0.01592684\n",
      " -0.0552676   0.01958653  0.02966127 -0.00071007 -0.01898125  0.00416069\n",
      "  0.02383732 -0.0335645  -0.08157106 -0.0030486   0.02203829 -0.00993443\n",
      "  0.02668986  0.08054875  0.03618089 -0.02045741 -0.03684332  0.02394952\n",
      "  0.06903745 -0.04128526  0.02280844 -0.03078563 -0.04659139  0.00226656\n",
      "  0.01160665 -0.03887435 -0.01819352  0.05655647 -0.01266094  0.00997258\n",
      " -0.0014546  -0.00635446  0.00154664  0.02114233 -0.02548737  0.00346685\n",
      "  0.03072146  0.02022653 -0.00115845 -0.07341529 -0.03896234  0.0046447\n",
      " -0.02336347  0.01373366 -0.03562576 -0.03474831 -0.00456243  0.02748399\n",
      " -0.05331738 -0.02813241 -0.0135466   0.0013946  -0.00554984  0.0050414\n",
      "  0.03100445  0.0112815  -0.02666258 -0.05333988 -0.03862121  0.01219796\n",
      " -0.01407059 -0.09013799 -0.02361532 -0.05356231 -0.04421669  0.00102984\n",
      "  0.0047593  -0.04714543  0.0169711   0.04590862  0.03861633  0.03717512\n",
      " -0.0045712  -0.01929927 -0.0456482   0.00888904 -0.01627827  0.01808284\n",
      "  0.02511711 -0.00192795 -0.00692029  0.00018144 -0.05257511 -0.0398353\n",
      "  0.04724632  0.02098573  0.00443127  0.03885539  0.01291411 -0.00171296\n",
      "  0.02096252 -0.04956961 -0.02349194 -0.0307213  -0.01286522  0.02516109\n",
      " -0.05843894 -0.00954153 -0.03604526  0.001806   -0.00658787  0.0303729\n",
      "  0.0022373   0.03123821 -0.04758175 -0.01228394 -0.042609    0.02610008\n",
      " -0.0163403  -0.03681619  0.00427054 -0.03840153 -0.04173851 -0.05839266\n",
      " -0.01016907 -0.04441432 -0.00555455  0.04124436  0.03392535 -0.01445532\n",
      " -0.01085361 -0.00627682 -0.04880031  0.03463027 -0.01436395  0.00530652\n",
      " -0.01244167  0.00448005  0.01077535 -0.0305138  -0.0396188  -0.00335646\n",
      " -0.00671598  0.00336939  0.00479949 -0.01792909 -0.03433354  0.02577434\n",
      "  0.02222173  0.01019631 -0.03439301 -0.01871697  0.03598936 -0.02199522\n",
      "  0.02790486  0.00189192 -0.07866502 -0.00179481  0.0339234  -0.01242722\n",
      "  0.0053093  -0.05391968 -0.04056224  0.0062081   0.06498367 -0.01168638\n",
      " -0.03075741 -0.02466838 -0.0238416  -0.01785942  0.00265652 -0.03972705\n",
      "  0.0181414  -0.04036508  0.01333124 -0.01373301 -0.01015298  0.00761705\n",
      " -0.02151706  0.03917397 -0.03605845  0.01726593  0.0219729   0.0224987\n",
      " -0.00981108 -0.01604368  0.03540478 -0.05703265 -0.02686739  0.01976545\n",
      " -0.01454341 -0.03727688 -0.04652565 -0.03904452  0.01098843 -0.0046896\n",
      "  0.05201575  0.02917917  0.03659823 -0.01563816 -0.00884328  0.01258565\n",
      "  0.03904061  0.05075222  0.04087451 -0.03486325  0.02292549 -0.01854993\n",
      " -0.02481034  0.06001516  0.03706451 -0.04619492  0.00319851 -0.00925771\n",
      " -0.00568143 -0.05917676 -0.02160892 -0.03135343 -0.00391515  0.01169517\n",
      "  0.00616068 -0.00828788  0.0260438  -0.03857281  0.05130136  0.03197138\n",
      "  0.02181839 -0.02647896 -0.06503819 -0.02206064  0.02150973  0.0116503\n",
      " -0.06425843]\n",
      "729722\n",
      "training: 40 testing: 28\n",
      "correctsource    26\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    18\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.83333333 0.5        0.83333333 0.33333333 0.6\n",
      " 0.8       ]\n",
      "Accuracy: 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.69      0.69      0.69        26\n",
      "       missed       0.43      0.43      0.43        14\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        40\n",
      "    macro avg       0.56      0.56      0.56        40\n",
      " weighted avg       0.60      0.60      0.60        40\n",
      "\n",
      "accuracy = 0.6428571428571429\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.70      0.78      0.74        18\n",
      "       missed       0.50      0.40      0.44        10\n",
      "\n",
      "    micro avg       0.64      0.64      0.64        28\n",
      "    macro avg       0.60      0.59      0.59        28\n",
      " weighted avg       0.63      0.64      0.63        28\n",
      "\n",
      "(325,)\n",
      "[-4.70565624e-02 -4.68279921e-02 -1.61401440e-02  6.58147026e-03\n",
      " -1.46044027e-02 -6.19624495e-03 -2.50395214e-02 -1.90947116e-02\n",
      "  1.77672567e-02  6.77248395e-03  9.17496456e-02  2.49211991e-02\n",
      "  9.57080539e-02 -5.63164964e-02  2.37656850e-03  2.02939651e-02\n",
      "  2.60062072e-02 -1.83216911e-02 -7.15935890e-02  2.91894897e-02\n",
      "  1.74346670e-02  3.15777996e-02 -5.88995027e-04 -1.89426187e-02\n",
      "  2.46789613e-03 -3.97904914e-02 -9.69491028e-03  1.53249493e-04\n",
      "  9.90131674e-03 -5.67522202e-02 -6.49060699e-04  8.69924785e-03\n",
      "  1.33433514e-02  6.25892506e-03  2.38391383e-02 -3.48627363e-02\n",
      " -4.85407017e-02 -4.13649893e-02  8.73145167e-04 -3.56761817e-02\n",
      " -1.80920717e-02 -4.53235782e-02  6.17069688e-02  3.63622424e-02\n",
      " -2.64987520e-02  2.87285980e-02  1.56404027e-02 -7.95465941e-03\n",
      "  3.91304886e-02 -1.09044299e-02 -2.02618101e-03 -1.40865738e-02\n",
      " -2.21174194e-02 -2.40441979e-03  5.52756587e-02  1.47276787e-02\n",
      "  3.78014074e-02  5.42737039e-02  7.06484571e-02 -2.37628627e-02\n",
      " -2.74404246e-02  6.85124853e-02 -5.04135961e-02  2.41019273e-02\n",
      " -3.02000167e-02  1.93369847e-02 -1.03489404e-02 -3.63529107e-02\n",
      "  3.99871260e-02 -2.72038326e-02  5.24405959e-02  6.06474792e-02\n",
      " -6.85313964e-02  2.45001974e-02  1.26230657e-02 -8.90446858e-03\n",
      " -7.58287108e-03 -1.73284098e-02  2.61597470e-02  2.02421789e-02\n",
      "  2.34159659e-02 -1.24000053e-02  5.23331604e-03 -4.59131890e-02\n",
      "  7.73522727e-02  6.04815328e-03  1.78249273e-02 -1.26001468e-02\n",
      " -7.28880909e-03 -1.08630309e-02  1.36963393e-02 -1.46339074e-03\n",
      "  7.79581878e-06 -5.71378208e-03  4.15744430e-03  1.71425127e-02\n",
      " -3.36342264e-02 -1.77325763e-02 -2.01773669e-02 -1.82289572e-02\n",
      " -6.08888923e-03  5.55025450e-03 -5.33376354e-02 -1.62524650e-02\n",
      "  3.13102782e-02  3.94842803e-03 -9.41879014e-02 -6.74931363e-02\n",
      " -7.86554124e-03 -3.40529300e-02 -2.40601290e-02  1.85830026e-02\n",
      "  1.68693908e-02  2.89293781e-02 -4.34066907e-03 -4.97661445e-02\n",
      " -1.42009068e-02 -9.28734992e-03 -1.61951156e-02 -3.02576002e-02\n",
      "  3.31031543e-02 -1.60002041e-02  4.61353655e-02  1.08275754e-02\n",
      " -8.90826362e-03 -1.11066874e-02  2.52405995e-02 -3.62789285e-02\n",
      " -3.07342968e-02 -3.03821323e-02 -4.92101276e-02  8.09565238e-02\n",
      "  1.85043162e-02  1.47297207e-02  4.31526019e-02  2.27352610e-02\n",
      "  3.40097109e-02  1.04130336e-02 -2.86247620e-02  4.00323394e-02\n",
      " -2.22593606e-02  4.08090706e-02  2.98659301e-02 -1.68044911e-03\n",
      " -3.50512067e-02  1.28426854e-02 -4.66048456e-03  2.10434938e-02\n",
      "  4.37757229e-02 -4.42933833e-02 -1.09060099e-02 -1.59552846e-02\n",
      "  8.39160996e-03  1.70709212e-02  1.64643020e-02  6.75335760e-02\n",
      "  4.47058225e-03 -3.63654701e-02  9.43658230e-03 -7.04541994e-03\n",
      " -7.58138989e-02  5.71347698e-02 -7.80395215e-03 -3.33858981e-02\n",
      "  5.76197418e-02 -2.66660786e-02 -5.66202509e-02  5.24163241e-03\n",
      " -2.77979172e-02 -4.26560775e-02 -6.47647803e-03  7.58398079e-03\n",
      "  1.54299969e-02 -3.81375576e-02 -5.61865038e-03  3.67886472e-02\n",
      " -6.53824923e-02  6.95413506e-03  1.97465191e-02  1.28837429e-02\n",
      " -1.17136958e-02 -1.70807237e-02  6.18263025e-03 -1.21423940e-02\n",
      " -3.61431865e-02 -6.56880770e-03 -6.12379402e-03 -3.79395184e-02\n",
      "  3.35948498e-02  2.84039166e-02 -2.04450176e-02  3.48986460e-02\n",
      " -6.19428356e-03 -9.53350752e-03 -1.62592692e-02  2.10548517e-02\n",
      "  2.50484238e-02  3.29074161e-02 -5.96178790e-03 -4.54775261e-02\n",
      " -3.08361414e-02 -3.69413667e-02 -2.54422158e-03  5.85424052e-03\n",
      " -1.98612473e-02 -2.36758702e-02  8.86854726e-03  3.22033876e-02\n",
      " -8.36694448e-03  2.48911298e-02 -1.73881343e-02 -9.82703462e-03\n",
      "  6.49732725e-02 -2.89512609e-02  2.77993886e-02  5.19576209e-03\n",
      "  1.40964914e-02  7.41009156e-03  5.51278858e-02  1.21840867e-02\n",
      "  2.29043323e-02 -9.16378512e-02 -8.38228917e-03  6.06192955e-03\n",
      " -8.68726113e-03  3.36147408e-02  6.02846499e-02 -3.04402155e-02\n",
      " -4.14212175e-03  1.04489739e-02  2.55257434e-02 -3.01207304e-04\n",
      " -4.11434681e-02  1.74578285e-03  3.84808284e-02  2.39265567e-02\n",
      "  8.44204746e-03 -1.74811462e-02 -4.96810094e-02 -3.79873967e-02\n",
      "  1.96390043e-02  7.91321544e-03 -1.66699582e-02  8.05808724e-02\n",
      "  8.74089786e-02  3.27984392e-02  1.13767538e-02 -5.03337387e-02\n",
      " -7.13047533e-02  5.16056300e-02 -5.79196507e-02  6.32981301e-03\n",
      " -2.01002190e-02 -1.26195110e-02 -3.20540886e-02 -9.51108453e-03\n",
      "  3.29278920e-02 -4.92702438e-02 -1.03407220e-02  8.78803936e-03\n",
      " -1.87863198e-02  2.90675526e-02 -3.05879800e-03  1.17522082e-03\n",
      " -4.67798489e-02 -7.19552363e-03 -5.16306534e-03 -2.71099643e-02\n",
      " -1.47436969e-02  1.92465256e-03 -3.59565451e-02 -2.56507522e-02\n",
      "  2.26810876e-02 -2.03989648e-02 -3.39371622e-02  1.28697956e-02\n",
      "  2.78864641e-02  1.16506350e-02 -2.30078633e-03  2.06555751e-02\n",
      "  2.56316591e-02  2.54097462e-02 -8.02194748e-03 -6.84679099e-02\n",
      "  3.05336765e-04  1.26695614e-02 -1.66135167e-02 -3.51374369e-02\n",
      "  4.61770501e-02  6.48185024e-03 -1.00191066e-02  9.69565253e-03\n",
      " -5.91045577e-03 -2.56450394e-03 -5.79887848e-02  1.70724323e-02\n",
      "  2.89121060e-02  2.40434116e-02  7.88330184e-03  4.07752424e-02\n",
      "  2.59507680e-02 -3.99239034e-03  4.79430791e-02  2.01855848e-02\n",
      " -2.31750194e-02  6.32804954e-02 -1.10901197e-02 -1.27646453e-02\n",
      " -1.52226045e-02 -1.99776329e-04 -2.83998705e-02  9.89081014e-03\n",
      " -2.10268605e-02  1.49554799e-02 -2.38215396e-02 -3.50428080e-03\n",
      "  3.21749700e-02 -2.60208086e-02  3.25274424e-02 -5.74892905e-02\n",
      " -5.76576940e-03  1.32827405e-02 -3.00267439e-02  4.17222127e-03\n",
      " -4.77765211e-02]\n",
      "739694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 32 testing: 22\n",
      "correctsource    20\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.4        0.8        0.8        0.2        0.6        0.75\n",
      " 0.33333333]\n",
      "Accuracy: 0.5625\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.71      0.50      0.59        20\n",
      "       missed       0.44      0.67      0.53        12\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        32\n",
      "    macro avg       0.58      0.58      0.56        32\n",
      " weighted avg       0.61      0.56      0.57        32\n",
      "\n",
      "accuracy = 0.5454545454545454\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.61      0.79      0.69        14\n",
      "       missed       0.25      0.12      0.17         8\n",
      "\n",
      "    micro avg       0.55      0.55      0.55        22\n",
      "    macro avg       0.43      0.46      0.43        22\n",
      " weighted avg       0.48      0.55      0.50        22\n",
      "\n",
      "(325,)\n",
      "[-2.64149603e-03 -7.57800693e-02 -3.64204923e-02 -4.42621824e-02\n",
      " -1.41533314e-02 -6.82383549e-02 -8.52731680e-03  2.93808547e-02\n",
      "  2.12849288e-02 -1.88549641e-02 -4.15919848e-02  1.04629740e-02\n",
      "  3.41332014e-04 -2.22643332e-02 -1.86494074e-02 -9.53211553e-03\n",
      " -1.19182563e-02 -1.68423346e-02 -7.04802681e-02  6.06534062e-03\n",
      "  1.74706574e-03 -7.14341867e-02 -3.54747990e-02  1.05720709e-02\n",
      "  3.29748394e-02 -1.21814962e-03  3.28685859e-02 -3.95745595e-02\n",
      "  2.85562437e-02 -9.93343510e-03 -2.86094745e-02 -8.75796882e-03\n",
      " -9.30795938e-03 -1.80530178e-02  5.05372956e-02  5.30452775e-03\n",
      "  5.63589143e-03  2.26619589e-04 -9.19975003e-03  7.05562421e-03\n",
      "  1.65298839e-02 -2.30656287e-03 -5.48916573e-03  3.55766218e-02\n",
      " -1.14932796e-02  1.98524257e-02  1.39069812e-03 -4.50287433e-03\n",
      " -5.31291273e-02  3.05159069e-02 -1.86191044e-02 -3.52960628e-02\n",
      " -2.52918263e-02 -3.34145853e-02  3.09073954e-02  4.74772531e-02\n",
      "  2.42056616e-02 -2.61788202e-02  7.27082105e-02 -5.86376935e-03\n",
      "  3.43204075e-03  2.10365117e-02  1.21849478e-02  4.72868907e-02\n",
      " -6.24986986e-02  5.18501683e-02 -7.27062145e-03  1.20853695e-02\n",
      " -2.19293276e-02 -2.77321399e-02  6.55262299e-03  2.53914011e-02\n",
      " -5.89214214e-03 -2.11673634e-02  5.32185792e-02  1.38543337e-02\n",
      "  1.32272513e-03  3.40878006e-03 -1.58988495e-04 -4.26690113e-02\n",
      " -2.21351991e-02  1.32068709e-02 -1.95397540e-02  4.25132322e-02\n",
      "  7.03295081e-02  5.90845362e-04  1.15198827e-02  7.25088918e-03\n",
      "  3.15439284e-02 -6.97356573e-03 -1.70320854e-02  4.22998115e-03\n",
      "  1.58677068e-02  3.19882210e-03 -5.24865831e-02 -1.54567741e-02\n",
      "  4.84699087e-03 -3.01959000e-02 -3.52928370e-02  1.52966397e-03\n",
      "  8.36589262e-03  5.39063113e-03  2.68057083e-02  2.38560275e-04\n",
      " -9.38729274e-03 -1.60413500e-02 -5.80540298e-02  2.99749131e-02\n",
      "  1.87366953e-02 -2.08132347e-02  1.52142742e-02  9.40265066e-03\n",
      " -2.80335796e-02 -2.09884692e-03 -3.11548158e-02 -1.44046304e-02\n",
      "  1.52449603e-03 -6.33622131e-03 -4.05198113e-02  2.18739275e-02\n",
      "  2.32025673e-02 -3.13749884e-02  3.47459920e-02 -4.47615568e-03\n",
      " -3.96337100e-02  2.82014802e-02  1.02616684e-02 -2.58611577e-03\n",
      " -1.30567081e-02 -2.36532940e-02 -4.34361598e-02  2.07641821e-02\n",
      "  6.72414245e-03 -6.13398454e-03  2.62845171e-03  2.25120402e-02\n",
      "  4.10374031e-02  1.18715818e-02 -4.33784655e-02 -9.39951476e-03\n",
      " -1.55989546e-02  3.52067002e-02  3.49030999e-03 -8.42391724e-03\n",
      " -1.07360944e-02  1.76829434e-02 -3.47365734e-03 -1.34772849e-02\n",
      " -6.19693822e-03  3.16137826e-02 -2.35604064e-02  3.71624145e-03\n",
      " -8.58079533e-03 -1.81647635e-02  3.41484569e-02 -2.27796244e-02\n",
      " -1.50030854e-02 -1.54536457e-02  8.09403467e-03 -6.48982961e-02\n",
      "  5.44052827e-02  4.73669772e-02  3.24987926e-02  7.81555227e-03\n",
      " -1.01776396e-02 -1.93536399e-02 -4.83026094e-02 -1.89304507e-02\n",
      " -1.66417756e-02 -3.18347963e-02 -1.00295247e-02  5.26982943e-03\n",
      " -1.30445911e-02  1.14190750e-02 -3.27065235e-02 -7.49389223e-03\n",
      " -3.99494755e-02 -3.70686703e-02  1.12453248e-02  1.15979081e-03\n",
      "  5.60741668e-03  4.35397701e-03 -4.34123477e-02 -2.60274624e-02\n",
      " -1.41967311e-02  2.33572711e-02  2.25026977e-02  1.45221809e-02\n",
      " -2.86092325e-02 -1.20271360e-02  1.38156244e-02 -7.29961060e-03\n",
      " -3.10116751e-02  3.37022069e-02  2.16259220e-02  4.33251825e-02\n",
      " -3.60280168e-02  2.69935726e-02  5.30499810e-02  3.34672435e-02\n",
      "  8.73230897e-04  9.12585923e-03  3.99461352e-03  4.20907270e-03\n",
      "  2.35481355e-02  2.17701565e-02  1.82355863e-02  4.19066299e-02\n",
      " -1.79727639e-05 -6.65262039e-02  3.61088257e-03 -1.43886658e-02\n",
      " -1.43105734e-02 -1.11074297e-02  3.62395812e-03  6.35211976e-03\n",
      " -4.61331828e-03 -3.73244086e-02 -1.58771562e-02  1.02890111e-02\n",
      "  3.41472866e-03 -8.16147641e-03 -1.69699726e-02 -1.42234529e-02\n",
      "  7.05263850e-03  3.42359708e-02  1.17232560e-02  1.04524994e-02\n",
      " -2.55414447e-02  2.81773578e-02 -1.44509123e-02 -1.83429157e-03\n",
      " -1.95972418e-02  9.69292140e-03  2.97315796e-03 -1.44316980e-02\n",
      "  2.48154007e-02  2.23532221e-02 -4.65245022e-02  3.97319624e-02\n",
      " -7.15204072e-02  1.56559002e-02  1.42852972e-02 -2.01140175e-02\n",
      "  1.43856796e-02 -2.59735909e-02 -2.60985613e-02 -2.05711465e-02\n",
      "  1.85205178e-02 -1.17784726e-02 -3.88840704e-02  3.36376257e-03\n",
      "  4.54670589e-02 -3.28015186e-02  1.56356642e-02 -2.22752044e-03\n",
      "  3.30414013e-02 -3.34720218e-02  7.98558668e-04 -8.77996106e-03\n",
      "  2.72504625e-02 -1.99589642e-02 -2.66602388e-02 -2.93959475e-02\n",
      " -3.30740186e-02  7.75215575e-03 -3.35155421e-02 -4.70602840e-02\n",
      " -9.90141771e-03  3.39514472e-02 -2.38470774e-02 -2.00390503e-02\n",
      " -1.39226668e-02  2.69076946e-03 -3.30436830e-03  3.98116947e-02\n",
      "  6.45646720e-03 -1.49639933e-02 -1.91368460e-02  7.95331934e-04\n",
      "  1.99709570e-02 -3.19650851e-02  1.14998338e-02  3.53189556e-02\n",
      " -5.71571716e-02 -1.76761922e-02 -2.25115264e-02 -1.14738926e-02\n",
      "  8.13780865e-03  2.47400324e-02  2.44883162e-02  4.70932724e-02\n",
      "  1.21842939e-02  2.34000120e-02  2.30042396e-02  2.85276804e-04\n",
      " -1.30720466e-02  1.94596085e-02  3.41556805e-04  2.60553081e-02\n",
      "  2.58354686e-03 -3.15797388e-04 -5.00081112e-02  4.21233590e-02\n",
      " -2.26298233e-02  8.77832960e-03  9.90744779e-03  4.02290791e-03\n",
      " -8.77241983e-03  5.69507802e-03 -2.20771711e-02  2.92143264e-02\n",
      " -2.31623636e-02 -4.66784805e-02 -2.29615397e-02  1.78822361e-02\n",
      " -1.62192517e-02  2.95600426e-02  1.51324577e-02 -1.81922841e-02\n",
      " -2.59508830e-02  1.83880639e-02 -4.27106682e-02  5.45129718e-02\n",
      " -6.55993910e-02]\n",
      "748676\n",
      "training: 36 testing: 24\n",
      "missed           26\n",
      "correctsource    10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           18\n",
      "correctsource     6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.66666667 0.5        0.4        0.4        0.5\n",
      " 0.5       ]\n",
      "Accuracy: 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.17      0.20      0.18        10\n",
      "       missed       0.67      0.62      0.64        26\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        36\n",
      "    macro avg       0.42      0.41      0.41        36\n",
      " weighted avg       0.53      0.50      0.51        36\n",
      "\n",
      "accuracy = 0.625\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.36      0.67      0.47         6\n",
      "       missed       0.85      0.61      0.71        18\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        24\n",
      "    macro avg       0.60      0.64      0.59        24\n",
      " weighted avg       0.73      0.62      0.65        24\n",
      "\n",
      "(325,)\n",
      "[-3.68078060e-03  2.35800791e-02  2.23437901e-03 -3.37505577e-02\n",
      "  5.70047751e-02  1.10118836e-02  2.07343001e-02 -1.66857974e-02\n",
      " -3.90412665e-04  8.77928441e-02 -5.36557335e-04 -8.91987020e-03\n",
      " -4.73807112e-02  1.41486623e-02  1.47141992e-02  1.47886332e-02\n",
      "  3.75364854e-02  3.40114449e-02 -2.84701270e-03  1.42742280e-02\n",
      "  1.37456996e-02 -2.64348435e-02  1.33887363e-02  7.89800312e-03\n",
      " -1.05291846e-02 -4.20353715e-02  1.81292240e-02 -3.31706876e-02\n",
      "  1.58774426e-02  2.23432370e-02 -8.87377312e-03  1.33623124e-02\n",
      " -5.70485594e-03 -6.36193368e-02 -2.92001810e-03 -2.24760501e-03\n",
      " -7.48474190e-02 -4.89903163e-02  7.54981618e-02 -2.94070790e-02\n",
      "  1.50636600e-02  9.02746709e-03 -1.16158905e-02  4.37178549e-02\n",
      " -2.32880910e-02  3.92265648e-02  1.72409647e-02  4.86233715e-02\n",
      " -4.07628640e-02  9.67362246e-03 -5.50472179e-02  1.11482072e-02\n",
      "  2.07116355e-02  3.06561456e-02 -1.81616116e-02  5.77052758e-02\n",
      " -1.51181245e-03 -1.59312173e-02  2.27429245e-02 -2.01103301e-02\n",
      "  3.57306529e-02 -3.72605877e-03 -4.88773131e-02 -9.75052001e-03\n",
      " -1.01298156e-02 -3.01035310e-02 -4.17429845e-02 -1.96647891e-02\n",
      " -7.96498128e-03 -4.86476537e-02  3.31409232e-02 -2.94432543e-02\n",
      " -3.27720990e-02 -2.31656014e-02  3.98434524e-02  4.85139401e-02\n",
      "  2.45801537e-03 -5.09384278e-02 -2.03787817e-02 -2.82169589e-02\n",
      "  1.62522735e-03 -3.26720669e-03 -5.27687892e-02  3.87457226e-02\n",
      "  4.14591041e-03  1.64727864e-02 -5.06439118e-02  3.46384688e-02\n",
      " -4.13587825e-02 -3.00442237e-02 -1.42740927e-03 -1.81036289e-02\n",
      "  3.62603807e-02 -7.77851390e-03 -3.45544199e-02  1.09939341e-02\n",
      " -1.18869198e-02 -6.22526160e-03 -9.59941244e-04 -2.46697525e-02\n",
      " -1.35002774e-02 -1.01424386e-01 -4.73149524e-02  3.69899998e-02\n",
      "  5.84923830e-03  1.64079859e-02  1.75358588e-02 -5.13776854e-02\n",
      " -1.78637208e-02  2.58874627e-02  1.36434505e-02 -2.63222219e-02\n",
      "  4.10995399e-02 -2.92921205e-02 -2.87301375e-02  3.08213670e-02\n",
      "  4.09148985e-02 -5.56893745e-03 -9.25941744e-03 -3.99816657e-02\n",
      " -2.73641527e-02 -1.92698281e-02  9.52551168e-03  3.23491628e-02\n",
      "  2.49939820e-02 -7.67844887e-02 -5.38292022e-02 -7.31678916e-03\n",
      " -2.37169602e-02 -3.66461382e-02 -2.01765110e-02  4.71882498e-03\n",
      " -3.12141293e-03  1.55066710e-02  2.35554862e-02 -2.50685516e-02\n",
      "  3.59507115e-02  6.05855974e-02 -1.98206631e-03 -2.35816017e-02\n",
      " -8.39567421e-03 -1.36929586e-03 -9.00538650e-02 -9.48976988e-03\n",
      " -2.33412312e-02  3.23626465e-02 -9.75630614e-03  1.69947886e-02\n",
      " -1.89921407e-02  6.33477847e-03 -2.94507752e-02 -8.94189088e-03\n",
      "  8.71328077e-02 -1.60864329e-02  3.17781695e-02  8.60556276e-04\n",
      "  2.29942295e-02 -3.60325349e-02 -3.08912690e-02  1.09425008e-02\n",
      " -9.33723584e-03  1.69090553e-02 -2.07508865e-02  1.56370170e-02\n",
      " -2.87135251e-02 -1.21703335e-03 -4.05780898e-03 -9.31604768e-03\n",
      " -4.74729120e-02 -1.73479078e-02  1.69938815e-02  1.27188249e-02\n",
      "  2.39518445e-02  9.49869043e-03  5.64085901e-02 -6.23281347e-02\n",
      " -4.64654569e-02  5.22316340e-02 -4.38476331e-02 -1.89417791e-02\n",
      "  1.11498851e-01 -2.94249446e-02  5.34315985e-03 -1.51891596e-02\n",
      "  1.04980587e-03 -6.04174332e-04 -2.29096082e-03  5.07244877e-02\n",
      " -6.32301451e-04 -4.28813137e-02  4.79526701e-02 -3.13285558e-02\n",
      "  4.89946168e-02 -6.31373985e-03 -1.11852891e-02  6.63604687e-02\n",
      "  2.02089549e-02  3.13144614e-02  3.50912819e-02  4.29059689e-02\n",
      "  1.66501601e-02  3.05099468e-02 -2.05208818e-02 -5.70273780e-03\n",
      " -5.95293545e-03 -2.31025076e-03 -1.22078399e-02  4.11246958e-02\n",
      " -1.88592490e-02  1.97871498e-02  4.75693547e-02 -2.39857315e-02\n",
      "  4.24959326e-02 -6.45922323e-02  3.24747714e-02 -2.44595382e-02\n",
      " -4.03704997e-03 -2.14485750e-04 -1.84773455e-02 -4.82743371e-02\n",
      "  1.99335087e-02 -7.47090718e-03  2.48999133e-02 -1.56826342e-02\n",
      " -1.33932062e-02 -9.71213751e-03  7.38551308e-03  6.17823488e-03\n",
      " -6.23808895e-03 -2.22326339e-02  3.00658810e-02 -4.77859418e-02\n",
      "  3.06356217e-03 -9.61903076e-03  3.03140909e-02 -6.20801198e-02\n",
      " -4.73095947e-02 -2.10083506e-03 -2.31299605e-02  3.78263707e-02\n",
      "  1.24026121e-02  2.32155462e-02 -1.33057506e-02 -1.44690101e-02\n",
      " -2.12059491e-02  1.41055066e-02 -1.69954231e-02  5.04042448e-03\n",
      "  2.78264724e-03  6.17407212e-03 -9.06671725e-02 -8.44269473e-02\n",
      "  2.09537477e-02  2.78429019e-02  5.17353197e-02 -2.34265699e-02\n",
      "  1.91977086e-02  1.39043372e-02 -2.71493093e-02 -3.15895172e-02\n",
      " -1.42442697e-02 -3.33825844e-02 -1.38018512e-02 -4.28019858e-02\n",
      "  3.28558709e-02  1.12942434e-03 -4.78170274e-02  2.00250161e-02\n",
      " -2.15802020e-02  1.29459561e-02 -4.47359962e-04 -4.15949869e-03\n",
      "  4.98762066e-02  7.01011464e-02 -5.79285723e-02  2.25659687e-02\n",
      " -4.09748487e-02 -6.87079076e-03 -5.76361526e-03  4.41827122e-02\n",
      " -1.74217664e-02 -2.47240777e-02 -6.65037432e-03  1.44596089e-02\n",
      "  2.53762994e-03 -4.81995789e-02 -4.90547967e-02  3.37370606e-02\n",
      " -9.80981787e-03 -2.45765049e-02 -2.37546936e-02 -5.54094284e-03\n",
      "  1.80289464e-02  2.29690367e-02 -3.11762744e-02  1.54727441e-02\n",
      "  5.17258039e-05  4.16651378e-02 -6.65853170e-02 -1.47040806e-02\n",
      " -4.54747151e-02 -2.34382659e-03 -2.62916300e-02 -1.65582120e-02\n",
      " -2.62011816e-02  5.50121136e-02  6.71060636e-02 -7.79485295e-02\n",
      "  6.20840072e-03  1.97128855e-02 -7.88692874e-03 -2.98705860e-02\n",
      " -9.46512657e-03 -5.41317307e-02  1.68416674e-02 -1.30736104e-02\n",
      "  9.44253180e-03  2.93365203e-02  8.46875334e-02 -1.37187103e-02\n",
      " -4.93849933e-03 -3.00235810e-02  7.85661230e-03  1.52843149e-03\n",
      " -4.30463766e-02]\n",
      "778749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 33 testing: 22\n",
      "missed           19\n",
      "correctsource    14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           13\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6  0.8  0.6  0.6  0.4  0.75 0.5 ]\n",
      "Accuracy: 0.6060606060606061\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.54      0.50      0.52        14\n",
      "       missed       0.65      0.68      0.67        19\n",
      "\n",
      "    micro avg       0.61      0.61      0.61        33\n",
      "    macro avg       0.59      0.59      0.59        33\n",
      " weighted avg       0.60      0.61      0.60        33\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.38      0.33      0.35         9\n",
      "       missed       0.57      0.62      0.59        13\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        22\n",
      "    macro avg       0.47      0.47      0.47        22\n",
      " weighted avg       0.49      0.50      0.49        22\n",
      "\n",
      "(325,)\n",
      "[-2.59775418e-02  2.29232841e-02 -3.99090323e-03  8.10100471e-03\n",
      "  8.25257632e-03 -6.45031479e-03 -3.57999625e-02 -6.09080863e-03\n",
      "  1.42642375e-04  1.93992261e-03  3.61189446e-02 -4.23625223e-02\n",
      "  2.52309494e-02  1.34979174e-02  2.26601697e-03  4.65318906e-02\n",
      "  8.13716318e-03  1.65844246e-02 -6.76242242e-03 -1.97099467e-02\n",
      " -6.03817805e-03 -1.06163487e-02  1.74964654e-02  4.95759367e-04\n",
      "  1.21008383e-02  1.56488678e-03  2.54395743e-02 -2.40267290e-02\n",
      "  7.24932973e-04 -1.76433735e-02 -2.29186167e-02 -1.47442778e-02\n",
      "  8.33021198e-03  3.17087484e-03 -4.40165364e-05 -2.78642293e-02\n",
      " -2.84360536e-02 -2.66298086e-02 -8.84414644e-03 -5.68776551e-02\n",
      "  9.23898353e-03  4.49985189e-03 -4.43958729e-02  4.70686491e-02\n",
      "  7.92396214e-03  6.41481045e-04  2.31509402e-02 -2.81634587e-02\n",
      " -6.76673280e-02 -1.10660438e-02 -1.95492331e-02 -4.45242439e-02\n",
      "  5.84300533e-03 -6.81039318e-04 -2.04739410e-02 -4.85128286e-02\n",
      " -1.05299393e-02 -1.11416786e-02  8.29698237e-03 -1.37878963e-02\n",
      " -6.82476625e-02  1.86607402e-02  1.87341852e-02  7.86388731e-03\n",
      "  4.14466459e-02  1.33133643e-02  6.44031755e-03  1.94608261e-02\n",
      "  1.46739021e-02 -2.92030449e-02  4.44814900e-02 -3.48923621e-02\n",
      " -1.65957792e-02 -2.18046800e-03  5.33760274e-03  1.17531437e-02\n",
      "  7.39828412e-03 -8.17383879e-03 -1.99729062e-02 -1.31935660e-02\n",
      " -1.57089743e-02  8.14655932e-03  1.29699206e-02 -9.48150833e-03\n",
      " -3.47310563e-02 -1.59793105e-04 -1.20525470e-02  3.15471175e-02\n",
      "  2.67527586e-02 -3.38954907e-02  3.15771689e-02 -1.13329243e-02\n",
      "  1.66984816e-02  3.20601059e-02 -2.21871004e-02 -3.65544196e-03\n",
      "  5.18018110e-03 -2.52892424e-02 -1.17236730e-02  4.60173744e-02\n",
      " -2.00301935e-02 -4.01392371e-02 -2.63835359e-02  2.78403802e-02\n",
      " -4.65698299e-02 -1.26624541e-02  6.32518368e-03  6.56765811e-03\n",
      " -2.86349608e-02 -3.86223869e-02  3.88333479e-02  2.31483562e-02\n",
      " -2.51907520e-02  1.34557971e-03 -1.72478390e-02  2.57711474e-02\n",
      " -3.17681409e-03 -1.73970916e-02 -3.65586676e-02 -2.34117576e-02\n",
      "  1.44422362e-02  3.62937051e-03  3.42196049e-03  9.96470943e-03\n",
      " -2.03976007e-02 -8.66361060e-03  2.19165023e-02 -1.57697320e-02\n",
      " -8.05000366e-03  3.51646919e-02  1.89770423e-02  3.95238916e-02\n",
      " -2.40717298e-02  2.51785211e-02  1.42072455e-02  2.32885051e-04\n",
      "  1.18936101e-02 -3.44417510e-03 -2.00274203e-02  3.83581850e-02\n",
      " -1.48966632e-02  2.92569282e-02 -4.14257729e-02  4.32812491e-02\n",
      "  3.50552013e-02  5.05858956e-03 -3.08007762e-02  2.19441262e-02\n",
      "  1.55938208e-02 -3.03254696e-02 -6.55550692e-03  1.60692763e-02\n",
      "  4.21221774e-03  1.79539752e-02 -1.39532053e-02  1.30921481e-02\n",
      " -2.45396822e-02  1.72656607e-02  1.66282105e-02 -4.53122823e-02\n",
      "  2.52132457e-02  3.61522191e-02 -2.94170446e-02  1.11092599e-02\n",
      " -9.09438230e-03 -2.09668567e-02  2.65796369e-02 -3.13788802e-02\n",
      " -3.98170193e-02 -1.83650923e-02  2.43148662e-02  7.73590999e-03\n",
      "  3.83846329e-03  4.46614664e-02  1.38107638e-02  3.00887463e-02\n",
      "  3.03710927e-02 -5.46434586e-03 -1.06482764e-02 -1.96621619e-02\n",
      " -1.06019806e-02 -4.00640086e-03 -2.47524784e-03 -3.14054736e-03\n",
      " -3.45124591e-02  6.86037786e-02 -4.68500715e-04  6.33911172e-03\n",
      "  4.50053337e-02 -2.75063324e-02 -4.57630854e-03 -4.25517298e-03\n",
      "  2.89963024e-02 -3.49550836e-03 -3.36186512e-02 -3.57954218e-02\n",
      " -2.82994190e-02  2.30744610e-03  7.68170730e-03  1.69601347e-02\n",
      " -2.20102531e-02 -3.40167030e-02 -3.18121282e-02 -3.40772402e-02\n",
      " -1.54856591e-02 -2.47381223e-02  3.69968567e-02  2.29017592e-02\n",
      " -6.82288560e-03  4.61310269e-03  1.07033120e-02 -1.70124375e-03\n",
      "  1.79976876e-02 -9.69659418e-03  2.49093180e-03 -2.98045481e-02\n",
      "  4.31073795e-02  5.76777633e-03 -7.44532732e-03 -3.85929885e-02\n",
      "  2.16743351e-02  4.35271515e-02  1.91871951e-02  3.84584965e-02\n",
      " -3.91925039e-02  5.02055410e-03 -8.58535530e-03 -1.76825285e-02\n",
      "  3.36909390e-03 -4.18665964e-03  1.35117153e-02  1.55976619e-02\n",
      "  2.22439033e-02  1.58466905e-02  1.77185441e-02 -3.20567673e-03\n",
      "  2.89353194e-02  6.24053185e-02  2.84334082e-02  2.70889987e-02\n",
      " -7.01263698e-03  3.36081685e-02  3.10082483e-02 -1.95848514e-02\n",
      "  2.13054042e-02  2.06564626e-02 -1.18387998e-02  3.64244279e-02\n",
      "  4.33805854e-02  4.08845341e-02 -1.07016872e-02  1.46482285e-02\n",
      " -7.22048957e-02 -2.31204676e-02  1.91005802e-03 -5.48157962e-03\n",
      " -3.45205910e-03  8.04926953e-03  2.65587557e-02  1.03889686e-02\n",
      " -2.04309505e-02  1.67464594e-02 -1.53843225e-02  6.00642476e-03\n",
      " -1.80765654e-02  1.38851590e-02  1.20031260e-02  3.12655287e-02\n",
      " -3.07217758e-02  6.65705404e-02  3.08196224e-02 -2.43189081e-02\n",
      "  4.22239578e-02  1.14664725e-02 -7.92331276e-03 -1.16893918e-02\n",
      "  8.41086420e-03  6.60029983e-03  8.84776142e-03  7.21269535e-03\n",
      "  2.08566096e-02  8.57087771e-03  3.13913391e-02  1.16095718e-03\n",
      " -3.34361505e-02  7.22919813e-03 -4.53424472e-02  3.13880093e-02\n",
      "  1.25651706e-02  2.00153948e-02  1.64852969e-03  2.33094643e-02\n",
      "  1.44251388e-03 -5.79397615e-02 -1.90588407e-03 -1.96417196e-02\n",
      " -2.58844051e-02 -3.38485196e-03  8.69200974e-03  2.78458192e-02\n",
      "  1.57872559e-02 -4.63555224e-02  1.84287706e-02 -2.59320876e-02\n",
      " -1.84329838e-03  2.01428862e-03 -4.20706981e-02  1.20338027e-02\n",
      "  4.72193181e-02  1.87673634e-02  2.89841692e-03  4.11369901e-02\n",
      " -2.48837254e-02  1.72414796e-02  1.79570925e-02  2.18593395e-02\n",
      "  2.91753401e-02 -4.92366781e-03 -8.37595280e-03  5.60734185e-03\n",
      "  1.67871712e-02  1.34270933e-02  1.18733121e-02 -3.67851847e-02\n",
      " -1.11625344e-02]\n",
      "783781\n",
      "training: 34 testing: 23\n",
      "correctsource    24\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.33333333 0.5        0.75       0.75       0.75\n",
      " 0.75      ]\n",
      "Accuracy: 0.5588235294117647\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.66      0.79      0.72        24\n",
      "       missed       0.00      0.00      0.00        10\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        34\n",
      "    macro avg       0.33      0.40      0.36        34\n",
      " weighted avg       0.46      0.56      0.51        34\n",
      "\n",
      "accuracy = 0.6521739130434783\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.70      0.88      0.78        16\n",
      "       missed       0.33      0.14      0.20         7\n",
      "\n",
      "    micro avg       0.65      0.65      0.65        23\n",
      "    macro avg       0.52      0.51      0.49        23\n",
      " weighted avg       0.59      0.65      0.60        23\n",
      "\n",
      "(325,)\n",
      "[-6.84140370e-02  8.83062336e-03  7.46752601e-03 -3.37928362e-02\n",
      "  1.05157505e-02 -3.20561726e-02 -2.75493928e-02  3.63370219e-02\n",
      "  3.00037877e-02 -9.70150000e-03 -2.07843954e-02  4.97397291e-02\n",
      "  1.32427091e-02  6.29620360e-04 -2.00891008e-02 -3.08965760e-02\n",
      " -1.81152068e-02 -1.21991615e-02 -2.55057136e-02 -3.61057894e-03\n",
      "  1.07094874e-02  9.71090399e-03  2.01966690e-02 -3.32018209e-02\n",
      "  4.88883944e-02 -7.50423793e-04  1.03136118e-02  3.77424334e-02\n",
      "  3.45777987e-02  3.53479865e-02 -2.85587364e-02  7.49869578e-04\n",
      " -1.66632347e-02  1.24433110e-02  2.68050128e-02  3.83452336e-02\n",
      "  3.41865390e-02 -2.49010217e-02 -7.54299257e-02  4.09171904e-02\n",
      "  1.08766167e-02  5.97116019e-03  3.39104825e-02  1.34537302e-02\n",
      "  1.71478487e-02  3.79436172e-02  2.79256209e-02 -2.16869232e-02\n",
      " -3.16670348e-02 -2.13938800e-02  5.59013796e-02 -6.49064675e-02\n",
      "  3.02645869e-02 -1.45801043e-02  4.73562464e-02  1.07157198e-02\n",
      "  8.78369245e-03  4.98931725e-02  3.42099976e-02  5.38042460e-02\n",
      " -1.63515484e-02 -2.74561438e-02 -1.05884674e-02  4.07975115e-02\n",
      "  1.75585229e-02 -3.76435157e-02 -3.09526597e-02  9.91904554e-03\n",
      " -2.53824220e-03 -3.79667879e-02 -1.51406304e-02  6.00457169e-03\n",
      "  1.89654416e-02 -5.78997844e-02  1.75199885e-02 -3.25879628e-02\n",
      "  5.80188400e-02  7.29710256e-03  5.00154251e-02  3.10642135e-02\n",
      " -6.11597798e-02  4.87063561e-02 -7.85742893e-03  1.99821007e-02\n",
      " -2.72239692e-02  5.62262952e-03 -2.83991015e-03 -6.40761154e-03\n",
      "  3.90721792e-03 -3.24174627e-02  3.74810283e-02  5.37898231e-02\n",
      " -2.74668114e-03 -5.63453171e-02 -1.80209857e-02 -2.97133315e-02\n",
      " -3.88547515e-02  2.55402200e-02  5.74710711e-02 -2.07511387e-02\n",
      " -3.31881539e-03 -2.02192738e-02 -2.91384603e-02  6.71377528e-02\n",
      "  1.35474708e-02  1.83223077e-02  4.01257534e-03  2.80623384e-02\n",
      " -1.50875457e-02  4.66702937e-03 -3.97235515e-03  3.56485051e-02\n",
      "  1.37937632e-02 -2.00011054e-03  2.30560244e-03  3.30976128e-02\n",
      "  6.95278826e-03  1.25430756e-03 -4.31257309e-02 -3.61377207e-02\n",
      "  5.26662274e-02  2.46778313e-02 -8.48363038e-03  3.33931796e-02\n",
      " -1.49801875e-02 -2.38056752e-02  4.27167439e-02 -9.31447556e-02\n",
      " -8.02901261e-02 -2.32282639e-02 -1.86527710e-02  1.34695303e-02\n",
      "  4.82375333e-03  1.95538911e-02  6.38644337e-03  5.02751155e-02\n",
      " -2.86973669e-02  5.05537344e-03  9.09314045e-03  2.83868831e-02\n",
      " -1.10903761e-02  3.35036846e-02  3.02429524e-02  1.54506579e-03\n",
      "  3.68799089e-02  2.19758952e-02 -4.91588153e-02  3.13432566e-02\n",
      "  7.53840502e-02 -4.48173225e-02 -1.48878857e-02  1.68880987e-02\n",
      " -3.38302417e-03 -1.70172500e-02  4.88277689e-02  1.17867533e-02\n",
      "  7.56636394e-02  2.22784120e-02 -1.61511229e-02 -6.00671181e-02\n",
      "  2.58636929e-02 -1.26527523e-02 -1.71534144e-02  4.95721619e-03\n",
      "  2.68087097e-02 -4.27990876e-02 -1.42429605e-02 -8.58177344e-02\n",
      " -8.09161649e-03  2.98237922e-02  3.44357080e-02 -7.67330960e-03\n",
      "  3.42964856e-02 -6.70178794e-02  2.18954316e-02  2.62400464e-02\n",
      " -2.04963833e-02 -4.57441882e-02 -2.03458523e-02  6.14288346e-05\n",
      "  8.95702372e-03 -1.87811888e-02 -1.55148967e-02 -4.94395440e-02\n",
      " -6.67801656e-02 -1.87340634e-02  2.16674551e-02  7.87879319e-03\n",
      " -1.98282625e-02  6.13461432e-03  1.87967030e-02  1.18395160e-02\n",
      "  2.41771368e-03 -1.56222376e-02  1.33754773e-02 -2.32783531e-03\n",
      " -6.09866060e-02  1.75402091e-03  8.61162146e-04 -1.41327919e-02\n",
      " -3.41396206e-02 -2.43539623e-02 -1.56295599e-02  5.99788151e-03\n",
      " -1.28012660e-02  1.87648087e-04  5.16084194e-02  4.98547422e-03\n",
      "  2.87221845e-02  1.29762305e-02 -1.79530713e-04  3.08074659e-02\n",
      "  1.45929826e-02 -2.43459168e-02 -6.75487829e-02  2.02472107e-02\n",
      " -8.37068326e-03  8.69065295e-03 -1.40372532e-02  3.57650757e-02\n",
      " -8.97845598e-03 -6.57762339e-03 -7.33913885e-03  7.04539316e-03\n",
      " -1.76935919e-02 -3.79329398e-02  3.31572799e-02 -4.73647992e-02\n",
      " -7.71154965e-02 -4.02458677e-02  3.68658440e-02 -1.46754414e-02\n",
      " -6.85527449e-03 -4.31293657e-02  2.78781766e-02 -4.45669635e-02\n",
      "  4.55483953e-02  1.16950953e-02 -1.33597072e-03  4.99478503e-03\n",
      "  7.59013626e-03  1.75863181e-02 -8.28921364e-02  4.93570717e-02\n",
      "  2.32536696e-02 -6.98724735e-03 -3.65006281e-02  1.16651643e-02\n",
      "  3.24351264e-02 -3.51135754e-02 -5.16177685e-02  2.64936328e-02\n",
      "  1.93449030e-02  1.25461411e-02 -1.59482439e-02  6.56135157e-02\n",
      " -1.30310058e-02  2.86152089e-02  1.87252315e-02  1.79760679e-02\n",
      "  8.50312537e-03 -1.48987585e-02  2.27108718e-04 -6.41363046e-02\n",
      " -2.85817932e-02  5.35335216e-03  2.43599481e-02 -2.57333284e-02\n",
      " -3.21311068e-02  1.17746878e-03 -8.85792855e-03  1.41848315e-02\n",
      " -1.04728888e-02  2.84441593e-02 -4.34943612e-02 -3.16546626e-02\n",
      " -4.87167550e-03  1.46033017e-03  1.05404453e-02 -6.30479007e-03\n",
      "  1.57289860e-02  4.46221737e-03 -3.35027079e-02 -2.94982952e-02\n",
      " -3.19914538e-02 -2.28513471e-02  2.78285876e-02  2.82248785e-02\n",
      "  3.22003112e-02 -1.29999874e-02  2.20316953e-02  3.47642470e-03\n",
      " -1.34456247e-02 -4.37602652e-02  4.38608615e-02  4.03078362e-02\n",
      " -2.28038561e-02 -1.79946629e-02  3.54860529e-02 -9.53639771e-03\n",
      "  2.36490132e-02  3.65453736e-02  4.14045329e-03  5.75170379e-02\n",
      " -2.69209082e-02  5.83332414e-03 -1.91526510e-02 -4.73401790e-02\n",
      " -5.35826801e-03 -5.60292158e-03  2.38427026e-02 -1.66432084e-02\n",
      "  1.74561448e-02  2.00999370e-02 -2.42836590e-02  3.70307996e-02\n",
      " -5.46534847e-02 -3.02951234e-02 -1.12897301e-03  4.26073277e-02\n",
      " -2.88542849e-02 -3.49549780e-04 -2.95063390e-02 -9.00099436e-03\n",
      "  2.54816587e-02]\n",
      "884343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 32 testing: 22\n",
      "correctsource    17\n",
      "missed           15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    12\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.2        0.4        0.75       0.5        0.25\n",
      " 0.5       ]\n",
      "Accuracy: 0.40625\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.45      0.53      0.49        17\n",
      "       missed       0.33      0.27      0.30        15\n",
      "\n",
      "    micro avg       0.41      0.41      0.41        32\n",
      "    macro avg       0.39      0.40      0.39        32\n",
      " weighted avg       0.40      0.41      0.40        32\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.54      0.58      0.56        12\n",
      "       missed       0.44      0.40      0.42        10\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        22\n",
      "    macro avg       0.49      0.49      0.49        22\n",
      " weighted avg       0.50      0.50      0.50        22\n",
      "\n",
      "(325,)\n",
      "[ 0.02341615 -0.05955044 -0.00819149  0.00604171 -0.00133015  0.00913419\n",
      "  0.04516187  0.03104949 -0.04319043 -0.00923775 -0.0437379   0.01930983\n",
      " -0.00343941  0.01322551 -0.01720609  0.03428062 -0.01935872 -0.0012188\n",
      "  0.03558171  0.00035625  0.03202574  0.02352547 -0.0211169  -0.03553035\n",
      " -0.03391255  0.01277476 -0.01182711  0.06334767  0.00483689  0.00224686\n",
      " -0.03093704  0.02056384 -0.04913086  0.02285039  0.02758363 -0.0109699\n",
      " -0.03245151  0.05084728  0.01276782  0.02580497  0.00093523 -0.06759496\n",
      "  0.01192686 -0.02888532  0.01419214  0.03252678 -0.01136688  0.0014432\n",
      " -0.01623093 -0.00373941  0.00646154 -0.02689444  0.06335782  0.00241301\n",
      "  0.03015477 -0.01530058  0.07330964  0.00956372  0.03836873  0.03459473\n",
      "  0.01467879  0.02769263  0.03392312  0.04419514 -0.00272668 -0.05515192\n",
      "  0.00118262  0.00866954  0.01363789 -0.06134523 -0.04080127  0.01515606\n",
      " -0.01397767  0.01379973  0.04545456  0.00091636 -0.05118371 -0.00359203\n",
      " -0.00506691 -0.0221882  -0.01470835 -0.02563433 -0.01722399 -0.02724843\n",
      " -0.01321776 -0.0052711   0.00455026  0.00309743  0.01987468  0.0157877\n",
      " -0.03896003  0.05661949 -0.02229521 -0.04510766  0.01074197  0.00021291\n",
      " -0.02737938  0.03527694 -0.00767705 -0.04408176 -0.01076646 -0.01678698\n",
      " -0.04152723 -0.00884233  0.02905823  0.03158099  0.12098085  0.02400628\n",
      "  0.00304056 -0.00223831  0.02146281  0.0259029   0.01232208  0.01184594\n",
      "  0.05809095 -0.02602789 -0.01450344 -0.01396037 -0.02595911 -0.04596421\n",
      " -0.00996428  0.00439685 -0.01640566  0.01823144  0.0051544  -0.05704044\n",
      "  0.00271344 -0.01244447 -0.00135312  0.01358714  0.02914732 -0.02152482\n",
      "  0.01454015  0.02455206  0.02140605  0.02004998 -0.03052639 -0.00621155\n",
      " -0.0132468  -0.0455243  -0.00241742  0.01347648 -0.01429973  0.00599169\n",
      " -0.04570152  0.02266669 -0.03739165 -0.02693966  0.01023499 -0.00382263\n",
      " -0.00373885 -0.04543206  0.03198301  0.01407603  0.01430447 -0.01278547\n",
      "  0.05347221 -0.00263464 -0.05403787 -0.04129443 -0.04781266  0.02155415\n",
      " -0.0324451   0.02590133 -0.01311685 -0.0015155   0.0166057  -0.03065021\n",
      "  0.01726233 -0.02883717  0.00314771 -0.00129231  0.00583323 -0.03834282\n",
      " -0.0151612   0.0159026  -0.00097458  0.02540942 -0.01008894  0.01392891\n",
      "  0.01560676 -0.02674041  0.01814562 -0.04259564  0.01031263  0.02445488\n",
      " -0.00376826 -0.0146816  -0.04585605  0.01055161 -0.01045979 -0.0262957\n",
      " -0.04906899  0.00109985  0.00929976  0.00998825  0.006778    0.02014147\n",
      " -0.0182988   0.01950985  0.00595306  0.01645962  0.01513306 -0.02006112\n",
      "  0.00213311  0.0309305  -0.03002433  0.01788088  0.0021395  -0.03477766\n",
      " -0.02949394 -0.01354723 -0.01056655  0.01237444  0.02973819 -0.00049337\n",
      "  0.00436518  0.02880934 -0.04018368  0.00287261 -0.01211243  0.00585022\n",
      " -0.03901351  0.04108632 -0.02450925  0.01148335  0.01215661 -0.00071583\n",
      " -0.01882523  0.01203115 -0.00837945  0.02462302  0.02207266  0.04525278\n",
      "  0.02118788  0.02163156  0.00967844  0.0188097   0.01220516  0.0190568\n",
      " -0.03612848 -0.01944967  0.00931776  0.01532459 -0.01229785  0.02064786\n",
      "  0.03288483  0.00634735  0.00330108 -0.04312344  0.03093014 -0.00027679\n",
      "  0.02582074  0.03239365  0.0329048  -0.0242016  -0.01253789  0.00462928\n",
      " -0.05033293 -0.0625802  -0.01067737  0.01572238  0.01730102  0.01354655\n",
      " -0.04528149  0.00243738  0.04871103 -0.02056409 -0.02013419 -0.06834915\n",
      " -0.00684148 -0.00499772  0.01594118 -0.00306783  0.03070359 -0.04657969\n",
      " -0.03827864  0.00694294 -0.01082998 -0.04872895 -0.0161379  -0.00856273\n",
      "  0.03369192  0.09191303  0.03727496  0.04573101 -0.02424504 -0.03344505\n",
      "  0.01464918 -0.01493461  0.02163     0.01654384  0.03654494 -0.04029385\n",
      " -0.02453662  0.04625869 -0.02549437 -0.02678603  0.03515946 -0.02338047\n",
      "  0.02765319  0.01197981  0.00491486  0.00574066 -0.0254461   0.0195241\n",
      " -0.00060213 -0.0062576   0.02162625  0.00948614  0.00095677 -0.00934779\n",
      " -0.02354455  0.00578448  0.01370352 -0.00032917 -0.03053086 -0.0036618\n",
      " -0.0375088   0.04995216  0.00336858 -0.01266431  0.03883935 -0.00195744\n",
      "  0.02340123]\n",
      "886007\n",
      "training: 39 testing: 26\n",
      "correctsource    27\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    18\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.66666667 0.5        0.33333333 0.66666667 0.6\n",
      " 0.75      ]\n",
      "Accuracy: 0.5897435897435898\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.69      0.74      0.71        27\n",
      "       missed       0.30      0.25      0.27        12\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        39\n",
      "    macro avg       0.49      0.50      0.49        39\n",
      " weighted avg       0.57      0.59      0.58        39\n",
      "\n",
      "accuracy = 0.6538461538461539\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.71      0.83      0.77        18\n",
      "       missed       0.40      0.25      0.31         8\n",
      "\n",
      "    micro avg       0.65      0.65      0.65        26\n",
      "    macro avg       0.56      0.54      0.54        26\n",
      " weighted avg       0.62      0.65      0.63        26\n",
      "\n",
      "(325,)\n",
      "[ 7.20357257e-02 -1.65377604e-02 -4.91006102e-03  6.85814195e-02\n",
      "  1.23801855e-02 -3.16440952e-02  3.86772588e-02 -1.00413782e-02\n",
      "  1.95538150e-03 -3.54022624e-02  6.57861887e-02 -6.78839490e-02\n",
      " -5.00716381e-03  5.07490543e-03 -3.07250884e-02 -4.69170057e-03\n",
      " -6.13046839e-03  7.21465838e-03 -2.71923073e-02  9.66536065e-05\n",
      " -8.10317804e-03  1.83901118e-03 -3.30194595e-02  4.02551613e-02\n",
      "  2.03508765e-02  3.33862063e-02 -4.43660013e-02 -5.41957270e-03\n",
      " -9.71916448e-03 -6.60236713e-03  7.07064929e-02  4.05921716e-02\n",
      "  1.54198217e-02  4.38537924e-02  2.81394764e-03  6.45506296e-02\n",
      " -4.56920367e-03  7.29646769e-03  5.38877349e-02  1.76036892e-02\n",
      " -2.51590272e-02  6.87820550e-02  4.55684665e-03 -3.06630873e-02\n",
      "  1.59109860e-02  2.70789129e-02 -2.86924209e-02  1.57500067e-02\n",
      " -3.20519825e-03 -3.28030793e-02 -2.12421087e-02 -5.68767730e-02\n",
      "  6.39810449e-02 -2.82411131e-03 -1.48971786e-02 -1.38600311e-02\n",
      " -6.12272081e-03 -4.30583196e-02 -2.47761269e-02  2.49252789e-02\n",
      "  5.59582667e-02  3.01897341e-02  9.41716800e-02 -1.32571222e-02\n",
      "  0.00000000e+00 -1.05632882e-02  8.06388605e-02 -5.48833191e-02\n",
      " -3.08394657e-02 -2.80186851e-02  5.34425464e-02 -1.88164713e-02\n",
      " -2.69790555e-02  5.22019066e-02 -4.50744558e-02 -4.99472643e-03\n",
      " -8.92929459e-03 -1.35588139e-02  3.91076922e-02  1.61486278e-03\n",
      "  3.43478782e-02  7.73654596e-03  4.11395739e-02 -1.10570769e-02\n",
      " -2.14079527e-02  2.06389185e-02 -1.44213690e-02 -7.39587434e-03\n",
      "  3.14317011e-03 -1.80119096e-02 -2.79697579e-02 -1.63110142e-02\n",
      " -5.46171849e-02  1.07000297e-01  3.30971623e-03  3.44019898e-02\n",
      " -3.23503126e-02  9.45196125e-03 -4.43967440e-02 -1.44324271e-02\n",
      "  1.51465152e-02  3.29242049e-02 -2.94442199e-03 -2.41262691e-02\n",
      " -2.49107273e-02 -1.61197821e-02 -4.71112172e-02 -1.35589616e-02\n",
      "  9.44749395e-02  2.81465784e-02  1.06207945e-02  1.48827636e-02\n",
      " -2.65580309e-02  2.09347501e-02 -1.91728156e-02 -1.89324625e-02\n",
      " -3.21218751e-02 -2.76759145e-02 -1.69204172e-02  4.70164892e-02\n",
      " -2.23248227e-02  4.09956528e-03  4.99348122e-02  6.43077415e-02\n",
      " -3.67355352e-02 -2.32128197e-02  3.86752520e-02 -1.18465582e-02\n",
      " -5.74931003e-02  2.33981605e-02  2.16273603e-02  5.13974229e-03\n",
      " -1.31957756e-02  2.69926592e-02 -2.14418804e-02  3.37283271e-02\n",
      " -3.25817273e-02  3.45864671e-03  9.92322230e-03 -1.27277703e-02\n",
      " -2.02823592e-02 -1.09417230e-02 -5.12446353e-02  3.04852083e-02\n",
      "  4.25544897e-02 -1.52019691e-02 -2.55217848e-02  2.79215621e-02\n",
      " -2.42157757e-02  1.78911249e-02 -7.22647726e-05 -2.27733992e-02\n",
      " -1.31610020e-03 -6.36490429e-02  1.99533143e-02 -3.56131167e-02\n",
      "  4.39024128e-02 -4.63749938e-02  4.67087335e-03  1.85202622e-02\n",
      " -7.64231887e-02 -5.32843697e-02  1.08514159e-02  1.70538809e-02\n",
      " -5.08411261e-02  1.27505407e-04  8.31226604e-02 -6.87244231e-03\n",
      " -5.63701825e-03  3.00615036e-03 -3.85738628e-02 -5.32271590e-02\n",
      " -1.89176239e-02  9.60073099e-03 -9.84266890e-05  0.00000000e+00\n",
      " -6.58699372e-02 -2.03365474e-02  4.01128177e-02  3.39218887e-02\n",
      "  3.53323326e-02 -5.07127687e-03 -9.64837552e-03  3.37599861e-03\n",
      "  1.47338304e-02  1.29867360e-02  1.74741518e-02 -4.46010106e-02\n",
      "  3.87695818e-02 -1.04602604e-02  2.40420270e-03  7.74843408e-02\n",
      " -3.36927713e-02  6.37016080e-03 -3.37412335e-02  2.04384647e-02\n",
      " -4.23115320e-02  6.72347928e-02 -9.27619127e-03 -9.02302720e-03\n",
      "  3.53401353e-03 -1.64120419e-02  3.88237943e-02  1.87773464e-02\n",
      " -1.07131873e-02 -5.17784480e-03  5.70100270e-02  1.84186505e-03\n",
      " -4.07041253e-02  2.55596398e-02  1.68993017e-02 -3.13385355e-03\n",
      "  3.41916196e-02 -3.13762005e-02  4.08423408e-03  4.37607270e-02\n",
      "  3.38590897e-02 -9.74053051e-03 -1.59711944e-02  1.23765710e-02\n",
      "  5.40803850e-04  2.44380135e-03  1.89624704e-02 -4.28254359e-02\n",
      " -4.07392612e-02 -2.39734846e-02  6.50267941e-02  1.00586366e-02\n",
      "  5.13317231e-02  7.89445503e-03 -1.14357908e-02 -3.33696978e-02\n",
      "  1.82252341e-02 -5.86745436e-03  4.25478123e-02 -3.26002783e-02\n",
      " -3.05026666e-02 -2.91160368e-02 -5.89404093e-02  3.05057536e-02\n",
      " -8.18351265e-03  3.43747369e-02  1.98485399e-02 -3.16151316e-02\n",
      "  4.91132918e-02 -2.77966175e-02  1.65241627e-02 -3.90896501e-02\n",
      " -1.60324719e-02  3.60745991e-02  6.56947623e-03 -3.53306203e-02\n",
      "  4.19420330e-02  3.80103035e-02 -3.46271737e-02  2.45510613e-03\n",
      "  2.04362632e-02  5.88351590e-02 -2.13732097e-02 -2.39592741e-02\n",
      " -2.33905303e-02  0.00000000e+00  1.34830612e-04 -2.19942232e-02\n",
      " -8.37678038e-03 -9.70031723e-03 -1.15493296e-02 -2.62639753e-02\n",
      "  2.38738646e-02  1.94463266e-02  7.24261355e-02  5.18832152e-03\n",
      "  1.17676435e-02  2.58481920e-02 -2.12995239e-02 -1.73031631e-02\n",
      " -1.20754754e-02 -2.59593119e-02  1.78721692e-02  4.63745020e-02\n",
      " -1.05914977e-02 -6.34382474e-02  7.81963299e-02  8.43003040e-03\n",
      "  9.75068179e-03 -4.01019570e-03 -1.96083525e-02  8.70453058e-02\n",
      "  4.84180115e-02 -1.11939563e-02 -2.35203084e-02 -5.31761352e-03\n",
      " -3.55889547e-02 -4.65024416e-03 -7.85680602e-03 -1.13956974e-02\n",
      "  1.98821796e-02  1.51487465e-02  9.68954527e-03  1.05590205e-02\n",
      " -4.23672958e-03  3.20287423e-02 -2.76405380e-02 -3.47365795e-02\n",
      " -3.47549721e-02  1.41630329e-02  7.42986673e-02  4.80793996e-03\n",
      " -2.39252377e-02 -3.98453074e-02 -1.57367673e-02  2.31704558e-02\n",
      " -1.36664641e-02 -1.13401499e-02  2.05144271e-02  2.24337303e-02\n",
      "  6.80227929e-04 -2.15048147e-02  3.53412602e-02 -2.27753344e-02\n",
      " -2.92711251e-02 -1.49448827e-02  1.59414272e-02 -6.01240311e-02\n",
      "  4.62487355e-03]\n",
      "936730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 testing: 24\n",
      "missed           18\n",
      "correctsource    18\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           12\n",
      "correctsource    12\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.33333333 0.66666667 0.33333333 0.5        0.75\n",
      " 0.25      ]\n",
      "Accuracy: 0.4722222222222222\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.47      0.50      0.49        18\n",
      "       missed       0.47      0.44      0.46        18\n",
      "\n",
      "    micro avg       0.47      0.47      0.47        36\n",
      "    macro avg       0.47      0.47      0.47        36\n",
      " weighted avg       0.47      0.47      0.47        36\n",
      "\n",
      "accuracy = 0.5416666666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.55      0.50      0.52        12\n",
      "       missed       0.54      0.58      0.56        12\n",
      "\n",
      "    micro avg       0.54      0.54      0.54        24\n",
      "    macro avg       0.54      0.54      0.54        24\n",
      " weighted avg       0.54      0.54      0.54        24\n",
      "\n",
      "(325,)\n",
      "[ 2.12825250e-02 -3.35219998e-02  1.78671188e-03 -8.12890487e-02\n",
      " -8.49806155e-02 -2.79771052e-01 -9.28893484e-02  3.19552185e-02\n",
      "  1.12481162e-02 -3.41485879e-02  2.99251677e-02 -8.20281783e-02\n",
      "  2.62823052e-02  7.44820990e-03  1.17515471e-01  2.44426143e-02\n",
      "  2.24200573e-01  2.96294027e-02 -8.74211045e-02  5.17830308e-02\n",
      " -5.79181261e-02  6.28861591e-02  7.63425064e-02  7.73151743e-02\n",
      " -3.23571910e-02 -5.80722241e-02 -1.19181948e-02  1.21633441e-01\n",
      " -1.37243302e-01  2.46087709e-02  2.71422142e-02 -1.45981108e-02\n",
      "  5.41608982e-02 -2.18421506e-02 -8.81222058e-02 -1.46803116e-02\n",
      " -1.13191937e-01  5.13803274e-02  8.20122477e-01 -1.05293124e-01\n",
      " -2.85268158e-02  7.88597301e-02  9.98929426e-02  1.29471439e-03\n",
      " -9.32514343e-02 -1.92524307e-01 -2.24981559e-02 -3.70336825e-01\n",
      " -1.14248642e-01 -5.29210906e-01 -4.76218559e-02  1.60544510e-01\n",
      " -1.36477292e-02  4.26245874e-02 -2.51860908e-02 -2.37143266e-02\n",
      " -1.16206739e-01  2.51549746e-02 -3.42655697e-01 -4.49245099e-02\n",
      " -4.58944291e-02  9.32532621e-03  2.85193467e-02  2.18111635e-02\n",
      " -3.36046727e-02  2.06247474e-02 -1.57219229e-02 -3.00777723e-02\n",
      " -3.09494950e-02 -4.82743083e-02 -3.17407749e-02  2.70945112e-02\n",
      " -3.63455271e-02  6.03450875e-02  6.06890211e-03 -4.16922175e-02\n",
      "  5.82730887e-01 -3.65925849e-02 -2.93196668e-01 -1.51127293e-02\n",
      "  7.75142921e-03 -1.97996368e-02 -1.24084682e-02 -8.11391052e-02\n",
      " -1.08245911e-03 -7.91018559e-02 -3.28373583e-03  2.45225750e-01\n",
      " -1.55915573e-02 -2.27136285e-02  6.61872258e-02  1.65149609e-03\n",
      " -1.15743895e-01 -3.64799954e-01 -2.00442707e-02  7.94720843e-02\n",
      " -1.94940606e-01 -1.19469526e-02  5.73237694e-02  3.46405616e-03\n",
      "  5.75792126e-02  3.42258695e-01 -5.50377663e-03  2.97707291e-02\n",
      " -5.72328048e-01 -2.81038058e-03 -2.85748440e-02 -1.07022236e-02\n",
      " -3.64897127e-02  2.71476783e-02 -1.49311949e-02  1.15203361e-02\n",
      "  1.10676541e-01  2.04353425e-03  7.38435316e-02  1.41309248e-01\n",
      "  2.85274221e-01 -1.02020679e-01 -2.84373221e-01 -3.63689747e-02\n",
      "  2.06288822e-01  3.80672800e-02  2.24808384e-02 -2.61999629e-02\n",
      " -1.30965695e-01  2.39750252e-03  5.02375945e-02 -3.14985320e-01\n",
      "  1.34770572e-02 -2.05170386e-02 -4.63904076e-02  7.69649291e-02\n",
      " -6.52236384e-01 -3.24030595e-01 -2.44090177e-01  2.46242375e-02\n",
      " -1.17925650e-01 -6.30824220e-02 -9.04193436e-02  3.26616681e-02\n",
      " -1.43530352e-02 -8.83936019e-03  2.76905655e-02 -3.17752728e-02\n",
      "  1.55935377e-02  3.52854246e-01  9.33239750e-02 -4.51816511e-02\n",
      "  1.35473947e-03  6.91157060e-02 -8.24292369e-02  3.74356328e-02\n",
      " -5.21219057e-01  3.94687998e-02 -1.25331522e-02 -3.32713482e-02\n",
      "  2.41328425e-02 -2.53469205e-02 -1.42101949e-02  1.76506136e-01\n",
      "  6.43045320e-02 -9.31117292e-02 -5.49232300e-02  2.06940968e-02\n",
      "  1.96751783e-02 -4.29631001e-02 -1.81414679e-02 -1.56234377e-01\n",
      " -1.85515802e-01 -8.09805619e-02 -2.65252377e-03  2.07442418e-02\n",
      "  3.53595831e-02  2.67006076e-02 -1.44245253e-01  4.94237317e-03\n",
      "  2.81806132e-02 -5.44634921e-02 -2.74027096e-02  1.49584388e-01\n",
      "  1.32499440e-01 -5.60799917e-02  2.74077886e-01 -5.46126288e-02\n",
      " -1.69837228e-01 -1.79891848e-01  3.35050074e-03  5.54381080e-03\n",
      "  3.90079322e-03 -5.38635057e-02  2.49042367e-02  2.30959366e-02\n",
      " -3.12502256e-02 -8.05280993e-01  3.86336505e-01 -2.89735516e-04\n",
      " -3.34496240e-01  1.02702143e+00  1.80061067e-01 -1.84629136e-01\n",
      " -2.36480887e-01 -3.61141781e-01 -1.01612581e-01  1.05140688e-02\n",
      " -4.18782948e-02 -2.74847895e-01 -8.63571358e-02  1.58480371e-01\n",
      "  1.04242773e-01 -5.36094474e-02 -1.67729000e-02 -3.00318561e-02\n",
      "  2.61350603e-02  3.22439664e-02  5.47065093e-02 -4.83180587e-03\n",
      " -2.28668928e-02  6.87694572e-02  2.80475245e-02  3.70870722e-02\n",
      "  7.99705698e-01 -1.23794585e-01 -2.51069350e-01 -7.15123204e-02\n",
      " -5.92405728e-02 -2.85052273e-02  2.73194196e-01  3.48469910e-01\n",
      " -1.47225941e-02 -7.14594106e-01 -2.25930557e-02 -3.84330780e-02\n",
      "  2.88591959e-02  3.28368774e-02 -1.30045014e-01 -3.76641731e-02\n",
      " -1.89405549e-03  3.28373938e-01 -2.01936766e-02  1.35228651e-01\n",
      "  6.56944657e-03  2.92382184e-01  2.89369880e-04 -2.66369796e-01\n",
      "  6.10389178e-02 -2.93050223e-02  4.67706177e-02  8.28853215e-02\n",
      " -4.25236865e-03  1.84992213e-01 -7.28733298e-03 -8.33827704e-02\n",
      " -3.26577128e-01 -6.30254577e-02  1.15528530e-02 -2.07077649e-01\n",
      "  2.97273308e-02  9.72686954e-02 -9.30557069e-03  3.37371464e-02\n",
      "  1.83186742e-01 -5.84855219e-02 -2.91273654e-02 -6.08945960e-02\n",
      " -4.87071387e-02  2.52450477e-02  1.43260497e-01  1.34806419e+00\n",
      "  3.17737426e-02  4.29413022e-02 -5.92797055e-01  3.54297550e-02\n",
      " -3.33907582e-02  1.96607643e-02  1.48092408e-01  6.60241415e-02\n",
      " -1.79497562e-01 -3.61137478e-03 -1.14941654e-02  1.60743601e-01\n",
      "  2.32024199e-01  2.50042768e-02  5.80733657e-03 -2.40444059e-02\n",
      " -3.66913995e-02  6.78794528e-02 -3.29336332e-01 -3.85862403e-02\n",
      "  1.81709162e-02 -4.59787457e-02  2.85662621e-01  2.88727569e-02\n",
      "  5.38171517e-01 -2.66323015e-01 -1.88769418e-01 -1.13647092e+00\n",
      "  2.92099594e-02  2.60777758e-01  1.55527671e-02 -1.23692536e-02\n",
      "  1.00844734e-01 -3.46638971e-02  6.00365144e-02 -3.07858603e-02\n",
      "  6.56461580e-02 -2.58807834e-02 -9.09009817e-02  1.18419331e-02\n",
      "  2.62845573e-01  3.99690851e-02 -9.36916078e-03  1.54798958e-02\n",
      "  2.29687553e-03  2.27075296e-01 -1.12344261e-02 -1.07382175e-01\n",
      " -1.72580147e-02 -2.28810721e-01  1.65895148e-02  2.56425198e-02\n",
      "  4.64881839e-02  2.50781088e-02 -9.25087629e-02  8.91731493e-02\n",
      " -6.34377042e-02]\n",
      "956130\n",
      "training: 38 testing: 26\n",
      "missed           20\n",
      "correctsource    18\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           13\n",
      "correctsource    13\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.16666667 0.5        0.33333333 0.4        0.6\n",
      " 0.5       ]\n",
      "Accuracy: 0.42105263157894735\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.38      0.33      0.35        18\n",
      "       missed       0.45      0.50      0.48        20\n",
      "\n",
      "    micro avg       0.42      0.42      0.42        38\n",
      "    macro avg       0.41      0.42      0.41        38\n",
      " weighted avg       0.42      0.42      0.42        38\n",
      "\n",
      "accuracy = 0.6538461538461539\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.83      0.38      0.53        13\n",
      "       missed       0.60      0.92      0.73        13\n",
      "\n",
      "    micro avg       0.65      0.65      0.65        26\n",
      "    macro avg       0.72      0.65      0.63        26\n",
      " weighted avg       0.72      0.65      0.63        26\n",
      "\n",
      "(325,)\n",
      "[-0.01205765 -0.06028062 -0.02180071  0.04847338 -0.02685292 -0.10300965\n",
      "  0.02681794  0.01477321 -0.01523304 -0.01998246 -0.04142139 -0.03141979\n",
      " -0.03467234 -0.04042714 -0.02343811  0.03300454  0.01614634 -0.00152005\n",
      "  0.00687327 -0.01880663 -0.01751592 -0.02841905  0.01309915  0.01432902\n",
      "  0.01522396 -0.06867288 -0.00879243 -0.00125872  0.00872066 -0.07345738\n",
      " -0.09310423 -0.03202579 -0.03943346 -0.01905617 -0.01017622 -0.01560059\n",
      "  0.07928652 -0.00859956 -0.03089792 -0.01865692  0.01690669 -0.01275224\n",
      " -0.04734583 -0.02244889 -0.03479956 -0.0057908  -0.09824495  0.0317954\n",
      "  0.01790272  0.01830785  0.01760095 -0.00222628  0.0089644  -0.04790491\n",
      "  0.02101449 -0.05687233  0.04923788 -0.01747592  0.08462018 -0.01228371\n",
      " -0.01226148  0.00560843 -0.01969014  0.00165977  0.00106176 -0.00440599\n",
      " -0.04671442  0.06143469 -0.03682855 -0.00593413  0.05765731 -0.03901423\n",
      "  0.02406252 -0.00816171 -0.06048235  0.04707926 -0.08277525  0.04055478\n",
      "  0.03630115 -0.0061444  -0.03929372 -0.00795277  0.1072334  -0.02736616\n",
      "  0.03571102  0.03130641  0.02790301  0.00722327  0.00356889  0.02635573\n",
      " -0.07148628 -0.0219164   0.00202553  0.03824277  0.02554723 -0.03392607\n",
      " -0.00960319 -0.03184468  0.07583791  0.09885036  0.00257219 -0.11789432\n",
      " -0.01297223 -0.00464442  0.06254043  0.04411926 -0.00238407 -0.04269949\n",
      "  0.02061566  0.02066647 -0.03564169 -0.0089312   0.00776479  0.03960979\n",
      " -0.06539665  0.01049331  0.02329899  0.06948327 -0.01850427 -0.03896189\n",
      "  0.06328386 -0.0259378  -0.00069111  0.01486555  0.04636336 -0.00378189\n",
      " -0.02129799  0.00768809 -0.02816682 -0.01419455 -0.04309968  0.01924814\n",
      "  0.04863061 -0.01796889  0.00452711 -0.02854411  0.07368085  0.00843626\n",
      " -0.12087752 -0.00089381 -0.01377213  0.00233379  0.00112655  0.00785941\n",
      "  0.03987415  0.01589328  0.0337204   0.0475884  -0.04817757 -0.0086729\n",
      "  0.0155175  -0.02259274 -0.00158486  0.01276695 -0.0147249  -0.02458562\n",
      " -0.04365    -0.04296307 -0.03246019  0.07207763  0.05298753  0.03298894\n",
      " -0.0375086   0.0128214  -0.02442607 -0.06429891  0.01025264  0.04034415\n",
      "  0.0059081   0.01418104  0.05121446 -0.02702268  0.02039216  0.01554487\n",
      " -0.09092883  0.05691846 -0.00298006  0.0477137  -0.00077556 -0.028291\n",
      "  0.00428349 -0.02968484 -0.04619715 -0.04612804  0.01154249 -0.02248099\n",
      " -0.01184649 -0.03160563  0.05109003  0.02590927  0.04155211  0.03085392\n",
      " -0.05182764  0.01771437 -0.0499619   0.02045103 -0.01731641  0.04742191\n",
      "  0.09122481  0.00230898  0.0214476  -0.05736184  0.04000734 -0.00699929\n",
      " -0.04182953 -0.0250651  -0.01898822 -0.01534374  0.02094297  0.04784942\n",
      " -0.04726328 -0.00572936 -0.04283659 -0.07095562 -0.06194747  0.01695089\n",
      " -0.01950306  0.04505858 -0.02651258 -0.0145329  -0.00717397 -0.05110865\n",
      " -0.00424802  0.00837072 -0.0507148  -0.03845629  0.03785902 -0.0393922\n",
      "  0.03237439  0.03612133 -0.00893759  0.02518861  0.0140136  -0.06798887\n",
      "  0.04654736  0.03332084 -0.01993439  0.01153472  0.05038963  0.07152536\n",
      " -0.00388488  0.06564299  0.07766527 -0.0047941   0.07133416  0.06674403\n",
      " -0.01881216 -0.01056452  0.00679767 -0.03168484 -0.03852013  0.01775383\n",
      "  0.01869295 -0.04270714  0.04011746  0.02498676  0.04975991  0.00869173\n",
      "  0.06338319  0.05141279 -0.02397139  0.02778129 -0.08851722 -0.01967952\n",
      " -0.0476523  -0.02236181  0.02171708 -0.00282257  0.03555068 -0.03278949\n",
      "  0.01302049  0.0080961   0.02000575  0.05689794 -0.0138533  -0.00066616\n",
      " -0.07292352 -0.01108567  0.02912823 -0.0527701   0.03693252  0.04758117\n",
      " -0.00820399 -0.04261317  0.01706014 -0.01793134 -0.01129169 -0.01405195\n",
      "  0.0069057   0.01561236  0.01363589  0.01019971  0.0671459  -0.00311958\n",
      " -0.03476241 -0.00416795 -0.08258425 -0.00145948  0.06522926 -0.00141116\n",
      " -0.05781047 -0.02282387  0.00916298  0.00424183 -0.04983069 -0.00130561\n",
      "  0.04929178 -0.01874377 -0.00933767  0.00467256  0.00250296  0.01280789\n",
      "  0.03525075 -0.04513659 -0.00726427 -0.04965467 -0.0615888  -0.06466254\n",
      "  0.04794381  0.02001066 -0.00127293 -0.01391295  0.02603    -0.03034225\n",
      " -0.05824298]\n",
      "983291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 40 testing: 28\n",
      "correctsource    29\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    21\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.57142857 0.33333333 0.66666667 0.83333333 0.4        0.8\n",
      " 0.6       ]\n",
      "Accuracy: 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.72      0.72      0.72        29\n",
      "       missed       0.27      0.27      0.27        11\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        40\n",
      "    macro avg       0.50      0.50      0.50        40\n",
      " weighted avg       0.60      0.60      0.60        40\n",
      "\n",
      "accuracy = 0.75\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.79      0.90      0.84        21\n",
      "       missed       0.50      0.29      0.36         7\n",
      "\n",
      "    micro avg       0.75      0.75      0.75        28\n",
      "    macro avg       0.65      0.60      0.60        28\n",
      " weighted avg       0.72      0.75      0.72        28\n",
      "\n",
      "(325,)\n",
      "[ 2.27626083e-02 -4.93255642e-02 -6.13881121e-03 -4.25000192e-02\n",
      " -1.08340461e-02 -2.79859060e-02  2.02123199e-02  1.24451202e-02\n",
      "  2.15255301e-02 -9.07100152e-03  7.73702530e-03 -1.89148136e-02\n",
      " -6.08057130e-03 -5.69621488e-03  7.82068140e-03  1.31032157e-05\n",
      " -1.39643632e-02 -4.09955793e-02  6.05299111e-02 -2.03450146e-02\n",
      " -1.56955080e-02  4.42125440e-02 -1.27479741e-02 -2.81854435e-02\n",
      "  2.30048708e-02  1.38430698e-02  9.77539704e-03 -1.85299717e-02\n",
      "  1.88206795e-02  3.96510562e-02 -3.36007498e-03 -2.84858697e-02\n",
      " -8.30791305e-03 -2.55134831e-02 -4.01403915e-02 -1.73070551e-02\n",
      "  6.96428947e-03 -1.02353482e-03 -1.13619941e-02 -3.23277065e-02\n",
      " -3.65379369e-02 -5.77376937e-02  6.64883278e-03 -2.99173332e-02\n",
      "  3.83552868e-02  2.42824423e-02 -4.67113559e-02  1.25760593e-02\n",
      " -5.30065251e-02 -2.32065340e-02 -1.61030276e-02 -3.69532889e-02\n",
      "  1.10295399e-02 -3.12082971e-03  1.53357536e-02  4.56225442e-02\n",
      " -4.92166182e-03 -2.80771866e-02 -4.14544862e-02 -2.49272910e-02\n",
      "  1.50208879e-02  2.40875135e-02 -1.35060273e-02  1.68315131e-03\n",
      " -9.42629639e-02  7.16401334e-03 -8.98934688e-03 -3.58119154e-02\n",
      "  2.10968732e-02 -3.79945166e-02 -1.30567904e-02 -5.47670166e-02\n",
      "  6.40221187e-02 -1.04753185e-03 -1.51821521e-03 -2.33017490e-02\n",
      " -3.94245945e-02  3.39297393e-02  5.98445256e-02 -6.55720045e-02\n",
      "  7.06899874e-02 -3.70072069e-02  3.84773094e-02  2.47049708e-03\n",
      " -6.12857370e-03 -6.33154183e-03 -5.69725634e-02  7.36594322e-03\n",
      " -3.14160173e-02 -4.15201536e-02 -3.25470685e-02 -7.20583568e-02\n",
      " -2.65057244e-02 -3.71326657e-02  3.13982348e-02  2.00369626e-02\n",
      "  4.24711547e-02  9.90008136e-03 -2.84577772e-02 -2.21551380e-02\n",
      " -5.09278776e-03 -1.12248220e-02 -5.37856872e-02 -3.00322400e-02\n",
      " -5.76283132e-03 -5.78055686e-02  2.50888358e-02 -5.37701814e-02\n",
      "  3.52239176e-02 -1.40472451e-02 -1.96964733e-02 -1.53850124e-02\n",
      " -9.71696976e-03 -1.52200842e-02 -2.96047923e-02  3.62296027e-03\n",
      "  9.20450259e-04  2.50898106e-02  9.63364267e-03  1.39314689e-02\n",
      " -2.61342178e-02 -1.21113896e-02  1.85122276e-02  1.04275774e-02\n",
      "  2.74668875e-02 -4.42220548e-02  4.07126239e-02 -1.75433206e-02\n",
      " -4.40032909e-03 -1.52254952e-02 -3.10072135e-02  4.21576021e-03\n",
      " -1.06920309e-02 -4.61376074e-02  3.96352245e-03  3.56744007e-02\n",
      " -4.75676427e-02  4.07058778e-02  5.68782309e-02 -9.18651832e-03\n",
      " -6.35650324e-02  1.69465448e-02  2.60665565e-03 -1.27985345e-03\n",
      " -1.52045052e-02  7.98476572e-03 -3.74529422e-03 -6.68438748e-02\n",
      " -1.13298324e-02 -6.71605225e-03 -3.85845717e-02 -2.77622686e-02\n",
      "  3.85830700e-02 -4.62232770e-03  5.33541313e-03  3.68635394e-02\n",
      " -3.74481645e-02 -3.43095287e-02 -5.31669199e-02  9.97998863e-03\n",
      "  5.75144192e-02 -9.39753451e-03  2.96633215e-02  3.64048960e-02\n",
      " -4.08650698e-02 -2.14428763e-02 -1.54772308e-04  3.48462829e-02\n",
      " -2.25175549e-02 -4.64063966e-02 -1.34949451e-02  3.73605541e-02\n",
      " -1.80344821e-02 -1.13286639e-02  2.32836033e-02 -1.91816709e-02\n",
      " -4.73521417e-02 -3.96505718e-02 -2.80618118e-02  4.00738289e-02\n",
      " -1.50059585e-02  6.59691488e-02  1.34321205e-02  1.27127153e-02\n",
      " -5.23147417e-02 -3.10443788e-02  2.98864163e-02  3.53100871e-02\n",
      " -5.63219333e-03 -3.86479900e-02 -3.37962736e-02 -1.38094323e-02\n",
      " -2.62183794e-02 -3.02886457e-02 -2.09766227e-02 -2.92870486e-02\n",
      " -3.92327108e-03  2.51087773e-04 -8.00466522e-03  9.11942215e-03\n",
      " -1.22879023e-02  1.05227342e-02  1.24420738e-02  1.39629478e-02\n",
      " -1.37525072e-02 -7.68121639e-02  1.59198212e-02  2.13565450e-02\n",
      "  2.51463493e-02 -4.80558823e-02 -8.74248064e-04 -2.38105091e-02\n",
      "  4.62066160e-03  2.58549726e-02  9.81571483e-03 -2.86392018e-02\n",
      "  3.74929514e-02 -2.52389824e-02 -1.59884701e-02  6.00358050e-03\n",
      " -5.16562535e-03  9.67083771e-03  7.08894342e-02  2.41720139e-02\n",
      " -2.70742626e-02  1.52970479e-02 -2.75421406e-03  1.63398020e-02\n",
      "  1.62305587e-02  9.46691245e-03 -3.92284055e-02 -4.19585024e-02\n",
      " -4.70977166e-02 -3.81674253e-02 -3.56412379e-02 -3.74439774e-02\n",
      " -2.50929781e-02  1.44793323e-02 -2.09485849e-03 -2.53116360e-02\n",
      " -9.00961310e-02 -2.70541373e-02 -4.41260961e-02 -5.12217055e-03\n",
      "  3.13002864e-03  1.83064815e-02 -2.93768918e-02  3.59373010e-02\n",
      " -2.57138561e-02 -8.46126224e-03 -2.22095570e-02 -2.79023444e-02\n",
      " -6.22423962e-03 -4.29756121e-02  4.62928188e-02 -2.89064482e-02\n",
      "  8.14171588e-02 -3.17419417e-02 -7.50882555e-03 -5.18135796e-02\n",
      " -1.24311374e-02  3.01966057e-02 -1.96266082e-02 -3.26151650e-02\n",
      " -1.68727017e-02 -2.24864516e-02 -6.17461029e-02 -4.50292526e-02\n",
      "  4.33783050e-02  2.33269187e-02  1.90800066e-02 -2.97218236e-02\n",
      " -2.62494640e-02 -1.30113254e-02 -3.27015003e-02  4.06706554e-02\n",
      "  3.60532046e-02 -3.55904829e-02  2.33926232e-02 -4.86834483e-02\n",
      "  7.25487080e-02 -3.08104156e-02  5.02994631e-02  4.75594354e-02\n",
      "  1.39449268e-02  1.64397495e-02  1.25891266e-02 -6.75742074e-02\n",
      "  2.60908157e-03  4.55414559e-02 -1.23905228e-02 -4.73310802e-03\n",
      "  9.20405354e-03  2.61736183e-02  6.50512965e-02  1.34437944e-02\n",
      " -2.97917802e-02  5.32230455e-03  1.62660906e-02 -7.74841677e-03\n",
      "  1.26344021e-03  4.16365261e-02 -9.89659094e-03 -2.40440381e-02\n",
      "  8.72261086e-03 -1.58060803e-02  3.75635621e-03  5.17876178e-03\n",
      "  2.16587703e-02  2.65996182e-02  2.69535991e-02  6.31148224e-02\n",
      "  9.21748363e-03  5.75338016e-02 -5.60664107e-02  2.78666082e-02\n",
      " -7.14869129e-03  3.36041287e-02 -1.79194704e-02 -6.97721939e-02\n",
      "  1.52034661e-02 -1.22131765e-02  5.20595592e-02  1.65127061e-03\n",
      " -5.38039147e-03]\n",
      "998166\n",
      "training: 37 testing: 26\n",
      "correctsource    21\n",
      "missed           16\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5 0.5 0.8 0.4 0.4 0.4 0.2]\n",
      "Accuracy: 0.4594594594594595\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.52      0.52      0.52        21\n",
      "       missed       0.38      0.38      0.38        16\n",
      "\n",
      "    micro avg       0.46      0.46      0.46        37\n",
      "    macro avg       0.45      0.45      0.45        37\n",
      " weighted avg       0.46      0.46      0.46        37\n",
      "\n",
      "accuracy = 0.46153846153846156\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.29      0.36        14\n",
      "       missed       0.44      0.67      0.53        12\n",
      "\n",
      "    micro avg       0.46      0.46      0.46        26\n",
      "    macro avg       0.47      0.48      0.45        26\n",
      " weighted avg       0.47      0.46      0.44        26\n",
      "\n",
      "(325,)\n",
      "[ 0.03705732  0.01776247  0.03960039  0.01694216  0.02121144 -0.01495854\n",
      "  0.02019146 -0.00592124  0.03294789  0.03721978  0.03088528  0.02486649\n",
      " -0.02029324 -0.00214242  0.0364746   0.05829224 -0.00691913 -0.03501249\n",
      " -0.03755146  0.03614226  0.00154594  0.01294095 -0.01189914 -0.02109409\n",
      "  0.01882404  0.02696742  0.08684938  0.01348678 -0.01950461  0.00560538\n",
      " -0.00086284  0.02886738 -0.00326397 -0.024285   -0.03315267 -0.0666551\n",
      "  0.02464963  0.00690823  0.02403238 -0.04326693  0.04653085 -0.02748297\n",
      "  0.0025318   0.05099682  0.0374624   0.0391293   0.0007495   0.02551919\n",
      "  0.00253811  0.00037672 -0.02702983  0.03000341  0.00351359 -0.0220737\n",
      " -0.02559578  0.01926918  0.00929626  0.00292931  0.04491033 -0.02767714\n",
      " -0.00508676  0.05585841  0.02385487  0.01772699 -0.02086753  0.0001685\n",
      " -0.00388315 -0.04779649 -0.03113408  0.01234173  0.01904688 -0.01527946\n",
      "  0.01669645  0.02415681  0.04915304 -0.04603419  0.01273672  0.06282079\n",
      " -0.05303272 -0.05851042 -0.02408525 -0.01303838 -0.00990471  0.02162585\n",
      " -0.04722602 -0.05364673  0.0392727  -0.02517611  0.03928453 -0.00497461\n",
      "  0.02458793 -0.00688227 -0.03073803  0.00445956 -0.01825993  0.00683263\n",
      " -0.02123805  0.0071179  -0.06063603 -0.08081976 -0.02168347 -0.00821853\n",
      " -0.00804264 -0.04805109  0.00721742  0.03051604 -0.03073714 -0.10584821\n",
      "  0.0237859  -0.00836533 -0.0588521   0.01286973  0.05327963 -0.08901421\n",
      " -0.02143319 -0.00855097 -0.08119363  0.01200963  0.00127742 -0.04233489\n",
      "  0.01909929 -0.01781267 -0.03132498 -0.00931841  0.06127873 -0.0534137\n",
      " -0.00708203  0.00027453  0.01747177  0.00543     0.03421084 -0.02661757\n",
      " -0.05103364 -0.01196965  0.0491039   0.03764359  0.07202373  0.03708314\n",
      " -0.03798016 -0.01102707  0.02926501 -0.00701997 -0.04346899 -0.04703534\n",
      " -0.04612793 -0.02291664  0.02231479  0.00281223 -0.02282486  0.01447112\n",
      " -0.01248657  0.04421268  0.07583976 -0.02576138 -0.00719797 -0.03882379\n",
      " -0.01913823 -0.01506555 -0.02554623  0.04082168 -0.02008605 -0.02105263\n",
      "  0.02061241 -0.01360065 -0.01419543 -0.01613122  0.0348336   0.00173881\n",
      "  0.03544691  0.01373144 -0.02852767 -0.00592696 -0.01911042  0.02051144\n",
      "  0.03183667 -0.00014762  0.00868704 -0.02106815 -0.02216965  0.02073421\n",
      "  0.07482686 -0.00646544 -0.03088003  0.02550274 -0.00260133 -0.04495261\n",
      "  0.007941   -0.02441216 -0.00150748 -0.0269683   0.00538588  0.0434895\n",
      "  0.04688831 -0.00359096  0.01225271  0.01710208  0.0359536  -0.07622867\n",
      "  0.08950576 -0.05798455 -0.05269582 -0.03379182 -0.00241424  0.01317222\n",
      "  0.00226393 -0.00356237 -0.04461739 -0.06433704  0.01280209  0.01743773\n",
      " -0.00693285 -0.01980888  0.03446535  0.00110028  0.00375276  0.01183711\n",
      " -0.04808563 -0.02180718  0.07134643 -0.00591828 -0.00253938 -0.03492074\n",
      "  0.01472441  0.01603693 -0.02678937  0.02383671  0.06161242  0.07181771\n",
      " -0.06128128  0.01578238 -0.00939925 -0.02196762 -0.04577431 -0.01344567\n",
      "  0.04070437 -0.01986529 -0.02870266 -0.02008584  0.01023551 -0.05387317\n",
      "  0.07040805 -0.00219014 -0.00750133 -0.00515663 -0.02688599  0.03597242\n",
      "  0.00501281  0.01004017  0.02481639 -0.00978533 -0.01259602  0.00410524\n",
      "  0.00767524 -0.00137373  0.01899658 -0.04984623  0.02910324  0.0064893\n",
      " -0.00037317 -0.07163484 -0.01984634  0.03695997 -0.03740198  0.04228113\n",
      "  0.02685857 -0.0149797   0.03264975  0.00402079  0.00933263  0.01784062\n",
      "  0.02854564  0.00156628 -0.04999901 -0.04348502 -0.00377052  0.04866349\n",
      "  0.01702888  0.0251646  -0.00316833 -0.04552887  0.00112001  0.00511138\n",
      " -0.02691483 -0.02116202  0.00994507  0.01866138 -0.04703753  0.08248894\n",
      " -0.03188894 -0.04314038 -0.01393359  0.01931046  0.0379502   0.00212663\n",
      " -0.01522866  0.00838063  0.00115177 -0.01138543  0.03461538  0.00184894\n",
      " -0.03609016  0.01271019  0.04889927  0.00839105 -0.01205984 -0.03180576\n",
      " -0.03236299  0.01410953  0.06926299  0.00467053  0.01157864  0.01965726\n",
      " -0.00795796  0.01521364 -0.00013526  0.00537828 -0.02198081 -0.02919799\n",
      "  0.00692842  0.0409298   0.06550607  0.06022661 -0.02428759  0.03219869\n",
      "  0.02272202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108391\n",
      "training: 34 testing: 24\n",
      "correctsource    25\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    18\n",
      "missed            6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.5        0.2        0.4        0.25       0.75\n",
      " 0.75      ]\n",
      "Accuracy: 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.64      0.65        25\n",
      "       missed       0.10      0.11      0.11         9\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        34\n",
      "    macro avg       0.38      0.38      0.38        34\n",
      " weighted avg       0.52      0.50      0.51        34\n",
      "\n",
      "accuracy = 0.75\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.80      0.89      0.84        18\n",
      "       missed       0.50      0.33      0.40         6\n",
      "\n",
      "    micro avg       0.75      0.75      0.75        24\n",
      "    macro avg       0.65      0.61      0.62        24\n",
      " weighted avg       0.72      0.75      0.73        24\n",
      "\n",
      "(444,)\n",
      "[ 1.63458478e-02 -2.04262876e-02  3.03655871e-02 -6.19357923e-03\n",
      "  0.00000000e+00  1.28954399e-02 -5.25992248e-02  1.16930694e-02\n",
      " -5.44197598e-03  9.77989502e-03 -2.46325916e-02 -1.70038588e-02\n",
      " -4.34336851e-03 -2.99861960e-02  1.44424918e-02  1.43790986e-02\n",
      " -3.45096346e-03 -1.85515451e-02 -3.56017193e-02  1.01651966e-02\n",
      " -1.83238316e-03  2.03745571e-02 -1.10582588e-02  1.16151237e-02\n",
      " -6.82287933e-03  2.41300480e-02 -1.93727491e-03  1.02021591e-02\n",
      "  2.62464465e-02 -2.59931269e-02 -4.01581498e-04 -2.94462632e-03\n",
      " -2.03901069e-03  1.13528507e-02 -2.52316380e-02 -1.49908195e-02\n",
      "  2.09392887e-02  2.47489682e-04  1.05650793e-02 -4.08239914e-03\n",
      " -1.77956368e-03 -1.28400140e-02 -2.94661643e-02  7.26174965e-03\n",
      "  4.60449524e-04  3.12892326e-03 -2.11770619e-02  6.10633988e-03\n",
      "  1.51963866e-02  5.23447940e-03 -2.01462741e-02 -2.09632945e-03\n",
      "  6.40920166e-03 -5.16185735e-03  5.80275438e-02 -3.87525700e-02\n",
      "  2.24654973e-02  2.11121879e-02  1.15862301e-02 -2.08729531e-02\n",
      " -1.39807135e-03 -4.15616678e-02  6.86970746e-03  1.89134639e-02\n",
      " -2.92543689e-02 -2.02663371e-02 -6.10767222e-03 -4.46474155e-02\n",
      "  1.81830790e-02 -3.47270900e-02 -2.40337985e-02 -2.13288474e-02\n",
      " -3.97374021e-02  2.36606028e-02 -3.07249333e-02 -1.53164756e-02\n",
      "  8.94570199e-03 -4.17234562e-02 -1.76421017e-03 -1.76369548e-02\n",
      "  1.78954179e-02  9.19224293e-03  3.62617639e-02 -2.75907975e-02\n",
      " -2.76736387e-02 -1.23323651e-02  8.00564118e-03  3.60761402e-02\n",
      " -3.93555690e-03 -6.63499397e-03 -9.64196386e-03 -2.73364088e-02\n",
      " -1.46861848e-02 -2.06746877e-02 -1.82252510e-02  1.82928224e-02\n",
      " -1.64167192e-02 -7.14614370e-03  1.97341406e-03 -4.82595267e-02\n",
      " -7.11129147e-03 -1.01962729e-02  4.12714173e-02 -1.50983823e-02\n",
      "  1.07536245e-02 -3.94469848e-03  3.44967225e-03  9.26797271e-03\n",
      "  1.92530476e-02 -5.43858155e-04 -3.16370177e-03 -1.88584680e-02\n",
      " -2.65941114e-04  3.33518152e-02  5.43834602e-03  1.21563474e-02\n",
      "  5.04721176e-02 -2.29968640e-02 -8.51563475e-03  3.04140720e-02\n",
      " -2.40336080e-02 -1.07945238e-02  4.14203708e-02 -9.99480431e-03\n",
      " -1.11333327e-02  4.37290378e-03 -2.89129539e-02 -3.43713224e-02\n",
      "  2.64615280e-02  1.46784677e-02  3.62545848e-02  1.61672290e-02\n",
      "  1.60629613e-02  4.21078669e-02  3.49610566e-02  3.13408616e-02\n",
      "  3.62461062e-02 -1.02329139e-02  6.08417589e-03  2.35619020e-02\n",
      "  5.30874335e-03 -6.62105778e-03 -4.64902561e-02  1.44522245e-02\n",
      " -3.87405941e-03  3.49135583e-02 -3.30187373e-02  3.39841903e-02\n",
      " -4.15691499e-02 -2.06361924e-02  3.36908690e-02  8.64725282e-03\n",
      " -3.58061724e-02 -7.21674314e-05  2.97609959e-03  5.01690330e-03\n",
      " -4.48508027e-03  2.70134636e-02  2.52550279e-02 -1.45394347e-02\n",
      " -2.58565878e-02 -1.03075864e-02  4.18719939e-02 -2.46702581e-02\n",
      " -5.36062217e-03 -1.58592376e-03  1.54641810e-02 -7.91368126e-03\n",
      "  6.88537386e-02 -2.89288969e-03  1.47276272e-02  2.05217210e-02\n",
      " -2.60065938e-02  3.46688796e-02  1.06626325e-02 -1.89205465e-02\n",
      "  1.30132966e-02  3.16113424e-02 -1.83246547e-02  7.71782234e-03\n",
      "  1.42546039e-03 -1.27675787e-02 -2.18723957e-02  1.19137450e-02\n",
      " -5.75066674e-03  2.17811335e-03 -1.45891773e-02  1.98648831e-04\n",
      "  2.45165764e-02  4.01266407e-03 -1.54483094e-02 -2.26315252e-02\n",
      "  3.17356872e-02 -1.49043095e-02  5.44157931e-02 -1.27365533e-02\n",
      "  8.13665895e-03  2.17417273e-02 -8.53308888e-03  1.27085316e-02\n",
      "  5.96637413e-03  8.20182376e-03 -4.06377405e-03  9.50615092e-03\n",
      " -1.56746960e-02 -1.61654888e-02  2.70703090e-03  2.12753085e-03\n",
      "  1.04982720e-04 -3.86675350e-02  3.47297136e-02 -4.45315242e-04\n",
      "  0.00000000e+00 -3.26634333e-02 -3.68506983e-02  2.59131987e-02\n",
      "  1.55706839e-02  3.31935543e-02 -2.32522447e-03  2.24058903e-02\n",
      " -1.53904692e-03  4.35675847e-02  1.02720044e-03  1.21438474e-03\n",
      " -2.75862929e-03 -1.00134801e-02  8.23375039e-03  1.06138452e-02\n",
      " -2.88179676e-02 -2.88196736e-02 -2.83605857e-02  1.46895017e-02\n",
      " -4.08656178e-02 -1.03797594e-02  3.92567868e-02 -8.76026621e-03\n",
      " -1.43821259e-02 -3.27336656e-02  5.43597181e-03 -3.26469124e-02\n",
      " -3.26726636e-03  1.89644824e-02  1.12385160e-04  4.00543451e-03\n",
      "  3.76576078e-02 -2.38461002e-02 -1.99719879e-02  4.46720826e-02\n",
      " -1.48545533e-02 -3.38074000e-03  2.27865889e-02 -1.41117562e-02\n",
      " -1.15337414e-02 -1.20036680e-02 -2.05789425e-02  2.27501246e-02\n",
      "  1.20313563e-02  2.33060085e-02 -3.47736158e-02 -2.72064759e-02\n",
      "  2.14377581e-03  4.81645416e-03 -8.60819884e-03  1.27615623e-02\n",
      " -4.43931497e-03  4.06954875e-02 -5.76957866e-03 -3.35000394e-02\n",
      "  4.10409780e-02 -1.73573826e-02 -4.79923752e-03  3.46126866e-02\n",
      "  4.65587802e-04  1.33492418e-02 -6.15316936e-03  2.70900631e-02\n",
      "  1.03406954e-02  5.22904006e-02 -1.91863336e-04  1.63760181e-02\n",
      "  2.73134721e-02 -4.61438096e-03 -1.43585832e-02  1.72622039e-02\n",
      "  1.28559289e-02 -4.48122869e-03 -2.70480669e-02 -4.77144018e-02\n",
      "  2.02732923e-02  3.54544074e-02  1.10730139e-02  1.86135609e-02\n",
      " -1.35482357e-02 -1.65781584e-02  3.85027180e-03  4.08281937e-03\n",
      " -3.61880837e-02  6.32728789e-03  2.89125485e-02 -3.01198556e-02\n",
      "  1.65251064e-03  3.36159012e-02  6.22426206e-03 -3.02367344e-03\n",
      "  2.92144294e-02  4.76242926e-03  3.28677746e-02  1.09279857e-02\n",
      "  4.07509858e-03  7.14842359e-03 -4.81663508e-03  1.48869252e-02\n",
      " -1.68731112e-02 -4.55641930e-03 -8.93778634e-03 -1.43940864e-02\n",
      "  2.55966756e-02  1.77552904e-02 -3.60198618e-03  1.68537554e-02\n",
      " -2.20223806e-02  3.32137504e-02 -4.26860489e-02  1.40469732e-02\n",
      " -1.41239989e-02  2.59829747e-02  1.39044030e-02 -4.08134716e-03\n",
      " -1.37520626e-02 -2.19350370e-02  2.39226847e-03  1.32787015e-02\n",
      " -9.88154708e-03 -4.28598132e-03 -6.07767071e-03 -1.05420652e-02\n",
      "  2.87274949e-02 -2.06623704e-02  2.80601500e-03 -2.97962482e-02\n",
      "  1.02073599e-02 -2.65413063e-02 -1.48995804e-03  1.69093812e-02\n",
      " -2.61653701e-02 -8.10436693e-03  1.75402671e-02 -2.19382708e-03\n",
      " -1.95809540e-02  6.11887310e-03 -1.70739614e-02  1.91640942e-02\n",
      " -1.89054199e-02  2.84582700e-02  5.62088373e-03 -2.84134395e-02\n",
      " -1.19607845e-02 -1.37349186e-02 -1.86675083e-02  3.63804782e-03\n",
      "  5.57471097e-03  5.93629831e-03  1.04669457e-02  1.47642920e-02\n",
      " -2.81264091e-02  6.57034591e-03  9.59637765e-03  2.11261616e-02\n",
      " -7.68030718e-03  3.41848742e-03  8.57637730e-03 -3.96895792e-03\n",
      "  6.79936392e-03  1.23262741e-02  3.66912286e-03  2.80678248e-02\n",
      " -8.27806414e-03  2.26141307e-02  2.81479073e-03 -3.22731090e-02\n",
      "  2.80085928e-02  4.05221086e-02  4.78159725e-03  3.99745457e-03\n",
      " -2.23268054e-02 -2.65699869e-03 -1.45901386e-03  9.82728368e-03\n",
      " -2.30373032e-02 -3.32261358e-02 -1.54920278e-03 -1.87263511e-02\n",
      "  1.57021852e-02 -5.51859009e-03  2.39619357e-02  1.36662258e-02\n",
      "  8.09725480e-03 -2.57602411e-02  1.94035312e-02  1.02276599e-02\n",
      " -1.44228447e-02  2.56170998e-02  1.02534825e-02  1.61881730e-02\n",
      "  2.14731311e-03 -2.21971884e-02  1.13382016e-02 -2.13991258e-02\n",
      "  2.06680265e-02  5.02279777e-03  8.94890909e-03  1.30333988e-02\n",
      " -4.49127921e-02 -5.62084156e-03 -2.13794942e-02  5.45422859e-02\n",
      " -1.54418113e-02 -1.48505267e-02 -3.33214429e-02 -8.70398294e-03\n",
      " -2.19647265e-02  6.51768728e-03 -8.31691856e-03 -7.28865343e-03\n",
      " -9.10679548e-03  1.54948998e-03  1.86367493e-02  6.10457668e-03\n",
      "  1.66996514e-02 -1.63068262e-02 -6.20327484e-04  5.82476950e-03\n",
      "  2.33109429e-02 -2.61294733e-03 -3.26058756e-02 -4.03568187e-03\n",
      " -2.76309885e-02  5.20586136e-03 -4.84866940e-02 -9.31578480e-03\n",
      "  6.21312936e-03 -1.31003893e-02  1.77920266e-02 -3.45863851e-02]\n",
      "122922\n",
      "training: 28 testing: 20\n",
      "correctsource    15\n",
      "missed           13\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           10\n",
      "correctsource    10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6  1.   0.5  0.5  0.75 0.5  1.  ]\n",
      "Accuracy: 0.6785714285714286\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.69      0.73      0.71        15\n",
      "       missed       0.67      0.62      0.64        13\n",
      "\n",
      "    micro avg       0.68      0.68      0.68        28\n",
      "    macro avg       0.68      0.67      0.67        28\n",
      " weighted avg       0.68      0.68      0.68        28\n",
      "\n",
      "accuracy = 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.60      0.60        10\n",
      "       missed       0.60      0.60      0.60        10\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        20\n",
      "    macro avg       0.60      0.60      0.60        20\n",
      " weighted avg       0.60      0.60      0.60        20\n",
      "\n",
      "(444,)\n",
      "[-2.06430551e-02 -1.86267380e-02  1.50231871e-02 -1.40408487e-02\n",
      " -1.45032010e-02 -4.82728614e-03 -1.92097636e-02 -3.46974610e-02\n",
      " -1.79959189e-02  3.71326883e-04  5.24026599e-03  3.72127027e-02\n",
      " -1.01884684e-03 -4.00771695e-02 -2.09658517e-02  1.04176613e-02\n",
      "  2.96151099e-02 -2.19435380e-02 -3.44693468e-03 -4.40865521e-02\n",
      " -1.65865091e-02 -2.22006202e-02 -1.66896163e-02  1.12636910e-02\n",
      "  2.55855478e-02 -1.81183745e-02  1.85492033e-02 -5.16263083e-02\n",
      "  2.30961893e-02 -5.56179256e-03  2.57377866e-03  7.30642651e-03\n",
      "  2.29946984e-02  2.13922578e-03  2.74647681e-02 -1.57487137e-02\n",
      " -3.98171814e-02  4.57470335e-03  3.58637192e-03 -4.38720694e-02\n",
      "  1.27345534e-02 -2.63984112e-02  3.17533496e-02 -2.38959425e-02\n",
      "  2.36282313e-02 -1.63606474e-02 -7.15624504e-03 -1.78652603e-02\n",
      "  1.03174541e-02  1.50190740e-02  1.14767465e-02  6.73901000e-03\n",
      " -1.27304797e-02  1.55620535e-02  2.42303761e-02  8.38745333e-03\n",
      "  1.50048625e-02 -1.20385195e-03  1.51808094e-03  1.68699059e-02\n",
      " -2.42127403e-03 -1.22838528e-03  2.47926271e-02 -2.07061594e-02\n",
      "  3.04354912e-02 -3.67207348e-02  6.16023062e-03  4.70829443e-02\n",
      " -7.57072743e-03  2.51783651e-02 -2.62552964e-03 -8.28349033e-03\n",
      " -1.18229717e-02 -4.29447898e-03  2.02388577e-02  1.12481657e-02\n",
      " -6.09493406e-03 -1.16208418e-02 -6.02717517e-04  1.93317483e-03\n",
      " -1.56418609e-02 -5.34608089e-04  2.60065028e-02  2.63948911e-02\n",
      " -9.54445810e-03  1.09900842e-02 -1.74194055e-02  3.05722160e-02\n",
      " -1.85006829e-03 -3.16247951e-02 -2.00722798e-02  6.88408360e-03\n",
      " -1.11950690e-02 -2.61358815e-02 -7.12114211e-03 -2.04877173e-02\n",
      " -3.05926592e-03 -1.56233495e-02 -1.25665319e-02  1.53267112e-02\n",
      " -1.41613644e-02  2.75249022e-02 -1.00549264e-02 -8.41439354e-03\n",
      " -2.44428191e-03  2.76811506e-02  2.32263786e-02 -2.62303500e-03\n",
      " -1.09056157e-02  5.69281439e-03  1.54680094e-02 -6.99723256e-03\n",
      "  8.12995256e-03  3.23083768e-02 -1.47403673e-02 -9.02847593e-03\n",
      "  3.52808110e-03  1.11678496e-03 -1.28899894e-02  4.73905393e-03\n",
      "  7.14654019e-03 -1.81453147e-02 -3.65465437e-03 -1.99422876e-04\n",
      "  8.55680069e-04  2.55693055e-02  5.50756260e-02  2.83454533e-04\n",
      " -8.51221845e-03  1.79651669e-02  1.22843283e-02 -1.33568184e-02\n",
      " -9.35478535e-03 -9.77133720e-03  1.78394413e-02  2.38879094e-03\n",
      " -2.88766247e-02  4.49924578e-03 -1.87466474e-02  1.14623393e-02\n",
      "  2.55997264e-02  2.06414570e-02  1.55580619e-03 -7.31787295e-03\n",
      "  1.23519596e-03  1.66608499e-02 -1.02649749e-02  4.52547703e-03\n",
      "  2.52532240e-02 -6.36328001e-03  1.63079108e-02  1.92784304e-02\n",
      " -5.58799934e-03 -3.06493715e-02  9.06028914e-03  3.34691640e-04\n",
      " -3.41641205e-03 -1.40786470e-02  4.91626423e-02  9.44278438e-04\n",
      "  2.35852848e-02 -2.09005262e-02 -3.32805794e-04 -1.54705814e-02\n",
      "  1.34687194e-02 -4.70299461e-04  2.37288289e-03  1.26549196e-02\n",
      " -2.06448700e-02  1.88141999e-02 -3.30173329e-02  5.46391982e-03\n",
      " -1.87989625e-03 -1.13036651e-02 -5.73100283e-03  2.48389761e-02\n",
      "  1.87818587e-02 -5.84867347e-03  9.42660529e-03  1.61245223e-02\n",
      " -3.21008498e-04 -4.40127783e-03  9.42663380e-03  2.29198296e-03\n",
      " -2.13761956e-02  7.82337934e-03 -3.50820445e-02 -9.81891921e-05\n",
      "  4.38694774e-03 -1.44823106e-03  2.64360923e-03 -9.42065356e-03\n",
      "  2.38000570e-02  2.92513459e-03  3.78734617e-03  3.40375137e-02\n",
      " -9.13066470e-03 -3.32439733e-02 -8.84470635e-03  3.04613998e-03\n",
      "  1.65201109e-02 -2.73551167e-03  1.26594420e-02  1.44380237e-02\n",
      " -1.92310536e-02 -8.66974452e-03  2.00054711e-03  2.87717263e-02\n",
      " -1.05715371e-02 -2.01254554e-02  1.76880059e-02 -1.11391206e-02\n",
      " -6.62966657e-03  2.75671472e-02  4.38278321e-03 -5.06454459e-03\n",
      " -1.08035940e-02  5.75546808e-03 -1.56322849e-02  1.21642641e-02\n",
      " -1.37026432e-02  5.16235293e-02 -8.76420696e-03 -5.34731423e-03\n",
      " -4.96708565e-03  1.36511344e-02 -8.67218175e-03  5.61814755e-03\n",
      "  1.93082268e-02  1.20635868e-02  1.95370366e-02 -2.09759493e-02\n",
      " -1.56211354e-02  1.45243813e-02 -3.04398013e-02 -3.48583839e-02\n",
      " -1.75784778e-03 -1.91157468e-02 -4.67261483e-03  1.06502445e-02\n",
      "  3.16023666e-02 -2.72172382e-03  6.32171469e-03 -1.09496755e-02\n",
      " -9.92535678e-03 -1.33648556e-03 -4.13952717e-03 -2.85252545e-03\n",
      "  3.19887285e-02 -3.15272851e-02  1.81805842e-02  3.00073163e-03\n",
      " -3.69854167e-03 -5.42838900e-04 -4.96034999e-02  5.92170137e-03\n",
      "  2.59986239e-02  3.93935057e-02 -1.70041866e-02 -4.17828335e-02\n",
      " -3.56629794e-02 -2.77956376e-02 -2.76949056e-03 -4.20680396e-02\n",
      "  1.42037457e-02  1.29886314e-02  1.32286273e-03 -2.40627054e-02\n",
      " -7.95131768e-03 -6.49458554e-03 -3.77815862e-04 -1.11007696e-02\n",
      "  9.11797939e-03 -1.04990705e-02 -4.26288190e-02 -2.51783563e-02\n",
      "  1.23359272e-02 -1.98109581e-02 -3.08746822e-02  1.64217042e-02\n",
      "  6.04676728e-04 -1.88408095e-02  2.14419291e-02  4.86424438e-02\n",
      "  6.55215784e-03 -7.39261670e-03  6.22583449e-03 -5.40119050e-03\n",
      "  3.32010885e-03  4.59888111e-03  1.95128424e-02 -2.61388480e-03\n",
      "  7.93179415e-04  1.82212467e-02  1.53702102e-03 -7.55585179e-03\n",
      "  6.44667599e-03 -2.44640293e-03  1.20354746e-02 -1.43763776e-02\n",
      "  6.16569115e-05 -8.64553995e-04  6.48408807e-03  1.73790305e-02\n",
      "  6.04532961e-03  9.55418866e-03 -4.13480860e-02  8.13169082e-03\n",
      " -1.04200725e-02 -7.27600610e-03  9.15751686e-03  5.77574029e-03\n",
      "  1.69442492e-02 -2.73182270e-03  6.69307315e-04 -8.93246225e-03\n",
      " -1.10834306e-02  2.76416648e-02  8.21317390e-04  1.55649922e-02\n",
      " -4.62114429e-03 -1.05664095e-02  2.17878558e-03 -3.17736975e-02\n",
      "  9.02445664e-03 -3.62043914e-02 -5.32573989e-03  9.86514220e-03\n",
      "  5.76556571e-03 -4.03359664e-02 -3.13785137e-02  3.02628636e-03\n",
      "  3.26612949e-03 -3.44007949e-02  2.14943569e-02  2.09626563e-02\n",
      " -7.39163649e-03 -2.09701732e-02  2.01814758e-02  1.25371085e-02\n",
      "  1.78832332e-02 -1.05142767e-03  2.10044912e-02 -1.54643570e-02\n",
      "  5.20921888e-03 -8.07659770e-03  1.70460581e-02 -2.87153220e-02\n",
      " -9.26640201e-03 -3.57603449e-03 -9.80831348e-03 -1.49263683e-02\n",
      " -5.80758175e-03 -3.94571996e-03 -1.83278177e-02 -7.47433468e-05\n",
      "  1.36986083e-02  1.28244661e-02  1.74541135e-02 -7.11301566e-04\n",
      "  6.78796973e-03  1.41560738e-02  7.95626491e-03 -2.81341043e-02\n",
      "  6.33496762e-03  2.80224017e-02  1.36986976e-03  2.06624213e-02\n",
      " -4.32647420e-03 -1.34386469e-03  2.58412872e-03  1.32336276e-02\n",
      " -1.25905528e-02  4.26586566e-02 -1.13044260e-02  2.11816993e-02\n",
      " -1.18764483e-02  2.12269194e-02  6.32447892e-03 -1.50295448e-02\n",
      "  9.21064239e-03  1.22333927e-02 -6.65740755e-03 -2.92847503e-02\n",
      " -3.09059773e-02 -9.39395572e-03  1.76440477e-02  1.28226010e-02\n",
      "  1.22476630e-02  1.22421971e-02  1.05002558e-02  3.36835533e-03\n",
      " -1.23751424e-02 -1.06751047e-02  3.82907388e-03 -1.91754463e-02\n",
      " -8.28922799e-03 -4.58116251e-03  8.05266079e-03  2.79712775e-02\n",
      "  2.02056934e-02  1.11312928e-02 -5.00589374e-02  8.82547290e-03\n",
      " -5.13061807e-03  1.18600347e-02  1.95330373e-02 -2.19435792e-02\n",
      " -3.69491762e-03  1.01268395e-02  5.79767370e-03  3.10519332e-02\n",
      " -5.31419611e-03  2.43612782e-02  1.46857379e-02  2.07879991e-02\n",
      " -4.65293046e-03  1.93192319e-02 -1.00698233e-02  1.85764425e-02\n",
      "  1.05428237e-03 -1.72738773e-02 -4.06809136e-04  1.59543447e-02\n",
      "  1.73643611e-02 -2.68338873e-02 -9.23054180e-03 -3.90543092e-03\n",
      " -9.00560155e-03  4.61295894e-03  3.56010873e-02  2.46003225e-04\n",
      " -1.54099305e-02 -1.43829365e-02  4.42623348e-03  2.54791171e-03\n",
      "  5.06236968e-03  1.58966186e-02 -1.95262796e-02 -1.35768625e-02\n",
      " -8.57478281e-03  1.37678319e-02 -3.24411344e-04 -2.33912423e-02]\n",
      "139593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 33 testing: 23\n",
      "missed           24\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           17\n",
      "correctsource     6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.16666667 0.6        0.75       0.5        0.75\n",
      " 0.5       ]\n",
      "Accuracy: 0.5151515151515151\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.00      0.00      0.00         9\n",
      "       missed       0.65      0.71      0.68        24\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        33\n",
      "    macro avg       0.33      0.35      0.34        33\n",
      " weighted avg       0.48      0.52      0.49        33\n",
      "\n",
      "accuracy = 0.5652173913043478\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.00      0.00      0.00         6\n",
      "       missed       0.68      0.76      0.72        17\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        23\n",
      "    macro avg       0.34      0.38      0.36        23\n",
      " weighted avg       0.51      0.57      0.53        23\n",
      "\n",
      "(444,)\n",
      "[-8.60103588e-03  3.13918277e-02 -4.26314259e-02  2.44747694e-02\n",
      "  7.46161369e-03  4.92860901e-03 -2.03780220e-02 -1.12238375e-02\n",
      "  4.37905288e-03  5.07340517e-04  2.96056993e-02  2.37863371e-03\n",
      "  3.13863946e-02  9.94573717e-03 -1.31076947e-02  5.06973443e-03\n",
      " -7.07122293e-03 -1.05421738e-02  1.53706941e-02 -8.58202375e-03\n",
      " -1.11821318e-02 -3.13015225e-02  3.40064167e-03  2.27756807e-02\n",
      "  3.14896998e-02  1.67396760e-02 -1.98716659e-02 -7.40976260e-03\n",
      "  2.86147839e-02 -3.20458257e-03 -3.46648416e-02 -1.63887513e-02\n",
      " -2.71211122e-02  3.40899190e-02 -1.65716104e-02  6.66500142e-03\n",
      " -4.61037602e-02  2.12781790e-02  8.90015466e-03  3.19013660e-02\n",
      " -9.67876120e-04  7.88448649e-03  2.27285811e-03 -1.24214242e-02\n",
      "  4.75540422e-02  2.48876250e-02  1.14328725e-02 -4.86508474e-02\n",
      "  1.35911869e-03 -2.81998368e-02 -8.16020019e-03  1.69192103e-02\n",
      " -3.90075178e-02 -3.15157959e-02 -1.62459053e-02  2.93944608e-02\n",
      "  1.64043477e-03 -1.98440459e-02 -1.83232421e-03  5.88773795e-03\n",
      " -8.52997620e-03 -5.75378148e-03 -1.57894748e-03 -1.18744013e-02\n",
      "  3.13600837e-02 -6.72021205e-04  1.64816745e-03 -4.13856213e-03\n",
      " -3.75206561e-03 -9.61036551e-04 -6.00439205e-03  7.70004389e-03\n",
      "  6.66461467e-04  5.06423310e-02  9.94753156e-03  3.06708609e-02\n",
      " -6.44629170e-03  3.13341750e-02  3.41923436e-02 -5.62297694e-03\n",
      " -2.63911834e-02 -1.21763598e-02 -7.80135330e-03 -5.72543406e-03\n",
      " -2.31329793e-03  4.71386066e-03 -1.68651311e-02 -5.48100861e-02\n",
      "  6.15603530e-03 -3.88870257e-03 -9.72947959e-03 -8.63717501e-03\n",
      " -4.24274103e-02 -5.82000606e-03  2.26078105e-02  3.45960976e-02\n",
      " -2.73366387e-02  9.44194090e-03  8.20555000e-04 -3.42609632e-02\n",
      " -4.52023635e-02  1.98042647e-02  5.21516777e-04 -4.97701206e-03\n",
      "  3.77621617e-03  1.27609826e-02 -2.65276559e-02  8.64516502e-03\n",
      "  1.14992085e-02  7.95073963e-03  6.03037673e-03  8.43542940e-03\n",
      "  1.29178790e-02 -1.85156057e-02  1.58710134e-02  2.81075187e-03\n",
      "  7.41026744e-03  1.07587887e-03  1.66641854e-02  6.68542373e-03\n",
      "  4.35775928e-03 -6.03758391e-05  1.78249776e-03  2.51984328e-02\n",
      "  2.38303021e-03 -2.70080382e-03 -3.48164372e-02  2.57770974e-02\n",
      "  3.19111161e-02  8.62412268e-03 -4.73217939e-02  1.84821162e-02\n",
      "  1.74627309e-03  2.11074791e-02  2.88338675e-02 -2.13112005e-03\n",
      " -2.22881769e-03 -1.47945908e-03 -1.87048982e-02 -2.28139034e-02\n",
      " -2.75833692e-02 -1.24600141e-02  1.27542227e-02  2.01009588e-02\n",
      " -5.92598158e-04 -1.28064077e-02 -1.02463342e-03 -2.78377801e-03\n",
      " -1.28621532e-02  2.26106321e-02 -1.49780154e-02 -1.24601041e-02\n",
      " -2.55219563e-02  2.58033031e-02 -1.27974945e-02 -1.63237293e-02\n",
      " -2.78561763e-02 -2.36138679e-02  2.89430843e-02  1.23659354e-02\n",
      "  5.28210617e-03  1.23205602e-02  2.35063139e-02  1.24205533e-02\n",
      "  4.34893165e-03 -7.80148466e-03 -2.52957024e-02 -1.58628888e-02\n",
      "  1.81987372e-02 -1.09015954e-02  1.07200558e-02 -2.02951772e-03\n",
      " -2.92889626e-02 -2.73796379e-02 -2.55587464e-03 -3.20207973e-03\n",
      " -5.12788641e-03 -7.57603105e-03 -2.05866254e-02  2.48344973e-02\n",
      " -1.24916772e-02  7.43345024e-03 -5.26913566e-02  1.26537852e-02\n",
      "  2.36024759e-02 -1.62575409e-02  2.03553381e-02  1.07221303e-02\n",
      "  3.32389699e-03  7.28894515e-03 -8.27080923e-04  3.77291159e-04\n",
      " -1.17944872e-02  1.00405276e-02 -1.93316274e-02 -5.81668004e-03\n",
      " -3.68154461e-03  2.51804233e-02  2.26072567e-02  1.74547967e-02\n",
      "  6.09635976e-04 -4.86957931e-02 -7.95020495e-03 -2.75909748e-04\n",
      " -5.48115718e-04 -1.73946162e-03  3.33565959e-02 -1.18114916e-02\n",
      "  1.30525393e-02 -3.18211960e-03  1.77435771e-02  2.48808570e-03\n",
      "  2.50673909e-02  2.97143037e-02  8.04398145e-03  2.66092882e-02\n",
      " -7.46322818e-03 -1.01905336e-03  1.48055485e-03 -3.76544763e-02\n",
      "  1.53768952e-02  3.01710466e-02  1.40035891e-02  2.89188033e-02\n",
      "  2.09980906e-02 -1.72127836e-02 -5.43505375e-02 -3.08519665e-02\n",
      " -2.81213210e-02 -3.35334851e-03  4.53199871e-03 -9.21455452e-03\n",
      "  2.41495732e-02 -2.00126104e-02 -1.14415264e-02  3.05003293e-02\n",
      " -1.40811220e-02 -4.46808961e-02  5.53683291e-03  1.75369560e-02\n",
      " -1.37725898e-02 -2.12991083e-02  1.58308733e-03 -1.46723619e-02\n",
      "  5.54200889e-02  2.91959415e-02 -1.98800367e-02  3.47938527e-02\n",
      " -1.34362653e-02  1.39610829e-02 -1.53680893e-02  1.39544171e-02\n",
      " -3.12653770e-02  2.06171138e-02 -2.14676884e-02  2.70281763e-02\n",
      "  4.45832447e-03  2.80368231e-02 -1.17304353e-02  2.23217066e-02\n",
      "  2.20043393e-02  2.30060116e-02  1.61027564e-02  4.02981201e-02\n",
      " -9.89477657e-04  1.85955418e-02 -2.21952749e-02  2.28384412e-03\n",
      " -1.80725312e-02  2.06479653e-02  1.35929805e-03  3.21677611e-04\n",
      " -1.27957147e-02 -1.37789485e-02 -5.90501693e-03  7.17233009e-03\n",
      "  1.87417494e-02 -1.62321242e-02  6.32333099e-03 -1.19028109e-02\n",
      " -2.45956202e-02 -8.76382484e-03 -1.32547371e-04  1.98951925e-02\n",
      " -2.70883409e-03  2.33299495e-02  1.74343032e-04 -4.96212707e-03\n",
      " -3.27591467e-05  9.63928494e-03  2.06124646e-02 -1.45136544e-02\n",
      " -2.36814740e-02 -2.92538724e-02 -2.78354819e-03 -2.97029630e-02\n",
      "  2.38671101e-02 -1.11531300e-02 -1.24891788e-02 -7.76602428e-03\n",
      " -6.63469253e-03  3.38572229e-02 -1.86139941e-04  8.07533560e-03\n",
      " -1.47358341e-02 -2.59918804e-02  3.72727828e-03  9.04338998e-03\n",
      "  6.03512916e-03  2.02604844e-02 -7.72057917e-03 -4.93867856e-03\n",
      "  6.33438048e-03  1.89398168e-02  1.10329369e-03 -6.92177641e-03\n",
      "  3.64044374e-02 -1.18724980e-02 -3.93168757e-02  1.81217422e-03\n",
      "  1.73335216e-02 -6.39680871e-03  3.93626609e-03  7.47692306e-03\n",
      "  2.68129451e-03  1.39052764e-02  4.95423116e-03 -2.90233240e-03\n",
      " -1.24491212e-02 -1.26653126e-02  4.69739121e-03 -1.66246137e-02\n",
      "  2.26814033e-02  2.40223975e-03  4.20710708e-02  1.70201923e-02\n",
      " -1.57722761e-02 -5.93132225e-03  9.62443109e-03 -2.93918049e-02\n",
      " -1.29737344e-02 -1.46644503e-02 -9.81690595e-03 -1.06157274e-02\n",
      " -2.91291850e-03 -1.28727078e-02  3.32286807e-02  5.32309471e-03\n",
      "  2.43450507e-02 -1.92235796e-02  1.68965772e-03 -1.95482180e-02\n",
      " -1.83912409e-02  2.30840029e-02 -1.84943021e-03  3.96345371e-02\n",
      "  1.88631680e-02 -2.13737044e-02  3.91475735e-02  2.65289308e-03\n",
      "  1.48674573e-02  1.10235639e-02 -8.51714247e-03  1.94884590e-02\n",
      " -5.39795655e-03 -2.07468506e-03  6.53409513e-03 -1.40311964e-02\n",
      "  5.41341020e-03  1.23373486e-02  9.24789264e-03  2.63287241e-02\n",
      "  6.72562019e-03 -2.96395625e-02 -5.57746540e-03 -1.57808553e-03\n",
      " -7.70272037e-03  1.70082829e-02 -5.44877689e-02 -1.07544294e-02\n",
      "  5.44128971e-03  2.21609098e-04  3.32286411e-02 -3.62076203e-02\n",
      "  2.56373712e-03  2.61415423e-02  4.79211576e-03  5.90950772e-03\n",
      " -2.61770510e-02 -4.39320100e-02  4.32655619e-03  5.08943665e-03\n",
      "  7.12087266e-03  1.05618259e-02 -1.65780044e-03  7.04893622e-02\n",
      " -7.07924241e-04  3.10981780e-02  1.74390572e-02 -4.26550935e-02\n",
      " -3.57886647e-03  1.23375774e-02 -1.59329412e-03 -2.47037368e-02\n",
      "  2.35460272e-02 -2.49604776e-02  1.07226245e-02  6.57545463e-04\n",
      "  8.54213359e-03  1.74835095e-02 -2.65621636e-03 -9.56799525e-03\n",
      "  4.25371599e-03 -2.53918878e-02 -8.93204868e-03 -3.61013625e-03\n",
      "  4.75435341e-03 -4.22001920e-03  1.62729354e-02 -9.81607028e-03\n",
      " -5.24082614e-03  1.46319060e-02 -7.00225834e-03 -2.73201586e-02\n",
      "  3.83198978e-02 -4.57985502e-03  1.55576321e-02 -1.58371978e-02\n",
      " -2.19418553e-02  1.46159854e-02 -1.57054418e-02 -8.70570816e-03\n",
      "  4.43084139e-02  1.78061481e-02 -1.06745237e-02  8.70216560e-03\n",
      " -1.53468605e-02 -2.21378822e-02  7.93350608e-03 -1.75656793e-02\n",
      " -2.62177312e-02  3.56641780e-02  1.20198975e-02  5.04403514e-02]\n",
      "164965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 39 testing: 27\n",
      "correctsource    24\n",
      "missed           15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    17\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.57142857 0.33333333 0.5        0.6        0.6        0.6\n",
      " 0.6       ]\n",
      "Accuracy: 0.5384615384615384\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.62      0.62        24\n",
      "       missed       0.40      0.40      0.40        15\n",
      "\n",
      "    micro avg       0.54      0.54      0.54        39\n",
      "    macro avg       0.51      0.51      0.51        39\n",
      " weighted avg       0.54      0.54      0.54        39\n",
      "\n",
      "accuracy = 0.5925925925925926\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.65      0.76      0.70        17\n",
      "       missed       0.43      0.30      0.35        10\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        27\n",
      "    macro avg       0.54      0.53      0.53        27\n",
      " weighted avg       0.57      0.59      0.57        27\n",
      "\n",
      "(444,)\n",
      "[ 2.70600102e-02 -1.44031026e-02 -1.24871737e-02  3.69281927e-02\n",
      "  3.49111489e-02 -1.62562782e-03 -2.69852478e-02 -7.61327876e-04\n",
      " -8.33645773e-03  4.02631174e-02 -5.65729818e-03 -5.65734772e-03\n",
      "  6.14572891e-03 -2.10045898e-02  2.34739711e-02  9.96918830e-03\n",
      " -3.94692899e-02  2.97449133e-02  9.32549719e-03  2.27809619e-03\n",
      " -2.11174149e-02  1.27382917e-03 -2.50916450e-02  8.72368099e-03\n",
      " -5.39746518e-03 -1.14010965e-02  9.03982325e-03 -9.44418035e-03\n",
      "  4.63952676e-02 -7.33765109e-03  1.68508751e-02  1.80087777e-02\n",
      " -8.56438248e-03 -1.12489833e-02  2.73189907e-03  2.20981325e-02\n",
      "  7.13771058e-03 -1.12082497e-02  0.00000000e+00 -6.67988266e-03\n",
      "  2.24798853e-02 -1.65728904e-02  2.36434455e-03 -2.11176392e-02\n",
      " -9.50695365e-03 -8.24518496e-03  2.66117017e-02  5.36043161e-03\n",
      "  8.55731560e-04 -8.77198606e-03  1.42207836e-02 -2.66162662e-02\n",
      "  2.11393584e-03  2.80971787e-02  1.79426524e-02 -1.23613699e-02\n",
      " -2.36423599e-02  2.94162540e-02  3.68918148e-02 -3.13474718e-02\n",
      "  2.50129472e-03  5.68601868e-03 -2.30770056e-03  1.38672083e-02\n",
      "  1.08923529e-02 -2.45349852e-02 -2.25539866e-02 -1.47094808e-02\n",
      " -2.41347592e-02  2.64818828e-02 -1.49751286e-02  5.73963038e-03\n",
      " -1.63225745e-02 -2.81132850e-03 -2.51903853e-02  5.10115466e-03\n",
      " -1.25900829e-02 -3.45964330e-02  5.77638091e-03  6.01237119e-03\n",
      " -1.46818564e-02 -1.71298000e-03 -8.74209345e-03 -2.96437289e-03\n",
      "  1.03851977e-02 -2.89586451e-02  1.51651246e-02 -2.57084481e-02\n",
      " -2.52339310e-02 -1.30217186e-02 -1.32320342e-02 -3.84493268e-02\n",
      "  3.07119004e-02  2.88550587e-02  1.56407152e-02 -2.78178544e-02\n",
      "  2.53689813e-02 -1.59884627e-02 -1.84915432e-02  1.27619467e-03\n",
      "  1.04002210e-02 -3.19376548e-02  2.40248779e-02 -6.90249578e-04\n",
      "  1.84748947e-02  1.27875728e-02 -1.19293592e-02 -6.07690355e-03\n",
      "  5.90635300e-03  1.97209026e-02  2.31220575e-03  4.59996024e-03\n",
      "  7.48238161e-03  9.53892534e-03  3.82445473e-02 -1.12851612e-02\n",
      " -2.96497519e-02 -2.71764291e-02 -1.41781311e-02 -2.02102008e-02\n",
      " -2.13190752e-02 -1.68446784e-02 -1.68496534e-02 -2.88539527e-03\n",
      "  4.12475239e-03 -1.20675156e-02 -9.09867470e-03 -2.79906022e-02\n",
      "  7.03780385e-03 -4.70530223e-03 -1.20994786e-02  8.89408820e-03\n",
      " -1.59202191e-02  2.73927691e-02 -1.44320242e-02 -1.36807189e-02\n",
      " -3.82603599e-02  9.90712811e-03  2.17449541e-02  9.53638731e-03\n",
      "  4.31125047e-03 -2.41279215e-02 -4.78665732e-03  3.27779947e-02\n",
      "  2.60231211e-02 -2.43865189e-02  4.06905940e-03  3.51425396e-02\n",
      " -5.32766834e-02 -3.47083311e-03  3.39593732e-02  3.14354963e-02\n",
      " -4.02699911e-02  5.90561380e-03  3.42291190e-02  1.47674630e-02\n",
      "  2.25157009e-02  3.12424736e-02 -8.31414395e-03  3.98505918e-02\n",
      "  6.38678192e-02 -5.57621138e-03  3.12814366e-02  5.09677255e-02\n",
      "  2.25876479e-02  4.72493297e-03  1.83461464e-02  2.45542556e-02\n",
      " -2.60191312e-02  1.85239541e-02 -1.52552196e-02  8.33412521e-03\n",
      " -2.60112221e-02  2.40445624e-02  7.36765920e-03 -4.58727362e-03\n",
      " -8.73349889e-04 -2.04864306e-02 -1.81682863e-02 -3.80985604e-02\n",
      "  1.78166079e-02  5.57687281e-02  2.96266007e-02 -9.56912972e-03\n",
      " -2.83455971e-02  9.29172274e-03 -3.26868322e-02 -4.54253398e-03\n",
      " -9.68592805e-03 -2.11969341e-02 -1.74420921e-02 -2.91241609e-02\n",
      "  4.11325525e-03 -3.34199977e-02  2.75746339e-02 -4.93275024e-02\n",
      " -1.43515133e-02 -1.74870346e-03 -9.87576329e-03  1.70907865e-02\n",
      "  2.96085526e-02  2.62771539e-02  3.30392924e-02  5.92881303e-02\n",
      "  1.87710918e-02  1.61728148e-02 -4.20914429e-03  1.23199590e-02\n",
      "  4.21850750e-02 -2.25233147e-03 -1.97836009e-02  5.05244816e-02\n",
      "  0.00000000e+00 -2.95097399e-02  1.32990801e-03  1.28898031e-03\n",
      " -1.68874624e-02  6.33694398e-03  2.63308136e-02  3.01761053e-02\n",
      " -2.10020031e-02 -1.63196351e-03 -2.95196747e-02 -2.98098446e-02\n",
      " -4.67665047e-03 -3.72729931e-02  1.01869526e-02 -3.74440944e-03\n",
      " -1.10727363e-02 -2.32627452e-02  5.20919225e-03 -3.71407776e-02\n",
      "  3.58925412e-03  1.28434244e-02 -1.20049200e-02 -9.44578628e-03\n",
      "  2.33507935e-02  1.61689340e-02 -2.30374346e-02 -2.04648502e-02\n",
      " -9.25354361e-03 -1.16258942e-02 -2.49722017e-02  6.72455260e-03\n",
      "  3.43110922e-02  4.87394538e-04 -3.31953844e-02  1.64300106e-02\n",
      "  1.26046113e-02  1.69228287e-04  3.11179810e-02  2.71874683e-03\n",
      "  9.15471589e-04 -8.15008480e-03 -3.20610939e-02  2.08395865e-03\n",
      " -1.77665531e-02  6.42037875e-02  5.77025140e-03 -2.19767008e-02\n",
      "  2.34167346e-02  7.80889847e-03 -3.25078842e-02  1.78902159e-02\n",
      "  2.37395968e-03 -2.80606791e-02 -2.12598125e-03  3.48481629e-02\n",
      " -1.45333160e-02  1.17711673e-02 -1.19260207e-02 -4.40455863e-03\n",
      "  8.95025759e-03  1.21956133e-02  4.69406269e-03 -6.83128509e-03\n",
      "  6.67829704e-03 -2.10314701e-02 -1.40145224e-02  2.36820777e-02\n",
      "  2.09320890e-02  2.09068481e-02 -8.34082600e-04 -1.16479060e-04\n",
      " -1.18356176e-02  1.68755808e-02  5.10763719e-03  3.99497180e-03\n",
      " -5.60450137e-03 -1.43623425e-02  1.25526042e-02  2.25640625e-02\n",
      "  7.98185368e-03 -3.06826556e-02 -3.58323369e-02  2.50528589e-02\n",
      " -1.91002628e-02 -5.76312346e-03  2.92593823e-03 -1.57737484e-02\n",
      "  1.39795103e-02  1.96315804e-02  1.34214269e-02 -3.99698614e-02\n",
      "  2.44437699e-02 -1.43136452e-02  2.66011307e-03  1.09525081e-02\n",
      "  3.62377053e-03 -2.30168079e-02 -7.40976612e-03 -3.50930142e-03\n",
      " -1.42056135e-02  8.97829770e-03  2.26484399e-04 -1.13998911e-02\n",
      " -1.27602508e-02  2.08992333e-02  3.39546359e-02  2.51198317e-02\n",
      " -2.60956427e-02  1.05529291e-02  2.48665361e-02 -1.59505084e-02\n",
      "  2.60840520e-02  2.33766040e-02 -8.50157444e-03  3.00846491e-02\n",
      "  1.53854535e-02 -1.11809478e-02 -2.02255163e-02  9.71069681e-03\n",
      "  3.72554902e-03 -9.38429964e-03 -1.44035712e-02 -1.63498156e-02\n",
      " -8.91173639e-04 -1.55385052e-02  1.83810499e-02 -1.96477983e-02\n",
      "  7.69769664e-03 -8.34099806e-03 -2.07538128e-02  7.18681369e-03\n",
      "  5.50939376e-03  2.60497575e-02  3.11654902e-03  9.21396431e-03\n",
      " -2.77138088e-03 -2.60114038e-02 -6.67358574e-03  1.61642539e-02\n",
      " -1.81712054e-02 -1.29473755e-02  1.65984841e-02 -3.64558172e-02\n",
      "  9.74855166e-03 -6.35147209e-03 -5.22319171e-03 -1.59480163e-03\n",
      "  8.31681737e-04  1.66116615e-02  2.18864634e-02 -4.07589959e-02\n",
      " -2.44956321e-02  4.33682805e-05 -1.25357568e-02  1.48446590e-02\n",
      " -3.63083306e-02 -3.06399251e-02 -6.51894229e-03  2.95059004e-04\n",
      "  1.28268660e-02  7.43809312e-03 -1.77983101e-02 -1.25972983e-02\n",
      "  1.50794821e-02 -1.14005799e-02 -2.04577644e-02  1.59783625e-02\n",
      "  2.05078620e-02  3.69781860e-03  1.10419816e-02  1.91889936e-02\n",
      " -2.05858701e-02 -2.43256268e-03  6.82103917e-03  8.55361502e-03\n",
      "  4.10083441e-03  3.46624136e-03 -4.62280953e-03 -1.52407220e-04\n",
      "  2.48554088e-02 -3.93000502e-03  4.27939860e-03  5.55358615e-03\n",
      " -2.44084544e-02  7.50400472e-03  8.08514732e-03 -1.09726804e-02\n",
      " -1.69841132e-02  1.38845383e-02  1.86670898e-02 -3.68897194e-02\n",
      " -1.88120183e-02 -3.01454447e-02 -2.64429110e-03 -2.56356288e-02\n",
      "  2.09103944e-02 -1.59956927e-03  1.41288771e-02  1.49510102e-02\n",
      "  2.49319886e-03  8.83382576e-03  0.00000000e+00 -4.23383211e-02\n",
      "  7.41529859e-03 -1.93687259e-02  3.09232439e-03 -4.65952815e-02\n",
      " -5.68466842e-03 -1.58847265e-02 -2.47869678e-02 -3.80997210e-03\n",
      " -6.36810097e-03  1.07779807e-02  2.06492187e-02  1.58355800e-02\n",
      "  3.75067228e-02  9.84984127e-03 -2.67424183e-03 -1.25314675e-02\n",
      "  4.52831346e-03  3.42618907e-03  6.05752835e-03  5.97680981e-02\n",
      " -1.89714872e-02  1.10218616e-03 -1.20252811e-02 -5.87193136e-03\n",
      "  4.45580550e-02  4.19868883e-03 -6.53859543e-03 -2.57758755e-02]\n",
      "199801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 20 testing: 14\n",
      "missed           11\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           8\n",
      "correctsource    6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.25       0.25       0.33333333 0.66666667 0.5        0.5\n",
      " 0.5       ]\n",
      "Accuracy: 0.4\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.33      0.33      0.33         9\n",
      "       missed       0.45      0.45      0.45        11\n",
      "\n",
      "    micro avg       0.40      0.40      0.40        20\n",
      "    macro avg       0.39      0.39      0.39        20\n",
      " weighted avg       0.40      0.40      0.40        20\n",
      "\n",
      "accuracy = 0.5714285714285714\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.33      0.40         6\n",
      "       missed       0.60      0.75      0.67         8\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        14\n",
      "    macro avg       0.55      0.54      0.53        14\n",
      " weighted avg       0.56      0.57      0.55        14\n",
      "\n",
      "(444,)\n",
      "[ 1.71484684e-02 -5.22424556e-03 -1.87740987e-02  2.03405841e-02\n",
      " -2.21218070e-02  2.30708197e-02  1.30952880e-02 -4.50914593e-02\n",
      " -9.44191745e-03 -6.08165130e-03 -1.81247108e-02 -1.85605852e-02\n",
      "  1.66724734e-02 -9.13029529e-03 -8.61440763e-04 -5.46439254e-03\n",
      "  9.25342054e-03 -1.77990785e-02  1.79699885e-02  8.48000814e-04\n",
      " -2.15414150e-02 -2.54585305e-02  2.85768716e-02 -1.64393093e-02\n",
      "  9.69066558e-03  1.65721505e-02  5.48642137e-03 -1.13771293e-02\n",
      "  4.82169030e-04 -1.38433026e-02 -2.05463842e-02  3.51549455e-03\n",
      " -6.58204118e-03  4.80777434e-02 -4.51431135e-02  4.56109980e-02\n",
      " -1.42529303e-03 -1.43677210e-02 -3.84037494e-03  3.90739231e-03\n",
      "  0.00000000e+00 -7.03709561e-03 -2.57518753e-02 -2.16752323e-03\n",
      " -6.79800353e-03  2.69618543e-03 -1.48133445e-02  1.37681740e-02\n",
      " -3.69995249e-02 -8.32391656e-03 -1.47035901e-02  1.70780533e-02\n",
      "  8.79516388e-03  2.25987275e-02 -9.51420143e-03 -7.04326616e-03\n",
      "  2.77287432e-03  3.73429164e-03 -8.88419363e-03  1.79456385e-02\n",
      " -9.67116295e-03 -1.14138129e-02 -1.34902971e-02 -8.62424075e-03\n",
      " -1.21941279e-02  2.37200505e-02 -2.59163031e-02 -1.49561986e-03\n",
      "  1.78284607e-02 -1.12363257e-02  1.40170842e-03  7.14457582e-03\n",
      "  2.07012720e-02 -1.74019169e-02 -2.25380508e-02  1.69920969e-02\n",
      "  2.16980895e-02  4.27732504e-03 -9.96154305e-03  5.66747083e-03\n",
      " -3.34594925e-02  3.64332803e-02  2.12831954e-02  4.58676468e-03\n",
      " -1.03151940e-02 -5.98922918e-03 -2.84151275e-02 -1.49728612e-02\n",
      " -2.00414125e-02  1.09711682e-02 -1.29414115e-02  2.99102872e-02\n",
      "  5.14192592e-03 -7.79697038e-03 -1.62308962e-02  1.37976793e-02\n",
      "  1.33965320e-02 -2.08606043e-02  2.09125219e-02  2.07935406e-03\n",
      "  1.82733154e-02  8.42446508e-03 -4.34403583e-02 -1.55240085e-02\n",
      "  1.65800405e-02 -9.69799796e-03 -2.94972194e-02  3.52855421e-03\n",
      " -2.87391794e-02  9.52030598e-03 -4.30644663e-02  8.40204944e-03\n",
      " -7.05942428e-03  2.95209801e-03 -6.55884252e-03 -8.28050977e-03\n",
      " -1.56817076e-02 -8.20890728e-03 -1.42220965e-02  4.45895670e-03\n",
      " -3.05733206e-02 -2.21075629e-02 -9.86667373e-03 -1.51994389e-02\n",
      "  5.23086421e-03  1.38820777e-02  1.26787433e-02 -3.41314936e-03\n",
      " -1.32012375e-02  1.11243313e-02 -1.14458108e-02 -2.13032371e-03\n",
      "  6.96257419e-03  2.45870514e-02  1.64123578e-02 -3.77414122e-03\n",
      "  1.69904025e-02 -8.21687949e-03 -3.24047396e-03 -2.74620919e-02\n",
      " -5.92566249e-03 -1.89712280e-02 -2.68316209e-02 -1.87366329e-03\n",
      "  1.72896268e-02 -2.75005550e-03 -2.27173117e-02  1.74555693e-02\n",
      "  7.76437334e-03 -8.42310635e-03  1.64154982e-02  9.58123492e-03\n",
      " -3.29132392e-03  3.39915861e-02  2.92487724e-02  1.96871849e-02\n",
      " -2.60411436e-03 -1.74801672e-03  1.08904763e-02 -2.01237415e-02\n",
      "  4.55279673e-03 -2.56174433e-03 -6.68991880e-03  2.52542047e-02\n",
      " -1.94226216e-02 -1.47105646e-02  1.33944706e-02  1.69312772e-02\n",
      " -7.05081560e-03 -3.43837429e-03  2.02416129e-03 -8.76645667e-03\n",
      " -4.62015801e-05 -6.28813973e-03 -4.02909520e-03  2.44862992e-02\n",
      " -6.22834508e-03  2.01686236e-03 -4.96093109e-02  1.23348760e-02\n",
      " -1.39386036e-02  2.00917908e-02 -3.05104669e-02 -5.49859974e-03\n",
      " -1.44282244e-02  1.06646293e-02 -4.56192632e-03  8.80338895e-03\n",
      " -1.47089495e-02  3.14517083e-04 -1.35098947e-03  8.58773414e-03\n",
      "  2.31156222e-02 -1.98064000e-02 -1.20252544e-03 -3.28529238e-03\n",
      " -3.45949516e-03  6.60816957e-03  2.03660526e-02  4.32579218e-03\n",
      " -1.64242366e-04  1.59351670e-03  1.99720386e-02 -7.04166441e-03\n",
      "  1.31608827e-02 -9.06023409e-03  2.59722182e-02 -1.28572948e-02\n",
      "  2.42506557e-02 -1.71508899e-02  8.53554129e-03  1.68164854e-02\n",
      "  1.46690267e-02  3.87356491e-04 -1.64663553e-02  1.45391944e-02\n",
      "  7.63423681e-03  2.96091250e-02 -3.09166515e-02 -2.42858119e-02\n",
      "  1.01230806e-02 -1.46432558e-02 -7.99403960e-03 -1.06025927e-02\n",
      "  1.57165876e-02  3.11407936e-03  3.60899573e-03  2.13952048e-02\n",
      "  1.73609638e-02 -2.04652737e-02  2.15693171e-02 -2.28944224e-02\n",
      "  2.47482096e-02  8.37931277e-03 -2.70307315e-02 -4.61967108e-03\n",
      " -2.67986848e-03 -3.47486712e-03  1.66985694e-02 -5.37568714e-04\n",
      "  2.25986334e-02  5.92760455e-03 -6.21540794e-03  2.14166081e-02\n",
      " -2.23924354e-03  7.40334189e-03 -1.96291736e-02  8.23639003e-03\n",
      "  2.80234993e-02 -9.52561669e-03 -9.98466809e-03 -2.11769970e-02\n",
      " -2.68094492e-03  4.24782802e-03 -4.27770550e-03 -1.36221123e-02\n",
      "  7.77607972e-03  2.90135073e-03  8.15629652e-04  1.95115551e-02\n",
      "  1.41843717e-02 -1.39796809e-02  6.04307238e-03  2.21191395e-02\n",
      "  1.14943505e-02  8.40213839e-03  7.59212454e-03  9.09361111e-03\n",
      " -2.35547965e-02  2.08299675e-02 -8.17263068e-03  1.66818979e-02\n",
      " -1.73606744e-02 -8.75272404e-03  1.19526965e-02  6.57516553e-03\n",
      " -2.23389072e-03 -4.78600066e-03 -1.74437685e-02  2.59106101e-02\n",
      " -3.76558686e-03  4.19188550e-03  1.35221817e-02 -1.06839238e-02\n",
      " -2.76203221e-02  8.10742376e-03 -1.25869371e-02  1.86451340e-02\n",
      "  1.68835735e-02 -2.16894549e-02  9.58246531e-03  3.73663318e-02\n",
      " -3.19087996e-02  3.66062701e-03  3.78329204e-03  4.70449793e-03\n",
      "  4.49078135e-03 -8.15779150e-03  2.77701411e-03 -7.60617109e-03\n",
      "  1.48233478e-02  1.85501887e-02  1.48730282e-02  2.66628502e-03\n",
      "  6.09207071e-03 -2.83493604e-02  4.44334930e-03  1.56395028e-02\n",
      " -7.37427415e-03  4.09250855e-03 -3.08125662e-02 -1.13176317e-02\n",
      "  2.25854415e-02  2.09095571e-02 -1.44715498e-02 -1.28018968e-02\n",
      "  2.03848067e-02 -3.60872985e-03 -1.06241753e-02  3.57660846e-02\n",
      " -6.75969391e-03  8.05661876e-03 -1.03418042e-02 -1.98912938e-02\n",
      "  1.21382094e-03  3.52641298e-02  1.19447482e-02 -5.83695869e-03\n",
      " -3.42112776e-03 -1.60774182e-02  2.32743383e-02  1.13960795e-02\n",
      "  1.49858768e-02  1.14325757e-02  1.73130019e-02  7.62768299e-03\n",
      " -2.83701475e-02  9.26774899e-03  4.45990121e-03  1.17748921e-02\n",
      "  5.25272703e-03 -1.87089378e-02 -9.39025781e-03 -1.11184737e-02\n",
      "  1.41981185e-02 -6.94437647e-03  9.08841875e-03  4.16781333e-03\n",
      "  3.35271542e-03 -1.43663905e-02 -1.68802483e-02  3.64876157e-03\n",
      "  1.88164485e-02 -1.98099270e-02  2.99944730e-03 -4.50564391e-03\n",
      "  6.64786103e-03  4.49116432e-03  2.18218741e-03 -1.31432725e-02\n",
      "  2.25058211e-03  3.07155444e-03 -1.23571430e-02 -3.57925422e-03\n",
      " -1.51164041e-02  4.41814566e-03 -3.16387650e-03  2.14150094e-02\n",
      "  1.12385234e-02 -1.76087850e-02  8.76935303e-03  2.28399811e-02\n",
      "  2.48864444e-02  7.02269719e-05  1.12651952e-02 -1.70316597e-02\n",
      "  2.01622125e-02  2.00878639e-02 -8.96646841e-03 -5.81361766e-04\n",
      "  3.80363277e-02 -1.70616833e-02  1.33763636e-02  2.19682763e-03\n",
      " -2.16902632e-02  1.77223832e-02 -2.39670966e-02  3.22465520e-03\n",
      "  2.16077437e-02  6.92256831e-03  1.30683011e-02  1.99375269e-02\n",
      " -6.14482308e-03  1.46569016e-02  8.91229487e-03 -4.53606483e-03\n",
      " -6.51175173e-03  7.07582111e-03 -2.00243482e-02 -1.05671609e-03\n",
      " -1.18687496e-02  2.25844114e-02  1.59656557e-02 -1.84707512e-02\n",
      "  2.95804269e-03 -8.91187740e-03 -1.89791195e-02 -1.56568841e-02\n",
      "  3.24354489e-02  4.18563658e-02  1.25317019e-02 -2.03196252e-02\n",
      "  2.05204845e-03 -1.44207361e-04  9.99793601e-04 -1.73790784e-02\n",
      " -2.72399272e-03  5.09195585e-03 -7.24198783e-03  3.41227362e-03\n",
      " -4.05506876e-02 -7.31467332e-03  3.43813470e-03 -1.45232354e-02\n",
      " -7.28844434e-03 -7.32565484e-03  1.81092755e-03 -1.23953900e-02\n",
      " -1.13753167e-02 -1.98026586e-03  1.58728183e-02  7.29039372e-03\n",
      "  5.93745429e-03  1.63505716e-02 -1.18360563e-02  9.96901172e-04\n",
      "  6.84269236e-03  1.19255322e-02 -5.39147117e-03 -2.17480357e-02\n",
      "  2.56171025e-03  8.81384292e-03 -1.81736134e-02  6.97378928e-03]\n",
      "247659\n",
      "training: 34 testing: 23\n",
      "missed           25\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           17\n",
      "correctsource     6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.5        0.8        0.6        0.5        0.75\n",
      " 0.75      ]\n",
      "Accuracy: 0.6470588235294118\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.33      0.33      0.33         9\n",
      "       missed       0.76      0.76      0.76        25\n",
      "\n",
      "    micro avg       0.65      0.65      0.65        34\n",
      "    macro avg       0.55      0.55      0.55        34\n",
      " weighted avg       0.65      0.65      0.65        34\n",
      "\n",
      "accuracy = 0.6521739130434783\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.33      0.33      0.33         6\n",
      "       missed       0.76      0.76      0.76        17\n",
      "\n",
      "    micro avg       0.65      0.65      0.65        23\n",
      "    macro avg       0.55      0.55      0.55        23\n",
      " weighted avg       0.65      0.65      0.65        23\n",
      "\n",
      "(444,)\n",
      "[-6.19093046e-03 -4.92878367e-02  1.95716077e-02 -1.69461715e-02\n",
      "  1.97282725e-02 -2.32867669e-02  1.55267779e-03 -2.20317001e-02\n",
      "  6.98568563e-03 -8.50946931e-03 -1.96980370e-02  1.41089956e-02\n",
      "  9.05490773e-03 -7.67425989e-03 -1.11299498e-04  7.46857923e-03\n",
      "  1.26099526e-02 -2.35758756e-02  1.11972134e-03  3.52339231e-04\n",
      " -1.26616421e-02  5.22787427e-03  9.69729448e-03  3.68089091e-02\n",
      "  1.59379742e-02  2.01594487e-02  4.24264071e-02 -2.27509450e-02\n",
      "  9.21337955e-04 -2.50242343e-02 -3.43977506e-03 -2.90064891e-02\n",
      " -2.32257999e-03  2.47360792e-02  2.66052094e-02  1.12910627e-02\n",
      "  2.69223909e-04 -2.80582797e-02  0.00000000e+00  4.00199596e-03\n",
      "  1.56944249e-03 -1.30554895e-02  8.01036279e-03 -1.41317830e-02\n",
      "  3.32582026e-02 -1.53960524e-02 -2.18137568e-03  9.40431767e-03\n",
      " -2.24593484e-02  1.72112008e-02 -1.03369158e-02 -4.61871435e-03\n",
      "  1.56022282e-02 -7.73320475e-03 -1.91720524e-02  7.69454006e-03\n",
      "  1.55242534e-02 -2.16775348e-02  7.71606517e-03  4.67670878e-03\n",
      "  1.48195657e-03  4.37507807e-04 -1.22112161e-02 -1.89042983e-02\n",
      "  1.12282343e-02 -6.01807537e-03 -1.72426669e-02  3.21212614e-04\n",
      " -1.13550365e-02 -2.30383434e-03  4.60844724e-03  2.35565029e-02\n",
      "  2.14509192e-02 -7.73286907e-03 -1.86010745e-02  3.20906901e-03\n",
      " -3.20391811e-02 -4.39259576e-02  4.46758686e-03 -1.68279056e-02\n",
      " -4.37908138e-02 -1.45072170e-02  7.63452235e-03 -2.25874987e-02\n",
      "  3.13457488e-02  1.02685169e-02 -1.18674487e-02 -1.03571425e-02\n",
      "  4.17827974e-03 -4.86779575e-03  2.75468269e-03 -9.99847665e-04\n",
      " -2.16400288e-02  2.34548732e-02 -1.17756395e-03  1.51905601e-02\n",
      " -2.16415378e-02  3.73803157e-03 -1.98809550e-03  2.42102118e-02\n",
      "  2.42497949e-02 -1.90502274e-02 -1.65588937e-02 -1.07894078e-02\n",
      "  2.20652966e-02  9.73103406e-03 -3.50122744e-02 -1.64489525e-02\n",
      " -3.41713646e-02  1.31009230e-02 -2.77622747e-02 -3.65323462e-03\n",
      "  1.59215161e-02  1.34513101e-02 -1.84415262e-04  1.70226068e-02\n",
      " -1.04593201e-02 -2.79478237e-02  1.08785435e-02 -3.91665317e-02\n",
      "  1.27198507e-02  1.14772875e-02 -6.43319929e-03 -3.62980273e-02\n",
      "  2.02330847e-02 -9.38392842e-03  6.91908131e-03  1.12916554e-02\n",
      "  8.07629050e-03  9.84175337e-03 -1.19546785e-02  4.00752023e-02\n",
      " -5.54837816e-03 -2.69752308e-03 -9.02893835e-04  6.74668890e-03\n",
      " -6.74773341e-03 -2.20793955e-02 -3.23846804e-02 -3.51962165e-03\n",
      " -2.02191390e-02 -5.20510433e-03  2.57662994e-03  2.54148866e-04\n",
      "  4.94975042e-03  2.44243111e-03 -1.40083808e-02 -2.06425693e-03\n",
      "  1.25082760e-02 -7.03474741e-03  4.61389524e-03  4.24132769e-02\n",
      "  1.54618061e-02 -1.92290711e-02  1.57471463e-03 -7.65888087e-03\n",
      " -1.48564817e-02  1.17324285e-02 -6.63280378e-03  5.03775737e-03\n",
      " -2.23561500e-02 -2.82956472e-02 -1.56955659e-02 -6.31340607e-03\n",
      "  2.68495993e-03  1.38851122e-02 -4.00455230e-03  3.32505362e-02\n",
      " -1.27454769e-02  7.23983331e-03 -2.31468705e-02  1.45762328e-02\n",
      " -1.13374902e-02  2.08326427e-03 -1.60051555e-02 -2.63954756e-02\n",
      " -3.93051949e-03 -2.50142342e-02  1.33267293e-02 -3.25027508e-02\n",
      " -1.19865988e-02 -1.51965152e-02  3.32684002e-02 -1.78649200e-02\n",
      " -3.29956569e-02 -2.70643195e-03 -2.83141696e-02 -1.97204013e-02\n",
      " -1.30359278e-02 -8.97269477e-03  3.79093024e-03  6.37459691e-03\n",
      " -8.00979829e-03  5.30076076e-03  1.29856328e-03  2.93951678e-03\n",
      " -2.28509459e-02  1.55694173e-02  3.03383140e-03  3.66086136e-03\n",
      "  1.67422979e-02 -1.05112479e-02 -1.27042989e-02 -2.60134007e-02\n",
      "  1.23798251e-02 -1.59109766e-02 -2.45023312e-02  5.12647111e-03\n",
      "  7.18790920e-03  1.73285836e-02 -4.55052827e-03  4.71709196e-03\n",
      "  0.00000000e+00 -2.26088495e-02  1.46805171e-02 -3.73777249e-02\n",
      " -7.40967301e-03  5.17829682e-02  2.26527106e-02  1.43129444e-03\n",
      "  3.10802522e-02 -8.81467217e-04  3.14598328e-02 -1.35900404e-02\n",
      "  4.11959157e-02  1.15333370e-02  1.55061744e-02  2.45080598e-02\n",
      "  3.52187761e-02 -2.18780273e-02  1.24222119e-02 -2.54275308e-02\n",
      "  2.73482793e-02  1.95419068e-02 -1.50559803e-02  9.13849023e-03\n",
      "  3.26132934e-03 -1.78035827e-02  1.66123530e-03 -8.53516629e-03\n",
      "  3.42477367e-02  5.71924515e-03 -1.72627425e-02  1.76836939e-02\n",
      "  1.80162900e-02 -2.09900338e-02 -1.66001359e-02  2.68299215e-02\n",
      " -2.13938731e-02  1.78170789e-02  4.19198330e-02 -2.58419142e-02\n",
      "  5.06841773e-03 -1.64529420e-02  1.59456310e-03  2.46922620e-02\n",
      " -1.83016417e-02 -4.39670587e-04  7.23033686e-03 -5.68924838e-03\n",
      "  1.32596104e-02  7.38437800e-04  1.53765340e-02 -2.88772585e-02\n",
      " -1.35699194e-02 -6.62109966e-03 -8.12858638e-03 -1.31311628e-02\n",
      "  5.49475979e-03  2.35945661e-02  7.88153764e-03 -8.82884177e-04\n",
      "  6.60530003e-05 -2.08293687e-02  2.60358853e-03 -7.08203322e-03\n",
      " -6.53516847e-04 -3.07481699e-03 -1.49903413e-02  1.99129292e-02\n",
      "  7.73086547e-03  3.98479395e-03  8.87553080e-03 -3.43876975e-03\n",
      "  9.89735959e-03  9.95400414e-03 -1.49022032e-02  1.25172923e-02\n",
      " -2.06521797e-02  9.71518435e-03 -1.63689817e-02 -1.29605040e-02\n",
      "  2.45065701e-02  1.02660840e-02 -1.77519218e-02  1.30742999e-02\n",
      "  2.75870758e-03 -2.69167965e-02  1.24565019e-02  1.20500455e-02\n",
      " -4.25093571e-03  2.52687207e-03  3.70409868e-02 -1.62647065e-02\n",
      " -3.37050289e-02 -2.11731979e-02 -2.50981176e-02  9.91348229e-04\n",
      " -1.59102606e-02 -2.61497101e-02  1.02284437e-02  2.55722200e-03\n",
      " -1.57325830e-02 -9.07642975e-03  8.17139760e-03  2.46222152e-03\n",
      " -9.11191443e-03  5.76090722e-03  3.17787158e-02  1.97979782e-02\n",
      " -3.29582238e-03 -4.75299848e-02  9.91672106e-04 -1.30287992e-02\n",
      " -3.89656678e-03  1.98568554e-02 -1.14257313e-02  1.60659873e-02\n",
      "  1.23877364e-02  1.08416705e-02  2.87824268e-03  6.02263221e-03\n",
      " -4.11584558e-03  4.00101288e-03 -2.65103117e-02 -1.70416184e-02\n",
      "  6.23339406e-03 -6.36616519e-04  1.09619938e-02  2.05331164e-02\n",
      "  6.34224288e-03 -3.44092228e-02 -2.04839510e-02  1.35361612e-02\n",
      " -3.90706625e-02  7.14920847e-03 -1.97534303e-02 -3.30141356e-02\n",
      " -4.11575902e-02 -1.32119112e-02 -1.63037011e-02 -4.20113010e-02\n",
      "  8.37584712e-03  6.25530768e-04 -2.89199819e-03 -8.42060237e-03\n",
      "  8.35812737e-03 -6.96167536e-03  1.22298317e-02  6.57466613e-03\n",
      " -2.48125047e-02  1.96272078e-03  1.92619245e-02  3.58907032e-03\n",
      "  6.86288761e-04  6.99692919e-03 -1.42280418e-02  5.19523718e-03\n",
      " -1.71674112e-03  1.19125371e-02  1.77014381e-02 -4.68650138e-02\n",
      "  1.41199052e-02 -2.67501899e-02 -7.96073159e-04 -1.64771883e-02\n",
      " -1.04788398e-02 -2.88133246e-02 -2.08193689e-02  1.04998257e-02\n",
      "  1.69526907e-02  5.33789182e-03  9.66980471e-03 -3.85205396e-03\n",
      " -2.30258019e-02  3.30966606e-02  6.73136717e-03  1.46986300e-02\n",
      "  1.88290965e-03 -1.42825323e-02  1.79764750e-02 -1.76204794e-02\n",
      " -2.05406537e-02 -3.54731948e-02 -1.99233787e-02  1.55287294e-02\n",
      " -2.60365056e-02  1.42809018e-02  3.61553533e-03  5.99361598e-03\n",
      "  2.05856419e-02 -3.47758726e-03 -9.50647663e-03  8.45648322e-03\n",
      "  2.46493402e-02 -8.64797409e-03  2.25490838e-02 -1.61839736e-02\n",
      " -2.05024126e-02 -3.64348352e-03  9.17693374e-03  1.31997840e-02\n",
      "  1.52152001e-02 -3.13963074e-03  0.00000000e+00 -5.61481780e-03\n",
      "  4.90440754e-03  1.91891275e-02  5.65796438e-03 -9.72157358e-03\n",
      " -6.15058907e-03 -1.68055721e-02 -4.25439248e-03 -2.36432398e-02\n",
      " -2.22649174e-02 -2.77551130e-03 -5.12381321e-03  1.82167989e-03\n",
      "  1.19689329e-03 -8.49687261e-03 -1.69003082e-02  2.57743098e-03\n",
      " -3.15299621e-02 -1.53019805e-02  6.25668989e-04  2.50092397e-03\n",
      "  1.58049532e-02  1.03864632e-02 -2.64721036e-02  1.90419373e-03\n",
      " -7.15956823e-03  9.52158736e-03  1.43028140e-02  2.65703884e-03]\n",
      "255499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 39 testing: 27\n",
      "correctsource    23\n",
      "missed           16\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.57142857 0.71428571 0.6        0.4        0.6        0.4\n",
      " 0.6       ]\n",
      "Accuracy: 0.5641025641025641\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.65      0.64        23\n",
      "       missed       0.47      0.44      0.45        16\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        39\n",
      "    macro avg       0.55      0.54      0.54        39\n",
      " weighted avg       0.56      0.56      0.56        39\n",
      "\n",
      "accuracy = 0.5185185185185185\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.56      0.58        16\n",
      "       missed       0.42      0.45      0.43        11\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        27\n",
      "    macro avg       0.51      0.51      0.51        27\n",
      " weighted avg       0.53      0.52      0.52        27\n",
      "\n",
      "(444,)\n",
      "[-5.72727870e-02 -3.38075093e-02  4.05004223e-02 -3.38958210e-02\n",
      " -8.54209631e-03 -8.73615627e-03  6.82215325e-03 -4.20435289e-02\n",
      "  8.36697128e-03 -4.46824226e-02  3.27880837e-02 -2.55945437e-03\n",
      "  6.90650762e-02  1.23914519e-01 -1.93948024e-02  4.58916878e-02\n",
      "  3.14387317e-02  6.91183307e-03  9.82461319e-03 -1.63013054e-01\n",
      " -3.22509964e-02 -2.33080114e-02 -6.47328908e-02 -2.32881132e-02\n",
      "  3.29146942e-02  1.19899189e-01 -4.19507054e-02  2.14428095e-02\n",
      " -3.23611991e-02 -2.00590675e-02  2.16560967e-03  1.34190078e-01\n",
      " -5.22842234e-02  2.06603049e-02 -3.20610724e-02  2.78213318e-03\n",
      " -3.29027894e-01  3.64467151e-03  1.33964229e-02 -3.86463192e-03\n",
      " -2.73415084e-02 -9.78681734e-02 -9.93215610e-02  5.72581147e-03\n",
      "  1.39607439e-02 -1.26839993e-02 -4.28953486e-02  1.34797854e-02\n",
      "  3.82195158e-03 -2.87452312e-02 -3.67051261e-02 -5.91519856e-02\n",
      " -1.13618935e-02 -4.98189284e-02  3.15525080e-04 -2.14986986e-02\n",
      "  2.13821003e-02 -4.28646672e-02  4.53251827e-02  7.50680630e-03\n",
      "  3.58203860e-02  2.98712910e-02  1.23473662e-01  8.46740537e-03\n",
      "  1.02710832e-01  1.35373469e-02 -2.36125811e-02  1.77602556e-02\n",
      " -2.28381841e-02 -9.04380988e-02 -3.70233114e-02  6.19223938e-02\n",
      " -3.41579686e-02  2.01908276e-02  2.73793945e-03 -1.21798325e-01\n",
      "  1.40060212e-03  6.21833103e-03 -5.42087719e-01 -2.79678801e-02\n",
      "  5.64052524e-01  6.41963862e-02 -2.48067029e-04 -1.92532385e-02\n",
      " -4.27203247e-03  2.04315618e-02 -9.75060204e-03  5.60669449e-01\n",
      "  2.90479978e-01  2.33268691e-03  5.53868618e-02 -4.37158107e-03\n",
      " -3.73762225e-02 -2.88106655e-01  1.19838633e-02 -3.44811858e-03\n",
      "  1.08883553e-01 -2.95260440e-02 -2.30354027e-01 -2.40277781e-02\n",
      " -1.14204373e-02  1.71580993e-02 -3.33826303e-03  1.21338861e-02\n",
      " -6.48210536e-03  6.98366750e-02  1.44203974e-02  3.10998143e-02\n",
      " -1.23275010e-02 -3.39064620e-02 -6.90404574e-02  6.92610419e-02\n",
      "  1.94387313e-02 -1.69887432e-03  1.89693388e-01  5.68134946e-02\n",
      " -5.56287392e-03  1.82243039e-01  5.64377537e-03 -9.19639890e-03\n",
      " -2.99737272e-03  1.68163829e-02  1.34161216e-01  1.46504565e-02\n",
      " -4.37017343e-02 -3.77696805e-02  1.26865245e-01  4.99184864e-03\n",
      "  3.20815857e-02  4.19797650e-02  1.64004166e-02  3.12604667e-02\n",
      " -6.14136234e-02 -3.08875699e-02  1.78650050e-03 -1.10704583e-01\n",
      "  1.35765941e-01 -4.88286976e-04 -2.09087115e-02 -2.01861674e-01\n",
      " -2.77879973e-02 -1.20429691e-01  4.52447249e-03  4.43357089e-02\n",
      "  4.43651833e-02  4.70215062e-03 -3.14100670e-02  7.98924493e-02\n",
      "  1.72879214e-01  9.32795721e-02 -3.47824084e-02 -2.85157711e-01\n",
      " -1.21766954e-02  2.37979610e-01 -4.25503561e-02 -2.98665571e-02\n",
      "  8.56351353e-02  4.58251329e-03  3.82877474e-03 -1.50247678e-03\n",
      " -6.11016217e-02 -3.08059514e-02 -3.19985307e-02 -1.35237477e-02\n",
      "  9.40940751e-02 -4.68290336e-02  1.59131906e-01 -1.00888688e+00\n",
      "  2.93378253e-03 -5.89669155e-02  3.23488544e-02 -4.64252680e-02\n",
      "  2.89521838e-02 -1.75652871e-02 -7.04539884e-02 -1.41889348e-01\n",
      " -9.97225559e-03 -2.81159053e-03  7.33169630e-03  1.54118940e-02\n",
      "  2.82001298e-02 -1.49768726e-03  2.34427251e-02 -6.24271975e-02\n",
      "  3.88689848e-03  2.95810009e-02  4.26055519e-03 -6.17273353e-02\n",
      " -7.63177731e-01  2.90020675e-02  8.79343057e-01 -2.75958981e-01\n",
      "  2.06040801e-02  3.25582076e-03  2.65199050e-02  1.60886335e-02\n",
      " -1.95775729e-02 -1.03924827e-02 -5.05473385e-02  3.74013553e-02\n",
      "  1.35268163e-02 -2.87359543e-01 -1.83215620e-01 -1.41457752e-01\n",
      "  6.32341160e-01 -6.93165329e-02  1.91474246e-02 -1.34621017e-02\n",
      "  5.68976209e-02  1.05150183e-02  1.50728120e-02 -2.94393714e-01\n",
      "  4.66962618e-01  2.67866683e-03 -8.83148173e-03 -6.60355908e-01\n",
      " -6.55412119e-02 -1.42065926e-01  5.89974834e-02 -7.99843807e-02\n",
      "  8.82422975e-03  3.19636528e-02 -2.17819650e-02 -7.43426088e-02\n",
      "  4.87311633e-02 -9.07885412e-03 -7.58367123e-03  2.14287776e-01\n",
      " -1.17680410e-01 -2.63266704e-01  1.44613470e-01 -2.39496194e-02\n",
      " -1.80570227e-02  1.64994532e-02 -7.06903637e-02  1.83440129e-02\n",
      " -3.08412424e-02  4.03800450e-02  4.91947571e-02  4.15611788e-03\n",
      " -8.97442191e-03  8.27797882e-03 -7.25435749e-02  5.63725235e-04\n",
      "  5.61401510e-02 -6.90218199e-02 -3.94055664e-02  1.46754683e-02\n",
      "  6.91628371e-03 -1.49797222e-02  1.33161473e-01  5.08590065e-03\n",
      "  2.70219693e-02 -2.03182684e-03  2.94418392e-03  1.75600900e-02\n",
      "  2.42682990e-02  6.48294778e-02  2.67166655e-01 -2.18092280e-02\n",
      "  3.15995302e-03 -8.01984974e-02  1.66944271e-02 -1.72742603e-02\n",
      " -1.03259720e-01  2.96583589e-02 -1.73466255e-02 -1.05343135e-01\n",
      "  1.83235902e-02 -8.57770296e-02  1.46145438e-01  5.36746860e-02\n",
      " -2.36331397e-02  2.30331613e-02  6.41761611e-02 -1.35374872e-02\n",
      " -1.89378468e-02  2.15970234e-02 -2.07911884e-02  6.62880178e-02\n",
      "  1.53776440e-01 -2.05729526e-02 -5.89163349e-02  3.24448014e-01\n",
      " -4.83539630e-03 -6.00925130e-02 -1.56306892e-02 -4.27323165e-03\n",
      " -5.98026417e-01 -2.61730675e-02  3.37390820e-02  6.78484853e-02\n",
      "  3.71096358e-02  2.64666778e-01 -4.49715329e-03  1.16126018e-02\n",
      " -4.31935414e-02  3.97849283e-02 -6.14438962e-02  2.59394206e-02\n",
      " -7.69313828e-03 -1.60469649e-03 -1.60460901e-02  2.63849557e-02\n",
      "  3.52683254e-02  3.19722982e-02  2.59640777e-02 -1.69832512e-02\n",
      "  1.67316882e-02 -6.25114568e-03 -2.86943746e-02  3.65640559e-03\n",
      "  2.65608063e-01 -2.91067516e-03  5.60837366e-02  3.72263169e-01\n",
      "  2.49772995e-02  9.79289197e-02  2.64741401e-01 -3.31325132e-03\n",
      "  1.27486874e-01  2.41041228e-02  3.85608032e-02 -1.94929498e-02\n",
      "  3.53766912e-03  1.78406476e-01  5.43281586e-02 -1.47827998e-02\n",
      "  1.88389496e-01 -4.95249313e-02 -9.47817639e-03 -6.65214711e-02\n",
      " -5.67802405e-03 -2.06982090e-03 -2.05329307e-03  3.35437583e-02\n",
      "  2.67349284e-02 -4.38480172e-01 -2.08045454e-02  2.38416748e-02\n",
      "  1.59603538e-03  1.04881626e-01  1.38332773e-02 -5.24417359e-02\n",
      " -5.02128023e-01 -1.35239397e-01  8.38117117e-03  3.90459774e-01\n",
      "  1.13583645e-02  4.41071225e-02  1.51171425e-02  3.13409188e-01\n",
      " -4.27734035e-03 -1.12058075e-03  3.58884319e-02 -1.91798999e-02\n",
      " -7.73613486e-02 -2.09671626e-02 -1.87621070e-02 -7.48136532e-01\n",
      "  1.02882600e-02  1.50771610e-02 -4.63289930e-02  2.80291234e-02\n",
      "  1.07997899e-02  7.56366630e-03 -4.04869748e-02 -3.38603124e-02\n",
      "  1.76049648e-02 -7.54110215e-02 -1.21127723e-02  2.16855923e-02\n",
      " -1.92953143e-02 -1.19012974e-01 -9.28282697e-01  7.17597451e-03\n",
      " -2.67263211e-02 -1.28107898e-03  2.10246115e-01  1.81924303e-02\n",
      "  7.56157985e-01 -1.11465996e-03  3.30784590e-03 -2.29827251e-03\n",
      "  6.38804338e-02  7.74188820e-03  1.52757575e-01 -3.16817491e-03\n",
      "  1.91826722e-02  1.54353681e-02 -4.70968401e-02  1.17832068e-02\n",
      " -2.74466951e-02 -2.28968796e-02  1.68338422e-01  8.64627456e-03\n",
      "  2.43473628e-02  2.56697353e-02 -5.62219946e-03 -7.47654455e-02\n",
      "  4.75714075e-02 -7.31832127e-03 -1.45719143e-02 -4.11711896e-02\n",
      "  7.43903670e-03  1.31916519e-02 -6.72018020e-02 -4.51442101e-02\n",
      " -1.72824821e-02  2.23281415e-02  3.91117050e-02 -1.56025037e-02\n",
      "  4.98443240e-02  3.55476714e-02  9.71050201e-02 -1.70155807e-02\n",
      "  6.49963721e-03  1.80467448e-02  3.51261149e-02  6.12531484e-04\n",
      "  9.08171338e-02 -1.86546784e-01  1.13784991e-01 -7.86954740e-02\n",
      " -2.02126029e-02  5.39834054e-02 -3.58861365e-02  1.46580946e-02\n",
      " -1.39026536e-02  3.49891594e-02 -1.53184470e-02 -2.76643961e-03\n",
      " -3.71635103e-02  7.52019754e-03 -3.97464842e-03  4.82207892e-02\n",
      " -8.50242812e-03 -1.94818410e-02 -9.06261593e-03 -3.27005102e-02\n",
      " -1.71358497e-02  1.82368203e-02  6.06791541e-01 -6.71506133e-02]\n",
      "258618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 testing: 25\n",
      "correctsource    20\n",
      "missed           16\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.83333333 0.5        0.6        0.6        0.2        0.\n",
      " 1.        ]\n",
      "Accuracy: 0.5277777777777778\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.57      0.60      0.59        20\n",
      "       missed       0.47      0.44      0.45        16\n",
      "\n",
      "    micro avg       0.53      0.53      0.53        36\n",
      "    macro avg       0.52      0.52      0.52        36\n",
      " weighted avg       0.52      0.53      0.53        36\n",
      "\n",
      "accuracy = 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.61      0.79      0.69        14\n",
      "       missed       0.57      0.36      0.44        11\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        25\n",
      "    macro avg       0.59      0.57      0.57        25\n",
      " weighted avg       0.59      0.60      0.58        25\n",
      "\n",
      "(444,)\n",
      "[ 2.05095197e-03 -6.82325619e-03 -7.65225589e-03  9.17725640e-03\n",
      "  2.14283910e-02 -2.20586759e-02  1.95583246e-02  7.99835486e-03\n",
      " -2.22106000e-02  1.87051658e-02  3.94122817e-04 -9.62388938e-03\n",
      " -2.65695623e-03 -3.07260253e-02 -1.66981182e-02  3.74199067e-02\n",
      "  7.36278802e-03 -3.04814246e-02 -3.43994373e-03  3.46586518e-02\n",
      "  1.60047484e-02  1.05390339e-02  4.29291243e-03 -8.23707411e-05\n",
      "  1.52386469e-02  1.41887354e-03  8.14521275e-03  2.19170057e-02\n",
      " -1.84389805e-02  1.23427797e-02  3.36271355e-02 -4.08998917e-02\n",
      "  9.32014840e-04  9.59141237e-03  1.16591412e-02 -2.68349280e-02\n",
      " -4.88655059e-04 -9.95849318e-03 -5.79154563e-03 -1.32827240e-02\n",
      " -2.17298572e-02 -8.34352903e-03 -9.02051163e-03 -1.33370502e-03\n",
      " -1.04986792e-02 -1.32355175e-02 -1.70868617e-02  1.26358282e-02\n",
      "  1.71767038e-02 -1.62612454e-02  1.33634118e-02 -7.68878471e-03\n",
      " -7.81362796e-04  7.44743543e-03  4.99488719e-03 -2.58444885e-03\n",
      "  3.89913810e-02  2.51884248e-02  3.29139079e-03  1.89632129e-02\n",
      " -2.21265787e-02  8.78330177e-04  3.72855095e-02  1.50091211e-02\n",
      "  1.07059535e-03  3.99423077e-02  3.01539066e-02 -2.26310650e-02\n",
      " -1.51759522e-04 -5.78086641e-04  3.21372420e-02 -3.55060370e-03\n",
      " -4.72981899e-03 -3.17066138e-02 -2.54564562e-03 -9.85424991e-03\n",
      "  1.73368625e-02  1.08858434e-02 -1.00373087e-03  1.79580848e-03\n",
      " -1.06612413e-02 -3.17198069e-02 -4.97817343e-03 -8.55492001e-03\n",
      " -1.97113780e-02 -3.29901818e-02 -3.76757130e-03  4.70296194e-02\n",
      "  1.47676745e-02 -2.96302284e-02  5.00699603e-03  9.34722093e-03\n",
      " -2.13810358e-02  2.42549287e-02  7.45248569e-03  3.55756762e-02\n",
      "  7.16589351e-03 -4.88453324e-02  8.27373758e-03  3.71866014e-03\n",
      " -1.18765191e-02  2.11364668e-02 -4.03759125e-02  1.14905580e-03\n",
      "  7.25764130e-03  2.15719131e-03 -8.70438694e-03  2.96833637e-02\n",
      " -4.29097479e-03  1.76128054e-02  3.79862679e-05 -3.87460704e-03\n",
      "  6.59256313e-03  2.76809363e-03  2.62942059e-03  3.26654860e-02\n",
      " -1.62414080e-02  3.27364882e-02  7.53805286e-03  6.17545502e-03\n",
      " -7.19908976e-02  5.79546833e-02  6.04794987e-02 -2.30007767e-02\n",
      " -1.85875662e-02 -3.12430423e-02  3.13282935e-03  2.31306498e-02\n",
      "  1.13475995e-02  1.55523830e-02  3.66610557e-02 -1.65514454e-02\n",
      " -4.03228063e-02  3.09481643e-02  2.67898908e-02 -2.23804947e-02\n",
      " -3.00105025e-02 -2.85503419e-02  1.45174244e-02 -2.33266172e-02\n",
      "  2.59625441e-02 -2.81375209e-02 -3.23675915e-03  1.91748373e-02\n",
      "  2.18549792e-02 -1.30125323e-02  2.08380619e-02 -7.75449805e-03\n",
      "  6.70604528e-03 -2.07325066e-03 -1.68024583e-03  2.20263874e-02\n",
      " -2.97904952e-02 -1.30940188e-02  4.30854331e-02  2.38227344e-02\n",
      " -4.78743422e-02 -1.23718872e-02  3.78986465e-02 -6.23756163e-03\n",
      " -1.52593487e-02  1.39616685e-03  1.74773537e-02 -1.71746982e-02\n",
      " -2.35150992e-03 -2.96038684e-03 -1.92439857e-02  4.61926973e-03\n",
      " -3.17271633e-02 -1.08151634e-02 -4.37876204e-02  1.29609259e-02\n",
      "  4.69389980e-02 -3.72118416e-02 -2.75210063e-02  1.86358437e-02\n",
      " -1.75229058e-02  1.27391024e-02  6.39240390e-02  2.00430809e-03\n",
      " -8.62494120e-03 -3.36296504e-02 -1.49724086e-02  7.25844509e-03\n",
      "  1.52443985e-03 -2.30396061e-02  1.51432721e-02  6.90991044e-03\n",
      " -2.63506272e-03  9.71414009e-03 -1.57755566e-02  7.86802661e-03\n",
      "  2.85652908e-02 -3.05382164e-02  1.72751826e-02  3.12311001e-02\n",
      "  2.35196070e-02  1.05831136e-02 -1.53362667e-02 -1.47942735e-03\n",
      "  5.01869923e-03  2.00328207e-02  2.42982603e-02 -1.83182218e-02\n",
      "  6.39311404e-03 -1.21433042e-02  8.58089552e-03  1.81943132e-02\n",
      "  3.41975294e-02 -1.69132014e-02  6.39350875e-03 -1.38543978e-02\n",
      "  1.04243315e-02 -1.56280801e-02  9.22071379e-04  1.34825978e-02\n",
      " -6.33815299e-03  1.86208572e-02 -1.97649486e-02  2.20204567e-02\n",
      " -1.24688996e-02 -5.33128123e-03  2.23332453e-03 -3.24447166e-02\n",
      "  2.76478277e-02 -9.53920588e-03  8.13546705e-03 -1.93004230e-02\n",
      "  6.82309567e-03  1.80517235e-02  6.44759434e-03 -1.59539825e-02\n",
      "  5.99079655e-03  2.22418524e-02  1.66869300e-02 -1.45716185e-02\n",
      "  1.65639455e-02 -1.21269673e-02  4.28864591e-03  6.89706472e-03\n",
      " -3.79441493e-02  3.11867380e-02 -8.79300280e-03  3.08867595e-03\n",
      " -1.46324868e-03 -2.44959709e-02  5.18383538e-02  7.49974511e-03\n",
      " -7.27022501e-03 -2.92833545e-03  1.60391659e-02  1.47955936e-02\n",
      "  1.65639141e-02  9.64942873e-03 -1.35739549e-02  1.94013555e-02\n",
      " -1.82036343e-02  5.07955283e-03 -1.83212773e-02  2.21347892e-04\n",
      " -1.95472556e-02 -4.58234549e-02  6.16665031e-03 -3.50612468e-02\n",
      " -6.98389700e-03  3.09574920e-02  2.03147118e-02 -2.52465438e-02\n",
      "  5.33788337e-02 -3.14354028e-02  2.01209283e-02  3.64071074e-02\n",
      " -6.46846191e-03 -4.22518955e-03  1.10805364e-02 -9.36893294e-03\n",
      " -6.54196841e-03  3.23978020e-02 -3.94237928e-03 -1.09180693e-02\n",
      "  1.12125220e-02  1.59721273e-02 -3.09735964e-02  2.81002741e-02\n",
      " -1.63575958e-02  7.18743955e-03  2.26755757e-02  7.02137830e-03\n",
      " -1.35460872e-03 -6.00003379e-03  2.55601553e-02 -3.89097756e-03\n",
      " -3.06458565e-02 -5.06198860e-04  4.50087995e-04  1.50284874e-02\n",
      "  1.86288327e-02 -1.22735414e-02  2.69993304e-02  9.02423525e-03\n",
      "  2.49141434e-02 -6.64108710e-03  1.48420698e-02  1.39539452e-02\n",
      " -3.44603030e-02 -1.27396433e-02  2.72958137e-03 -1.99421943e-02\n",
      "  1.44399868e-02 -1.89334973e-02  3.46572027e-02  9.03115828e-03\n",
      "  4.19145809e-03  5.94960517e-03  1.17702596e-02 -3.11148543e-02\n",
      "  6.67887756e-03 -9.29806753e-03  3.37813317e-03  4.02185021e-02\n",
      " -2.11864376e-02  2.57538311e-02 -1.20465418e-02 -1.14893192e-02\n",
      "  8.03995373e-03 -3.67082781e-03 -2.93274894e-02  1.24721608e-02\n",
      " -1.27442825e-02 -1.02585634e-02  1.73728998e-02 -3.47505777e-03\n",
      " -7.78760322e-03  1.33536004e-02 -2.81910292e-04  7.64276412e-03\n",
      "  3.71983662e-03  3.01349255e-04 -2.54270510e-02  2.46599884e-02\n",
      "  2.79430296e-02 -1.87735379e-03  1.21624592e-03  2.42611545e-02\n",
      " -1.81055432e-02 -1.90019380e-02  3.22018310e-02 -3.86439358e-02\n",
      "  2.23997379e-03 -3.19406327e-02  1.66289606e-03  3.20613165e-02\n",
      " -7.52314505e-03  8.73646614e-03 -2.77182625e-02 -1.84988061e-02\n",
      "  1.21782547e-02 -2.51808143e-03 -1.30794575e-02 -1.78010868e-02\n",
      " -2.34119126e-02  3.31264725e-02  2.27158376e-02 -1.90006343e-02\n",
      " -4.11738490e-02  3.98435819e-02 -1.87379358e-02 -2.12505536e-02\n",
      " -6.08639716e-03 -9.45815324e-05 -1.55418225e-02 -1.35132413e-02\n",
      " -1.01732930e-02  1.19970516e-02 -1.66327318e-02 -1.80661649e-02\n",
      " -3.00940759e-02  2.53645198e-02 -1.78317542e-03 -3.71899643e-02\n",
      "  1.24741913e-02 -5.31021476e-03 -2.31788297e-02  2.56221064e-02\n",
      "  2.26038281e-02  1.88825218e-02 -3.74261146e-02 -2.06656582e-02\n",
      " -6.47021710e-04  1.25237423e-02  1.16480408e-03  6.37218580e-02\n",
      "  4.40307504e-03  4.03972754e-02 -2.30830169e-02  2.76409253e-02\n",
      "  2.33503976e-02  2.12330856e-02 -3.78688424e-03  1.47482297e-04\n",
      " -8.05723014e-03  1.91401640e-02 -4.20986337e-03 -1.93658596e-02\n",
      "  7.18996338e-03  6.17402662e-03  9.96474319e-03 -7.09287821e-03\n",
      "  7.89423551e-03  1.69471098e-02 -2.84003252e-02  3.89899386e-02\n",
      " -4.48418467e-04 -9.05375766e-04 -6.06252815e-03 -1.08111100e-02\n",
      " -4.26682437e-06  1.15305963e-02  1.44640352e-04  1.39210818e-02\n",
      "  2.34136232e-02  2.29884230e-02  1.63643043e-03  3.33383408e-02\n",
      "  1.40041527e-03 -2.32259646e-02 -4.06242502e-03  3.13462264e-02\n",
      " -7.79205222e-03 -5.96125161e-03  6.32680937e-03 -6.00539198e-03\n",
      " -3.68426461e-03 -3.89475261e-02 -4.94811116e-03  1.72492734e-02\n",
      "  4.32304862e-02  1.75089176e-02 -1.23088300e-02 -1.28267251e-02\n",
      " -3.20946727e-02  7.89858413e-03 -1.00226316e-02  2.02129362e-02]\n",
      "271596\n",
      "training: 36 testing: 25\n",
      "correctsource    22\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5 0.2 0.8 0.8 0.6 0.4 0.4]\n",
      "Accuracy: 0.5277777777777778\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.59      0.60        22\n",
      "       missed       0.40      0.43      0.41        14\n",
      "\n",
      "    micro avg       0.53      0.53      0.53        36\n",
      "    macro avg       0.51      0.51      0.51        36\n",
      " weighted avg       0.53      0.53      0.53        36\n",
      "\n",
      "accuracy = 0.52\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.75      0.67        16\n",
      "       missed       0.20      0.11      0.14         9\n",
      "\n",
      "    micro avg       0.52      0.52      0.52        25\n",
      "    macro avg       0.40      0.43      0.40        25\n",
      " weighted avg       0.46      0.52      0.48        25\n",
      "\n",
      "(444,)\n",
      "[ 0.00487298 -0.00925764 -0.04377892 -0.00675367  0.00255945  0.02669513\n",
      "  0.02762165 -0.02646936  0.01697827 -0.01345121 -0.00321009  0.01168339\n",
      " -0.02327234 -0.00594368  0.00738537  0.03442764  0.00545543  0.02101276\n",
      " -0.00286973 -0.0146268  -0.00092927 -0.05640632  0.02026565 -0.02497803\n",
      " -0.01480263 -0.00018473  0.00998727  0.01440804  0.00803827  0.03664324\n",
      "  0.01465655  0.01744242 -0.02024828 -0.01994642  0.03814995  0.01493756\n",
      "  0.00309276  0.02738108  0.01450447 -0.02635296  0.         -0.00263833\n",
      "  0.0069958   0.0051599  -0.0018102   0.0264283  -0.00484678  0.00755132\n",
      " -0.00895379 -0.00874698  0.02360497  0.03204495 -0.02230766 -0.00939632\n",
      " -0.0336984   0.0149053   0.02449425 -0.0079535   0.00291629 -0.03479027\n",
      " -0.01055536  0.03828622  0.01165748  0.00796723  0.0003597   0.0270033\n",
      " -0.01345219  0.01171909  0.03181892 -0.00926128  0.01499188  0.0004493\n",
      "  0.02241888 -0.00524476 -0.00090337  0.03119147  0.03451342  0.00303404\n",
      "  0.01923849 -0.00965964 -0.02182203  0.02064088 -0.04046429  0.01234062\n",
      " -0.03248199 -0.00438911 -0.01940937  0.00824337 -0.01942624 -0.01841622\n",
      " -0.01852392  0.01734008 -0.00919369  0.05388071  0.00720194  0.01715062\n",
      "  0.01828647 -0.00307863  0.0036042  -0.030331    0.00622516 -0.02760432\n",
      "  0.00449243  0.01355231 -0.03334564 -0.0146293   0.01191182 -0.00293377\n",
      "  0.02113672 -0.00583025  0.02003859 -0.00990064  0.02338759  0.00213193\n",
      " -0.0194045   0.01835097  0.00422402  0.01082328  0.00765203  0.04229137\n",
      "  0.00159766 -0.04461423  0.01375378 -0.05572523 -0.00476016 -0.0155975\n",
      "  0.0203936   0.02002167 -0.03174435  0.         -0.01220919  0.00789646\n",
      " -0.02321017  0.01611972  0.00464764  0.0100567   0.00192438 -0.00620674\n",
      " -0.01672868 -0.00579368 -0.03625084 -0.01108774 -0.00975333 -0.001967\n",
      " -0.01023803  0.00884108 -0.01680842 -0.04844392 -0.01636287  0.01640228\n",
      " -0.03490852  0.01326803 -0.01028492  0.00795417 -0.01892191 -0.00950441\n",
      "  0.00929027 -0.05234713  0.00058958 -0.02867839 -0.02550397 -0.00469402\n",
      "  0.01312079  0.00181399  0.03418837  0.04053023  0.01106784  0.01243734\n",
      " -0.02954638  0.0507162   0.01129878  0.06734469  0.02817018  0.02049776\n",
      " -0.00682552  0.01716591 -0.0352133  -0.00237437  0.01758691 -0.0012498\n",
      "  0.01048115 -0.01300098 -0.0004422   0.0064458  -0.02531243 -0.02359283\n",
      " -0.00085362 -0.02232932  0.00544584 -0.03204006  0.00095773  0.01264644\n",
      " -0.02212373 -0.00943412  0.02555608  0.00223724 -0.00655677 -0.00463859\n",
      "  0.03759761  0.02053125  0.02914707 -0.01172302  0.00990566  0.01240981\n",
      " -0.04088402 -0.02804054  0.00207367 -0.02968362  0.0422912  -0.0080712\n",
      " -0.00506592  0.00487974  0.02214482 -0.0264078   0.00938795  0.00591867\n",
      " -0.01788538  0.00067453  0.07364239 -0.03713396 -0.01401273 -0.01490142\n",
      "  0.02709852 -0.02323353 -0.01837391 -0.02337844  0.000334   -0.02950701\n",
      " -0.01011204 -0.00158748  0.01341619 -0.02334326 -0.00467077 -0.00128744\n",
      " -0.0273213  -0.02128164 -0.03283657 -0.0203523   0.01227213  0.00782421\n",
      "  0.01980853  0.00284389  0.00728055  0.00623403  0.01468921 -0.02706509\n",
      " -0.01785286  0.01061983  0.02347779 -0.01147131 -0.01941956 -0.002604\n",
      "  0.00859481  0.03281291 -0.01453808 -0.00374163 -0.01305512  0.0101992\n",
      "  0.03034594  0.03919549  0.00927714 -0.02329778  0.00152362  0.01689122\n",
      " -0.00809479 -0.02579417 -0.01236367 -0.02804247  0.00897395  0.00179966\n",
      " -0.02184061  0.03002    -0.01670692  0.03221657 -0.00241153  0.00597385\n",
      "  0.01762404 -0.007514   -0.02035512 -0.00309765  0.01651381 -0.02643032\n",
      "  0.00836989 -0.00111238  0.0325803  -0.00540311  0.0273125   0.00200898\n",
      " -0.01233308 -0.00899798 -0.00824774 -0.00166489 -0.01758764  0.01336298\n",
      "  0.01750965  0.00024402 -0.00231854  0.01459626  0.02111633 -0.01930411\n",
      " -0.00085858  0.01850218 -0.01925527 -0.04857038  0.00636488  0.02395025\n",
      " -0.00954434  0.01810064 -0.03151798 -0.01766712 -0.0200183   0.02631719\n",
      " -0.05165734  0.02594056 -0.01398039 -0.02115039 -0.01663377  0.02979865\n",
      " -0.02877466 -0.02103195 -0.00474191 -0.01525194 -0.00502247 -0.01419807\n",
      " -0.01943553  0.00728843 -0.02104405  0.03980672 -0.00621659  0.01348589\n",
      "  0.02235447 -0.00519824 -0.00592877  0.00972833  0.00671874  0.02358727\n",
      "  0.03194589  0.01255496  0.0254787   0.03884654  0.02286175 -0.01966364\n",
      "  0.01802563  0.02622233 -0.01105585  0.01530934  0.01652015  0.00408496\n",
      "  0.00917072 -0.02743501  0.01831524 -0.01233442 -0.01090714  0.02234014\n",
      "  0.04134771 -0.01259286  0.00920649 -0.00699513 -0.00093269 -0.04009473\n",
      " -0.00333362  0.01718533  0.05833984 -0.02660464  0.0163901  -0.0014366\n",
      "  0.01428131  0.00024579  0.01374773  0.00264669  0.02800323 -0.00409348\n",
      " -0.00912949 -0.00317642  0.00629373 -0.0138977   0.01604272  0.00342887\n",
      " -0.00603822 -0.02309892 -0.01162042 -0.00787682  0.02874472 -0.01737265\n",
      " -0.00516077 -0.03159066 -0.03687591  0.00046433  0.00642299  0.00337352\n",
      " -0.01187093  0.01466872  0.01779617  0.01362584  0.03099995 -0.00126702\n",
      " -0.02074763  0.00038225 -0.01007058 -0.02272792  0.02421893  0.00710035\n",
      "  0.00181156 -0.00906231  0.002238   -0.00949088 -0.00133402 -0.01400808\n",
      " -0.00778107 -0.00220418  0.02611004  0.01527305 -0.04409997  0.01403051\n",
      "  0.0408796   0.00488543  0.01226186  0.01945374 -0.01624977 -0.02813434\n",
      "  0.0120692   0.01771724  0.03948749  0.01978007 -0.02074907  0.02171065\n",
      "  0.00152103  0.01113548 -0.03055351  0.01434908 -0.03392721 -0.00381869\n",
      "  0.01852939 -0.00525151 -0.00958895  0.01583568  0.04407329 -0.01774094\n",
      " -0.00317376 -0.01186582 -0.00378143  0.00162726  0.00546136 -0.03224726]\n",
      "314409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 40 testing: 28\n",
      "correctsource    21\n",
      "missed           19\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           14\n",
      "correctsource    14\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.33333333 1.         0.5        0.83333333 0.2\n",
      " 0.4       ]\n",
      "Accuracy: 0.575\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.48      0.54        21\n",
      "       missed       0.54      0.68      0.60        19\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        40\n",
      "    macro avg       0.58      0.58      0.57        40\n",
      " weighted avg       0.59      0.57      0.57        40\n",
      "\n",
      "accuracy = 0.4642857142857143\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.47      0.50      0.48        14\n",
      "       missed       0.46      0.43      0.44        14\n",
      "\n",
      "    micro avg       0.46      0.46      0.46        28\n",
      "    macro avg       0.46      0.46      0.46        28\n",
      " weighted avg       0.46      0.46      0.46        28\n",
      "\n",
      "(444,)\n",
      "[ 1.96653159e-03 -1.17995974e-02 -2.16090100e-03  1.01347655e-02\n",
      " -3.08442246e-03  1.74433210e-02  2.12985732e-02 -2.19541001e-02\n",
      "  1.24066096e-02 -2.93842525e-02 -1.53836546e-02  2.55059655e-02\n",
      "  5.11914767e-04 -6.31535400e-03  2.81301416e-02  3.10064078e-02\n",
      "  5.46083260e-03  8.21455106e-04 -1.26982718e-02 -2.30302649e-02\n",
      " -9.01959881e-03 -3.28211664e-03  1.13718070e-03  4.61594357e-02\n",
      " -1.78340375e-02 -1.46052042e-02  2.18228379e-02  1.55688459e-02\n",
      " -1.95598017e-02 -1.21788035e-02  2.29790989e-02  7.11881727e-04\n",
      "  1.43421400e-02 -6.83482785e-03 -5.23355694e-03 -3.72352551e-02\n",
      " -1.46835671e-02  1.11063686e-03  4.78815191e-02 -2.19871289e-02\n",
      "  0.00000000e+00  2.31747428e-02 -5.60510675e-03 -2.12987427e-02\n",
      "  2.85237911e-02  9.85838055e-03 -5.60846405e-02 -2.99111937e-02\n",
      " -1.31314238e-02  1.66817523e-03 -2.60804456e-04 -2.66901449e-02\n",
      "  4.93460831e-02 -1.16754014e-02  2.08083678e-02 -2.49398834e-02\n",
      " -1.04168438e-02  5.28461102e-02 -6.39328294e-03  3.68812345e-02\n",
      "  2.02649649e-03 -5.41560002e-02  4.15743288e-02 -2.18391051e-02\n",
      "  2.48953021e-02  4.72962641e-02 -8.93500511e-03 -4.96375795e-02\n",
      " -1.28260234e-02 -4.23905529e-02 -4.07507618e-02 -2.46397475e-02\n",
      "  9.25501959e-05  7.45594052e-05 -7.98293132e-03 -6.18292803e-03\n",
      "  2.44830199e-02  8.43585778e-03  1.59666860e-02 -3.35648383e-02\n",
      " -1.74037088e-02  1.13503143e-03  2.28347687e-02 -5.25254883e-02\n",
      " -3.72282876e-03 -9.96804145e-03 -7.51366547e-03 -3.88376809e-02\n",
      "  1.13939596e-02 -6.49823892e-03  3.49540906e-02  1.99381791e-02\n",
      "  5.87927561e-03  5.79415320e-03  7.17013270e-03  4.61639907e-02\n",
      " -3.42257719e-02 -1.92439582e-02 -1.10913772e-02 -7.80105711e-03\n",
      " -7.72605038e-04 -1.05687243e-02 -2.35578068e-02 -1.66305105e-02\n",
      "  5.80866711e-03  1.75436434e-03 -1.99647130e-02  5.70855345e-03\n",
      " -2.87569219e-02 -5.70219831e-03 -1.00251979e-02  1.12033224e-02\n",
      "  3.08151259e-02  3.57000386e-02  3.93971105e-03  6.49364423e-03\n",
      " -2.97456305e-02 -4.55030297e-03  2.41037889e-02  8.24487729e-03\n",
      " -2.52995242e-02 -4.63725795e-03  1.56850873e-03 -1.21413037e-02\n",
      "  1.36018219e-02 -3.54624724e-03 -3.49889772e-03 -2.49846805e-02\n",
      "  2.32278326e-02  0.00000000e+00  9.15628746e-03 -2.54859015e-03\n",
      " -1.20144436e-02  1.93183140e-02 -1.28004982e-02 -4.01692468e-03\n",
      "  4.14726979e-02 -1.63301217e-02  3.11868949e-02  1.76851833e-02\n",
      " -3.31756980e-02  6.31670865e-03  2.76524701e-02  6.99651593e-02\n",
      "  3.53429431e-02  8.13650383e-03 -3.06190628e-02 -3.43876515e-02\n",
      " -4.61079683e-02 -1.92632131e-03 -1.35868713e-02 -5.17646463e-03\n",
      "  3.31153015e-03 -4.69056204e-04 -2.51027987e-02  2.40166352e-02\n",
      "  5.85241815e-04  2.82652370e-02  4.59220452e-03  1.45662948e-02\n",
      "  1.72590476e-02  3.18694459e-02  5.38640994e-02  1.49183947e-03\n",
      "  2.07234554e-02 -1.96685451e-02 -3.84376275e-02 -1.30628192e-02\n",
      " -1.13548318e-02 -2.61396131e-02  8.67375178e-03 -3.19947161e-02\n",
      " -1.31831655e-02 -1.71654586e-02 -2.55384158e-02 -2.57834197e-02\n",
      " -1.37302828e-02 -1.67708156e-02 -5.95527084e-03 -1.96283043e-02\n",
      " -3.47846390e-02  4.69650898e-03  1.67893871e-02  1.88526432e-02\n",
      "  2.77775983e-02  8.00671035e-03 -4.42072315e-03 -4.14499655e-02\n",
      "  2.16014415e-02  1.37539211e-03 -7.29249972e-03 -7.58552612e-03\n",
      " -9.05298617e-04 -4.46045006e-04  3.90854465e-02 -1.20521613e-02\n",
      " -4.11816344e-02  2.98845723e-02  1.73218200e-03 -1.84977510e-02\n",
      " -4.29132290e-03  2.29428562e-03  3.24293154e-02 -2.03567382e-02\n",
      " -1.31896043e-02 -5.53721059e-03  4.60690670e-03 -3.03402485e-02\n",
      "  3.03897558e-02 -1.58180664e-03 -1.08813955e-02 -3.04832005e-02\n",
      " -1.34529488e-02 -1.72805240e-02 -2.36212968e-02 -3.00899877e-02\n",
      " -4.70972666e-02  4.38973306e-04  7.71308842e-02  1.96211794e-02\n",
      "  1.48350889e-02  1.33929311e-02  5.03823730e-02  5.51274778e-03\n",
      "  6.69739701e-03 -2.50203053e-02 -9.46409069e-04 -1.72903315e-02\n",
      " -1.27282172e-02 -3.13121791e-02 -3.86613707e-04 -2.14146633e-02\n",
      " -9.12764969e-03 -2.47334359e-02 -7.50908627e-03  2.34467175e-02\n",
      " -2.67383354e-02  1.43744193e-02  1.87094731e-02 -1.00095857e-02\n",
      " -1.72857872e-02 -1.81702054e-02 -4.67917676e-02  3.98852820e-03\n",
      "  1.19942919e-02  6.10368681e-02 -8.95694518e-03  1.25433230e-02\n",
      " -8.09600789e-03 -1.70925936e-02  2.68079127e-03 -1.29407923e-02\n",
      "  1.54524370e-02 -5.73646379e-02 -2.09887469e-02  3.73033783e-02\n",
      " -4.87920061e-02  1.76870140e-02 -5.12479077e-04 -3.34135606e-03\n",
      " -1.12210770e-03  1.98593071e-02  5.75700528e-03 -2.17355468e-02\n",
      "  1.26819261e-03  8.76074733e-03  2.55679326e-04 -1.08095976e-02\n",
      " -2.24383354e-02 -7.65321427e-03 -3.24194278e-02  4.79281855e-02\n",
      " -4.83016872e-02  6.19325495e-03 -2.77832955e-02 -6.98940553e-03\n",
      " -2.29226732e-02 -5.81166858e-03 -7.04328347e-03  1.14414431e-02\n",
      " -1.15973928e-02 -6.40251857e-03  1.49711216e-02 -1.23254510e-02\n",
      "  8.33982955e-03  3.88509516e-02 -9.98580478e-03  1.16150367e-02\n",
      "  2.64408042e-03  2.26034957e-02  1.44273754e-02 -1.01636709e-02\n",
      "  1.18781794e-03 -4.52336574e-04 -1.66626300e-02  7.68200702e-03\n",
      "  1.38220818e-02 -4.25167766e-03 -2.90233861e-02 -5.14761733e-03\n",
      " -1.18084783e-02  1.46480569e-02 -1.12250761e-02 -6.16984208e-03\n",
      " -5.85506347e-02 -1.19842638e-02 -1.99598929e-03 -4.52347611e-03\n",
      " -1.46828540e-02 -3.17567388e-04 -5.54532550e-03 -2.70713725e-02\n",
      " -7.12651756e-03  8.00443716e-03  1.32665936e-02  1.95205749e-02\n",
      " -9.52530359e-03 -4.64164913e-03  1.45891240e-02  9.39542697e-04\n",
      " -2.30244247e-02  9.16446659e-03 -1.33760673e-02 -1.41069816e-02\n",
      "  3.33308784e-02  6.31832451e-06  1.42514086e-02 -1.84513859e-02\n",
      "  1.54414805e-02  1.97553127e-02  7.04503936e-03  5.63286418e-03\n",
      " -6.14412516e-03 -2.37106987e-02 -2.07244057e-02 -1.34509270e-02\n",
      "  4.91133841e-03 -1.68623424e-02 -1.18022573e-02  9.25459900e-03\n",
      " -2.69697468e-03 -1.67287359e-02  2.35827797e-02  3.57177728e-02\n",
      " -8.43419387e-03  4.97341904e-03  4.37379278e-02 -1.13469973e-02\n",
      "  9.55528894e-03  7.87689935e-04 -4.82210394e-03  1.14555283e-02\n",
      "  8.20742628e-03 -6.75298858e-03 -1.26449999e-02  2.83795763e-02\n",
      "  1.29935624e-02 -1.84383428e-02 -5.42506921e-03 -1.78150522e-02\n",
      " -4.21804179e-02 -1.90135486e-02 -1.76320155e-02  1.76922506e-03\n",
      " -2.56132159e-02  6.61259089e-03 -1.56194416e-02 -1.79627587e-02\n",
      " -1.51377197e-02  3.00132480e-02 -3.83770248e-02 -2.89028541e-02\n",
      " -3.15640102e-02 -6.14962469e-02  3.84517428e-03  2.77913364e-03\n",
      "  1.74326868e-02 -2.73429676e-02 -1.61550021e-03  9.45077788e-03\n",
      " -1.02414766e-02  1.54606365e-02 -3.64019993e-02  2.25994919e-02\n",
      " -2.92351375e-02  3.87983863e-02 -1.38696398e-02 -4.16898448e-02\n",
      "  1.18708610e-02 -3.23271863e-02  1.63537475e-02 -5.10692689e-02\n",
      "  4.03298526e-02  2.23572458e-03  5.36723162e-03 -7.15046426e-03\n",
      "  2.49897346e-02  8.01206917e-03  5.69795365e-03  1.99387332e-02\n",
      "  2.63677992e-03  3.71233107e-02 -5.69754239e-03  8.30604819e-03\n",
      " -2.11695031e-02  3.76424721e-03  4.08897181e-03 -8.81448445e-03\n",
      " -1.75489292e-02 -8.45313739e-03  9.39765648e-03  2.83224598e-02\n",
      " -8.82217852e-03 -3.16890685e-03  2.71878793e-02 -1.73770052e-02\n",
      " -2.14252904e-02  2.78129479e-02 -2.84871614e-02  1.57715893e-02\n",
      "  4.18523586e-02 -1.40928141e-02 -3.70074092e-02 -5.21870539e-02\n",
      " -1.08940954e-02 -4.40420989e-02 -5.57971340e-02  3.69812473e-02\n",
      "  2.60337427e-02 -3.10152715e-02 -4.45169821e-03  1.92760600e-02\n",
      "  9.43017562e-03 -4.05529891e-03  3.37383805e-02 -6.46898560e-03\n",
      " -9.46166758e-03  1.96147841e-02 -6.01880707e-02  4.07511205e-03\n",
      "  2.06535848e-03 -1.07027308e-02 -1.56716004e-03 -4.79410187e-03]\n",
      "336665\n",
      "training: 30 testing: 20\n",
      "correctsource    16\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    11\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.4  0.8  0.75 0.5  0.5  0.5  1.  ]\n",
      "Accuracy: 0.6333333333333333\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.62      0.65        16\n",
      "       missed       0.60      0.64      0.62        14\n",
      "\n",
      "    micro avg       0.63      0.63      0.63        30\n",
      "    macro avg       0.63      0.63      0.63        30\n",
      " weighted avg       0.64      0.63      0.63        30\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.56      0.45      0.50        11\n",
      "       missed       0.45      0.56      0.50         9\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        20\n",
      "    macro avg       0.51      0.51      0.50        20\n",
      " weighted avg       0.51      0.50      0.50        20\n",
      "\n",
      "(444,)\n",
      "[-2.10505509e-02 -2.91382883e-04  2.54716426e-02 -9.57470636e-03\n",
      "  7.78193610e-03  7.12701899e-03 -1.25402235e-02  4.76891669e-03\n",
      " -2.40116119e-02  7.50179707e-03 -3.28815920e-03 -2.28520859e-02\n",
      "  7.88520583e-03 -2.74088934e-02  1.40841570e-02 -2.08074215e-02\n",
      " -8.96467693e-03  1.42397784e-02 -7.53764464e-03  1.90257703e-02\n",
      "  2.64714963e-02 -3.13344361e-02  2.37458181e-02  4.44349928e-02\n",
      " -1.62426904e-02  2.32368289e-02 -2.49371318e-02  2.47276365e-03\n",
      " -1.59305789e-02 -4.49991857e-03  9.98717166e-03 -2.24273130e-02\n",
      " -1.06891712e-02 -2.73707786e-02  1.66839882e-02  8.60516568e-03\n",
      " -5.71573543e-04 -1.72784551e-02  4.99180352e-03  1.83593837e-02\n",
      "  1.98031767e-02  6.06927699e-03 -9.87455592e-03 -3.67115115e-03\n",
      "  9.79715787e-03 -1.43758049e-02 -5.91237124e-03  1.11981154e-02\n",
      " -1.68619563e-03  6.32349095e-03 -3.24330266e-03  7.07805088e-03\n",
      "  1.67108790e-02 -1.46777425e-02 -1.45186505e-02 -1.65184748e-02\n",
      " -2.40663849e-03 -4.18516980e-03  2.93070996e-02  2.99368023e-02\n",
      " -1.18734290e-02 -4.76081422e-03 -2.24834732e-05 -1.05647134e-02\n",
      " -2.22999310e-02 -2.10705257e-02  1.82950440e-03  2.03630897e-02\n",
      "  8.76704622e-03 -4.43914630e-02  6.99108338e-04 -8.27177897e-03\n",
      " -4.47505526e-02 -1.39280366e-02  2.47508157e-02  2.13472456e-02\n",
      " -3.97405038e-03  6.44622396e-03 -4.42468470e-03  1.22278661e-02\n",
      " -6.51547742e-03 -2.80644431e-02 -1.54480544e-03 -3.83876804e-02\n",
      "  5.79704987e-03 -5.66493521e-03  2.00765714e-02  1.04162903e-02\n",
      "  1.07034750e-02  1.23938096e-02  3.29351292e-04 -7.63576485e-03\n",
      " -2.54801890e-05 -3.43058021e-03  1.84479413e-02  1.48314416e-02\n",
      " -1.30804475e-02 -2.31231874e-02  1.30794587e-02 -1.51658345e-02\n",
      " -8.37488960e-03 -4.97235557e-03 -4.06962491e-03 -6.30173476e-03\n",
      "  6.01093908e-03  3.37987256e-03 -2.32574548e-02  3.39436020e-02\n",
      " -3.23725291e-02  1.83733879e-02  4.35232825e-04 -1.10872121e-02\n",
      " -1.71612572e-02 -1.63902976e-02 -6.53583768e-03 -1.33284698e-02\n",
      "  4.39764699e-04  1.14959796e-02  2.71085331e-02  8.79764977e-03\n",
      "  2.11370273e-02  1.64799044e-02 -2.98200016e-02  1.24753575e-02\n",
      " -1.24083010e-02  2.11604204e-02 -1.94125769e-02 -1.60225544e-02\n",
      " -1.74646339e-02 -1.13976047e-03 -7.24494349e-03  4.75347511e-03\n",
      "  1.55779996e-02 -2.40937777e-02  1.62650960e-02 -4.15374894e-03\n",
      " -3.78087167e-03  2.93870527e-04 -1.74282403e-02  2.07482686e-02\n",
      " -3.50207964e-03 -1.05709701e-02  4.10600400e-02  1.48049091e-02\n",
      " -1.00775411e-02  1.32572899e-02  5.35186979e-03 -1.51975782e-02\n",
      "  3.36347896e-02  1.79873978e-02 -1.52193420e-03  8.78284695e-03\n",
      " -2.20892946e-02 -3.10972821e-03  2.12426354e-02  1.96261973e-02\n",
      "  1.95511038e-03 -1.55259520e-02  4.15495071e-03  2.07367811e-02\n",
      "  4.89636473e-03  2.89138945e-04 -1.81162313e-02  3.05759300e-02\n",
      " -4.57235333e-03  4.36243037e-02 -3.55166157e-02 -3.57458691e-02\n",
      "  2.99700021e-02  1.97595791e-02 -2.51953199e-03 -1.58091537e-02\n",
      " -6.84917524e-03  1.96181742e-02 -3.36676768e-02  8.64040788e-03\n",
      "  1.10168608e-03  5.45834739e-03  3.43774382e-02 -5.60937946e-03\n",
      "  2.42766333e-02  1.03207817e-02 -6.66242829e-03 -9.19795230e-03\n",
      " -5.41545214e-03 -1.72384452e-02  6.38568692e-03  1.48112279e-02\n",
      "  1.29355458e-02 -5.85992705e-03 -1.44664563e-02  2.47825404e-03\n",
      "  5.80358864e-03  9.56501936e-04  4.74028084e-03 -1.43129443e-02\n",
      "  2.89807790e-02  4.69585458e-04 -1.10857705e-02 -2.07602583e-02\n",
      "  5.30594795e-03  1.24337728e-02  1.33824644e-02  2.51531321e-03\n",
      "  1.21667589e-02 -1.04276908e-02 -9.32100301e-04  1.85965051e-03\n",
      "  2.26877822e-02 -2.48704922e-02 -3.26155484e-03 -7.05125168e-03\n",
      " -2.69837686e-02  2.90351459e-02  4.50570097e-03  2.51954761e-02\n",
      "  1.53683070e-02 -7.10203736e-03  2.07448079e-03 -1.37785875e-03\n",
      "  3.54429440e-02  1.56495029e-02  4.61766829e-02 -7.54523159e-03\n",
      " -1.93945289e-02 -1.44401870e-02 -3.96925088e-02  1.08454188e-02\n",
      "  1.65618857e-02  1.19800979e-02 -9.00757007e-03 -8.61392251e-03\n",
      " -3.56738237e-03  1.81729231e-02  1.06810199e-02 -5.27726255e-02\n",
      "  2.84757509e-03 -3.90725463e-02  2.43487434e-02  7.93199573e-03\n",
      "  6.26803095e-03  1.74032537e-02 -1.84581039e-03 -1.37126102e-02\n",
      "  2.34508228e-02  2.77408435e-02  3.19924893e-02  5.85265646e-03\n",
      " -2.73113215e-02  1.91982893e-02  3.43599465e-03  1.78286901e-02\n",
      " -1.99889601e-02  2.00113516e-02  1.33711044e-03 -9.41286636e-03\n",
      " -3.23681478e-03 -6.30400129e-03  3.25734667e-02 -5.37511611e-04\n",
      " -6.45467937e-03  7.26793081e-03 -1.38867696e-02 -4.31891824e-03\n",
      "  3.66009604e-03 -5.16761642e-03 -2.66602588e-03  1.38857212e-02\n",
      " -2.00388707e-03  3.80789112e-02 -1.47199911e-02 -2.50540357e-02\n",
      "  1.03045228e-03 -9.06038047e-03 -1.06598014e-02  4.29583835e-03\n",
      " -3.57542989e-03 -1.52407385e-02 -2.08202855e-02  2.35520811e-02\n",
      "  2.30679392e-02  2.08988631e-02  3.71433588e-02  3.78762110e-04\n",
      " -9.86690301e-03  2.05632491e-02  3.98466956e-03  8.73646733e-03\n",
      "  2.22920309e-02  8.42511725e-03  1.83724539e-02 -1.57679610e-02\n",
      " -2.40650428e-03 -2.14303491e-02 -2.24411180e-02  6.79322893e-03\n",
      "  1.16349314e-02  6.68639180e-03  7.58110880e-03  1.48606012e-03\n",
      "  1.92168664e-02  1.45737573e-02  3.91197788e-02 -7.86456309e-04\n",
      " -1.08486162e-02 -6.94351458e-03  3.30141424e-03 -4.49800673e-03\n",
      "  1.84303115e-03 -2.58773535e-03  2.95088307e-02  1.20217600e-02\n",
      " -1.68292136e-02  5.92112471e-04 -1.85478052e-02 -2.10956035e-02\n",
      "  2.28036189e-03 -4.07227233e-03  1.38558369e-02  9.71960851e-04\n",
      "  4.87226041e-03  2.76133785e-02  1.47576094e-02 -6.12226336e-03\n",
      "  4.20126244e-02  2.67582724e-03 -2.04624512e-03 -1.91353058e-02\n",
      " -1.14021626e-02  1.53107149e-02  8.85965188e-03  2.19133721e-04\n",
      " -6.47679683e-03  1.05380111e-02 -1.03692199e-02 -7.90005062e-03\n",
      "  3.18772512e-02 -4.52946023e-03 -3.88855958e-02 -4.64955619e-04\n",
      " -2.85610585e-03  6.59759052e-03  3.92805244e-03 -2.73746940e-03\n",
      " -4.32460131e-03 -1.08267642e-02 -1.49405963e-02  1.47886607e-02\n",
      " -2.98084210e-02  5.82530565e-03  1.66917629e-02  4.28882041e-02\n",
      " -4.22150916e-04  1.90284195e-02 -4.97424725e-04  1.72038900e-03\n",
      " -1.76789537e-02  2.38504842e-02  3.78606805e-04 -1.52606023e-02\n",
      " -3.16116942e-02  2.31779342e-03 -4.98929026e-03  8.64115510e-03\n",
      "  2.53049323e-02 -4.02119415e-03 -2.32627678e-02 -9.81820744e-03\n",
      "  1.89563260e-02  1.48452731e-02 -5.94804109e-03  1.81793724e-02\n",
      "  4.03087100e-04  1.73569124e-02  4.44553333e-03 -4.92086706e-04\n",
      " -1.15547966e-02 -1.98635609e-02  1.71951520e-02 -2.37384387e-02\n",
      " -3.64032208e-03  9.04436701e-03  3.88766196e-03 -5.76099468e-03\n",
      " -1.67977235e-02 -1.77599163e-02 -2.76731573e-03 -3.58862525e-02\n",
      " -2.82777697e-03  1.75337051e-02  1.53861617e-02  1.92468988e-02\n",
      "  2.43159988e-03  1.27564550e-02 -1.87380931e-03 -2.67332265e-02\n",
      " -3.57988073e-03  6.02845508e-03  1.51143439e-03  2.43072274e-02\n",
      " -1.21675505e-02 -9.71030163e-03 -4.49470942e-03 -4.10917061e-04\n",
      "  9.83942425e-03 -2.13398805e-03  6.79823821e-03  1.71648349e-02\n",
      "  3.33593988e-02 -2.41242277e-02  6.61762585e-03  1.65015466e-03\n",
      " -2.80936966e-02  1.39194386e-02 -3.71744090e-03 -2.24134832e-02\n",
      " -1.62305961e-02  3.26911219e-02 -7.51292688e-03 -2.34104626e-02\n",
      "  2.83108105e-02  6.87088859e-03 -7.15925980e-03 -1.76792000e-03\n",
      "  2.33551990e-02 -1.15314650e-02  2.60883203e-02  8.18061414e-03\n",
      " -2.89880898e-02 -3.68707699e-03 -2.37219016e-02 -8.50990867e-03\n",
      "  6.93067043e-03  2.39754179e-02  3.22146997e-02  1.02779890e-03\n",
      "  3.86114369e-02  5.21141567e-03 -1.93934480e-02 -7.55058574e-03\n",
      " -1.93236067e-02 -1.67158101e-02 -1.24000647e-02  2.85707012e-02]\n",
      "337021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 34 testing: 23\n",
      "correctsource    24\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[1.         0.83333333 0.83333333 0.5        0.5        0.5\n",
      " 0.5       ]\n",
      "Accuracy: 0.7058823529411765\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.77      0.83      0.80        24\n",
      "       missed       0.50      0.40      0.44        10\n",
      "\n",
      "    micro avg       0.71      0.71      0.71        34\n",
      "    macro avg       0.63      0.62      0.62        34\n",
      " weighted avg       0.69      0.71      0.70        34\n",
      "\n",
      "accuracy = 0.4782608695652174\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.64      0.56      0.60        16\n",
      "       missed       0.22      0.29      0.25         7\n",
      "\n",
      "    micro avg       0.48      0.48      0.48        23\n",
      "    macro avg       0.43      0.42      0.43        23\n",
      " weighted avg       0.51      0.48      0.49        23\n",
      "\n",
      "(444,)\n",
      "[-1.37978714e-02  1.65230924e-02 -2.09383831e-02  3.44612976e-02\n",
      " -1.95349424e-03  1.21869996e-02  1.40264574e-03 -1.47615632e-02\n",
      "  3.72772886e-03  9.90256534e-03  1.18139546e-02  2.68355089e-02\n",
      " -5.17934284e-03 -1.19789451e-02 -7.39339636e-04  1.56251561e-02\n",
      "  1.31763690e-02  3.82669344e-03  1.15719255e-02 -4.49919440e-04\n",
      " -1.50238147e-02  1.04548644e-02  2.68436827e-02 -1.68287704e-02\n",
      " -1.97379291e-02  1.24554731e-02  9.50968426e-03 -2.02224264e-02\n",
      "  1.35814957e-02 -3.65594358e-02 -5.60320003e-03 -4.70468446e-03\n",
      "  3.32559404e-03 -1.33650420e-02 -2.65304815e-02  7.96129889e-03\n",
      " -1.26251645e-03  7.66807932e-03 -1.33805409e-03 -4.30515782e-03\n",
      " -8.25125564e-03  2.56896403e-03 -9.22483660e-03 -1.15456983e-02\n",
      "  2.47693876e-03  8.17767366e-03 -1.00368509e-02 -2.13761800e-02\n",
      " -6.95906930e-03 -9.33700739e-03 -1.41235484e-03  1.58629030e-02\n",
      "  1.42102348e-02  2.56632243e-02  1.70487131e-02 -3.52688808e-03\n",
      "  2.21409313e-02 -8.47799390e-03  1.38035350e-02 -1.54610384e-03\n",
      "  3.19318255e-02 -9.97882780e-03  2.67342740e-02 -1.26092919e-03\n",
      "  4.03943662e-03  4.42895210e-03  1.18463175e-02  1.78335930e-02\n",
      " -4.02219816e-03  1.32096325e-03  2.07212013e-02 -1.62807551e-02\n",
      " -2.01735915e-02  1.69865289e-02 -1.96155017e-03 -9.03101821e-03\n",
      "  2.80602097e-03 -2.42196323e-02 -2.00139498e-02 -4.12872262e-02\n",
      " -3.01447235e-02 -6.34835176e-03  1.64643666e-02  1.76925226e-02\n",
      " -5.98060270e-03 -8.16516550e-03  9.63958087e-03  1.11581914e-02\n",
      "  1.78817379e-02  1.83794452e-02  4.47242411e-03  4.40129165e-03\n",
      "  9.50096715e-03 -2.05267521e-02 -5.85859083e-03  1.17897374e-02\n",
      "  1.31356325e-02  4.31750535e-02  1.45503447e-02  6.74206892e-04\n",
      " -2.32664756e-02  1.51084997e-03 -1.21777841e-02  3.23080337e-03\n",
      "  3.38420584e-04 -6.03281128e-03 -2.14942170e-02 -1.97569055e-02\n",
      " -1.20847176e-02 -1.99097943e-02 -3.20903676e-02  2.92625121e-03\n",
      "  6.57242789e-03  2.57031841e-03  4.43146998e-03  9.92656597e-04\n",
      " -1.24341538e-02  1.58762267e-02  2.84984019e-02  8.23569574e-04\n",
      " -1.25945327e-02 -2.56829829e-04  6.63177127e-03  3.53349152e-04\n",
      "  7.97625069e-03  1.08901788e-02  1.74957751e-02 -4.41545739e-03\n",
      "  9.24367581e-03  4.48102398e-03  2.27463042e-02 -2.79519485e-03\n",
      " -1.18562114e-02  5.48954594e-03  1.70418482e-02  7.77065059e-03\n",
      " -3.94934023e-03  1.88826463e-02 -1.12126941e-02 -3.71823022e-02\n",
      " -3.27220824e-02  1.77464425e-02  7.44284606e-03 -1.75397282e-02\n",
      " -7.35732998e-03 -2.02849085e-02 -2.88812137e-03  2.70459107e-02\n",
      " -1.62196266e-02  1.30138530e-02 -2.43557580e-02  2.76812299e-02\n",
      " -4.87023368e-04 -3.05635920e-03  3.14175782e-02  3.21031907e-02\n",
      " -3.90448736e-03 -3.34654680e-02  3.62515409e-02 -9.26433886e-03\n",
      " -2.71460751e-02 -1.55253567e-02  1.58554473e-02 -3.28076636e-03\n",
      "  2.49596903e-03  1.80266129e-02  1.42272674e-03 -2.09808816e-03\n",
      " -8.39684224e-03 -2.91943683e-02  2.85288952e-03  1.29877247e-02\n",
      "  2.23045436e-02  5.47817807e-03  6.12136324e-03 -9.86946831e-03\n",
      "  1.21865469e-02 -1.04184307e-02 -8.65011737e-03  6.46169292e-03\n",
      "  1.11691837e-03  1.31754971e-02  6.93620892e-03 -2.49402321e-03\n",
      " -2.98493995e-02  1.73380187e-03  7.88602595e-03  7.40463539e-03\n",
      " -3.05831037e-02 -3.34287235e-03 -2.28608368e-02 -1.01531497e-02\n",
      "  2.05206949e-02 -2.98193717e-03  1.26322903e-02  1.13645237e-02\n",
      "  1.51948559e-02  6.35304137e-03  1.72989409e-02  8.35922400e-03\n",
      " -3.28511369e-02  2.91938654e-04  6.79251992e-03 -5.92829232e-02\n",
      "  8.71983566e-03  2.80067575e-02  1.04786149e-02 -5.43977235e-03\n",
      " -5.85841661e-03 -9.83089339e-03 -1.26223508e-02  9.84033946e-03\n",
      "  1.71716875e-04  1.36089941e-02  5.81344723e-03 -1.10626301e-02\n",
      "  1.45941003e-02 -3.51978636e-02  3.00857150e-02 -5.40867925e-02\n",
      " -2.25133295e-02  8.26675091e-03 -1.66543601e-02  1.04194237e-02\n",
      " -1.06745989e-02  1.32106626e-02 -1.51531211e-02  9.64002605e-03\n",
      " -6.12735266e-03  3.20612591e-02  3.15062154e-02 -2.48394534e-02\n",
      " -2.96628301e-03 -3.27624973e-02  7.78879500e-03  8.32716495e-03\n",
      "  1.70928671e-02  1.13202716e-02  2.72276872e-02  8.33382197e-03\n",
      " -2.10797297e-04 -6.98003913e-04  1.50344339e-02  1.94094884e-03\n",
      "  1.53291006e-02  4.87501472e-02  1.52593728e-02  2.92093644e-02\n",
      "  2.00049036e-03  1.88504462e-02  3.34222983e-02 -1.62446311e-02\n",
      "  1.73957906e-02 -8.78274364e-03 -2.49898227e-02 -3.05992792e-02\n",
      "  2.94696121e-02  1.21356487e-02  8.16783743e-03  1.02843085e-02\n",
      " -1.14723382e-02 -1.64016298e-02 -3.48208313e-02 -1.88085965e-02\n",
      "  1.37670477e-02  1.26250860e-02 -1.30630347e-02  2.75725822e-02\n",
      "  4.02361246e-02  3.08772014e-02  1.54123046e-02  4.63440788e-03\n",
      " -3.16448905e-03  1.15499057e-02  1.46394689e-02 -2.46559265e-03\n",
      "  4.45386452e-03  1.05609484e-02 -1.60427137e-03 -8.75708378e-03\n",
      "  1.65418086e-02  2.01818702e-02  2.36214980e-02  3.00801140e-03\n",
      "  3.53751532e-02 -4.24685591e-03 -1.25703264e-02 -9.96105361e-03\n",
      " -8.69905458e-03 -2.04492953e-02  7.67297710e-03 -2.06869121e-04\n",
      "  8.87897412e-04 -2.27713899e-02  4.55428840e-03  2.23067817e-02\n",
      " -7.50207882e-05 -5.82392973e-04  1.36382246e-02  1.12149713e-02\n",
      "  3.87253664e-02  5.47942878e-03  1.56846379e-02  1.96270376e-02\n",
      "  1.51287852e-03 -1.01099008e-02 -2.36117345e-02  1.98407939e-02\n",
      "  6.88960830e-03 -1.64471782e-02 -3.09253563e-02 -7.61548467e-03\n",
      "  7.93454236e-03  2.22264185e-02  3.79760050e-03 -4.28014322e-03\n",
      "  2.80034364e-03 -2.17149689e-04 -2.16884720e-03  2.90651254e-03\n",
      "  3.83464297e-03  5.77890297e-03 -3.01988522e-02  1.40999270e-02\n",
      " -1.00896705e-02  1.54337359e-02  1.03930927e-02  2.03740613e-03\n",
      "  4.04512452e-04 -2.04416669e-02  1.61349197e-02  1.08930362e-02\n",
      " -1.05137237e-02 -1.76027192e-03  1.42251550e-02 -2.97607637e-03\n",
      " -1.05181891e-02 -1.84633768e-02 -2.69910332e-02  1.68246294e-02\n",
      "  1.32043036e-02  5.04510209e-03 -1.05323193e-02  7.18822569e-03\n",
      "  1.87704672e-02 -6.25851052e-03  3.45343383e-02 -6.94562438e-03\n",
      "  3.16529451e-03 -6.32536826e-03 -2.07482227e-02  1.17507832e-02\n",
      " -2.99098574e-02  7.10083928e-03  2.17798973e-02 -2.78971428e-02\n",
      "  2.62825147e-03  2.04551498e-04  9.93237616e-03 -4.07706770e-02\n",
      " -2.80441878e-02  1.34808329e-02  1.38790654e-02 -1.95106180e-03\n",
      " -2.39214702e-03 -7.17515635e-03  9.23589111e-03 -5.81176432e-02\n",
      "  1.35813957e-02 -2.95374920e-02  7.98072292e-03 -1.68906199e-03\n",
      "  1.88039005e-02  1.35364901e-02 -5.20499457e-03 -8.03416341e-03\n",
      " -3.14448453e-03  3.20878009e-02 -1.48767917e-03  1.67476345e-02\n",
      "  3.32212661e-03  1.62491583e-03  3.15143219e-02  2.19147501e-03\n",
      "  3.36979283e-03  3.12588792e-03 -7.34784846e-03  8.07925197e-04\n",
      "  2.26538394e-02 -1.35814013e-02  5.77372380e-03 -1.37118081e-02\n",
      "  3.10576155e-03  1.85425551e-02  6.45689545e-04 -1.83141937e-02\n",
      "  1.96600678e-02 -2.04585008e-02 -2.94707631e-02 -1.59144369e-02\n",
      " -3.73665519e-02  2.86555718e-02 -2.26439571e-02  3.57062027e-02\n",
      " -1.92097701e-02  6.82412036e-03 -2.43417004e-02  5.14667085e-03\n",
      "  1.52037087e-02 -4.86510573e-03 -6.39714667e-03 -2.97273974e-02\n",
      "  8.75073241e-03 -6.14457371e-04 -2.65039916e-03  5.76538361e-03\n",
      "  6.76976207e-03  3.28570139e-02  1.26290540e-03  1.59497115e-02\n",
      " -2.07553286e-02 -8.28155379e-03 -9.93170074e-03 -1.52800820e-02\n",
      "  2.61491066e-03 -1.19379766e-02  7.06997960e-03  1.73171992e-03\n",
      " -1.99455584e-02  8.97921962e-03  9.85958651e-03  8.56086081e-03\n",
      "  1.61542040e-02 -1.50578508e-02  2.33092874e-02  1.91601325e-02\n",
      "  2.03895441e-02  3.25861710e-03 -3.64217419e-03 -1.31761354e-02\n",
      " -2.81823925e-02 -2.63201382e-02  5.77781067e-03 -2.36248920e-02]\n",
      "396250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 39 testing: 27\n",
      "missed           21\n",
      "correctsource    18\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           14\n",
      "correctsource    13\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.5        0.66666667 0.66666667 0.6        0.4\n",
      " 0.8       ]\n",
      "Accuracy: 0.5641025641025641\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.53      0.56      0.54        18\n",
      "       missed       0.60      0.57      0.59        21\n",
      "\n",
      "    micro avg       0.56      0.56      0.56        39\n",
      "    macro avg       0.56      0.56      0.56        39\n",
      " weighted avg       0.57      0.56      0.56        39\n",
      "\n",
      "accuracy = 0.5925925925925926\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.57      0.62      0.59        13\n",
      "       missed       0.62      0.57      0.59        14\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        27\n",
      "    macro avg       0.59      0.59      0.59        27\n",
      " weighted avg       0.59      0.59      0.59        27\n",
      "\n",
      "(444,)\n",
      "[-0.00390258  0.00960449  0.0235724   0.00919719  0.00552854 -0.03047217\n",
      " -0.02794488 -0.03197131 -0.00141432  0.04837045  0.01945349 -0.02062373\n",
      "  0.00147416 -0.01984642 -0.00148953 -0.00900531 -0.0224832  -0.03272577\n",
      " -0.00668521 -0.0591974  -0.02737106 -0.01158283 -0.04219091  0.00793943\n",
      " -0.01276656  0.01746639 -0.00503158 -0.0202193  -0.03298398 -0.03452038\n",
      " -0.02573179  0.002989    0.00668506 -0.00730274 -0.0457967  -0.00619459\n",
      "  0.00927417  0.01759303  0.01317422  0.02207836 -0.00025323  0.00492465\n",
      "  0.00802843  0.00032822  0.07183798  0.0030873   0.00270303 -0.02146673\n",
      " -0.02624858 -0.03790902  0.0130788  -0.0393886  -0.03109427 -0.00147301\n",
      " -0.00700859 -0.02637509 -0.00677574 -0.04340552  0.00971463  0.04780414\n",
      "  0.00858295 -0.01540287 -0.01500821 -0.00599388 -0.02200247  0.01071087\n",
      " -0.0110593  -0.02803601 -0.03357311 -0.02895055 -0.00659545  0.04233092\n",
      " -0.01064364 -0.00213319 -0.00817262  0.01333538 -0.0351451   0.01302638\n",
      "  0.02270324  0.00349841 -0.00219218  0.00608604 -0.01106431  0.00370741\n",
      "  0.01936424  0.03242018 -0.0226095  -0.01397984  0.00120131  0.01365743\n",
      "  0.01592866  0.01535775  0.02530341 -0.00517292 -0.04235254  0.03888552\n",
      " -0.04154974 -0.0282295  -0.01226874  0.00335198 -0.02176113 -0.00275147\n",
      "  0.00603429  0.03406608 -0.03923538  0.02361248  0.00176178  0.02409961\n",
      "  0.0527111  -0.01041375 -0.00778212 -0.00425703  0.00845792  0.01579424\n",
      " -0.01622283  0.00333716  0.0205274  -0.02441415  0.01956533 -0.03724281\n",
      "  0.03544113 -0.00737583  0.00390272  0.01160018 -0.03202265  0.00952732\n",
      "  0.03078197  0.02799503 -0.01980971 -0.01458085 -0.00874163 -0.00191545\n",
      "  0.00215766  0.02830856 -0.01712431 -0.01527613 -0.00839428 -0.00228685\n",
      " -0.04784187 -0.00163639 -0.04712514 -0.01175717 -0.02974351 -0.02568642\n",
      " -0.0127091   0.02820345 -0.06503362  0.02372858 -0.02441157 -0.04495967\n",
      " -0.04540724 -0.0111771  -0.00284867  0.06682371  0.02763127 -0.01422481\n",
      " -0.01058888  0.05234574  0.02828972 -0.04308312  0.02817851 -0.00916428\n",
      " -0.00430225  0.00100937 -0.03176242  0.00179879  0.00757234  0.00144672\n",
      "  0.0086446  -0.02810643 -0.09938192 -0.00595787  0.02327984 -0.00285343\n",
      " -0.00758635 -0.00127463 -0.00149134 -0.02611002 -0.01429468 -0.01564448\n",
      " -0.03551179  0.0070115  -0.01145528 -0.01696361 -0.0035614   0.0108563\n",
      "  0.00328245 -0.03001712  0.01251805 -0.00797149  0.00705826 -0.04829587\n",
      " -0.01266648 -0.02452257  0.00399737 -0.00135444  0.01256147  0.03968593\n",
      " -0.00511772  0.00960288  0.0156418   0.0166051   0.00889016  0.00418759\n",
      "  0.00468646  0.00868861 -0.00506192 -0.00584145 -0.02643434  0.06208768\n",
      " -0.01902109  0.00821595  0.00189126  0.02981015  0.02474676  0.00767385\n",
      " -0.05325012 -0.00152199  0.03736024  0.01334002  0.03554652  0.02960721\n",
      " -0.027468    0.02498077 -0.0150111   0.02174114 -0.01018426 -0.0229038\n",
      "  0.01072399 -0.00067056 -0.01572813  0.02131564  0.02486259  0.01808342\n",
      " -0.04194333  0.002587   -0.03747774  0.00785145  0.00399232 -0.03641801\n",
      "  0.01484629 -0.06766208 -0.01528745 -0.04009869 -0.01651517 -0.011561\n",
      "  0.01666639 -0.04661218 -0.01835526 -0.04256931 -0.01310253  0.00011703\n",
      "  0.03838363 -0.00530587 -0.06808667  0.00551008  0.01606697 -0.04173523\n",
      "  0.00205691  0.03582978 -0.01049978 -0.02523952 -0.00232397 -0.0132053\n",
      "  0.00448423 -0.02834736 -0.02387792  0.01195772 -0.02334157  0.0092821\n",
      " -0.0307135   0.0196712   0.0132705   0.00131983 -0.02712348 -0.00764086\n",
      " -0.01203121  0.03755565  0.05159432 -0.04224546  0.01352878 -0.01154708\n",
      "  0.01273008 -0.00013665  0.03306541  0.00472346  0.00393875  0.0121724\n",
      " -0.00247733  0.04479296  0.01023846  0.00767653 -0.02581134  0.02537319\n",
      " -0.013653    0.04750688  0.00861873 -0.01446405 -0.01125952 -0.03705065\n",
      "  0.02280514 -0.02156981 -0.01560016 -0.00199521  0.00336241  0.01560618\n",
      " -0.00280003 -0.00044232 -0.02202189 -0.00668059 -0.01574391 -0.02520392\n",
      " -0.01775635  0.02149679  0.00044705  0.01574085  0.03581532 -0.00891237\n",
      "  0.00826817  0.03999815 -0.0345947   0.0025482  -0.03037485  0.00576703\n",
      " -0.02957783  0.02888    -0.08587805 -0.03040294  0.05390008  0.0432203\n",
      "  0.03526716  0.01035833 -0.00413667 -0.01959209 -0.04057015  0.00307747\n",
      " -0.0321946  -0.02394565 -0.01165552 -0.01796075 -0.01901452  0.02376631\n",
      " -0.04717972 -0.01329215  0.0163178   0.00439873 -0.00324937 -0.02744759\n",
      "  0.00639361 -0.04441427  0.01653347  0.0493796   0.00730516 -0.01698769\n",
      "  0.00636479 -0.04432024 -0.00224015 -0.05362105  0.01654046  0.0353682\n",
      "  0.05714263 -0.01050705  0.00315967  0.01786389  0.00646299 -0.00827\n",
      "  0.01054261 -0.02583573  0.019057   -0.06907253  0.00601311 -0.01427276\n",
      "  0.03728377  0.02079479 -0.04993846  0.00986434 -0.02593592  0.02933153\n",
      " -0.0188567  -0.01925641  0.03395649 -0.01657822 -0.00573086  0.0298145\n",
      "  0.00901474  0.03751507  0.00873135 -0.00402059 -0.0184997   0.02842386\n",
      "  0.01775151 -0.00809524 -0.03288227  0.04422751 -0.03983935  0.05016956\n",
      " -0.00449701 -0.02824625 -0.01853465  0.02482368  0.01952352  0.0161206\n",
      " -0.02768097 -0.01006144  0.01269422 -0.01880782  0.01436692 -0.03913827\n",
      " -0.00986901 -0.0027833  -0.0077204   0.00667409 -0.02392257  0.01196793\n",
      " -0.0289274   0.03974808  0.01428883  0.02578298  0.03895176 -0.01336852\n",
      "  0.02771161 -0.03184752 -0.0119767  -0.02338983 -0.02023477 -0.00462452\n",
      "  0.05544403 -0.01491151  0.03741325 -0.0006303   0.00597866  0.01485132\n",
      " -0.01383424 -0.03276502  0.02014963 -0.03986575  0.02125238 -0.00203705\n",
      " -0.0094845   0.01591633  0.00090595  0.04947865  0.00564512 -0.01747373]\n",
      "403131\n",
      "training: 42 testing: 28\n",
      "correctsource    23\n",
      "missed           19\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.57142857 0.71428571 0.33333333 0.66666667 0.5        0.6\n",
      " 0.8       ]\n",
      "Accuracy: 0.5952380952380952\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.61      0.74      0.67        23\n",
      "       missed       0.57      0.42      0.48        19\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        42\n",
      "    macro avg       0.59      0.58      0.58        42\n",
      " weighted avg       0.59      0.60      0.58        42\n",
      "\n",
      "accuracy = 0.5714285714285714\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.59      0.81      0.68        16\n",
      "       missed       0.50      0.25      0.33        12\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        28\n",
      "    macro avg       0.55      0.53      0.51        28\n",
      " weighted avg       0.55      0.57      0.53        28\n",
      "\n",
      "(444,)\n",
      "[ 1.15061355e-03  1.77082639e-02 -3.46385536e-04 -1.79807389e-02\n",
      " -2.45525300e-02 -4.27038887e-02  1.47868394e-03 -5.49860812e-02\n",
      " -2.38511972e-02  4.86816866e-02 -3.03239905e-02  2.14995380e-02\n",
      " -1.49934450e-02  2.35307545e-03  3.32680042e-02 -2.56511571e-02\n",
      "  1.19365102e-02  3.72306360e-02  1.60134290e-03  2.39050542e-02\n",
      " -2.13912493e-02  2.52745160e-02 -2.39878409e-03  1.20492260e-02\n",
      "  1.66995830e-03 -1.50404736e-02 -2.60496282e-03  3.89199434e-03\n",
      " -8.82042163e-03 -1.97739212e-02  6.73356754e-03  1.31784665e-02\n",
      "  1.65752213e-02  1.08007910e-02 -1.51279685e-02  1.82070715e-02\n",
      " -2.70218769e-02 -1.27322491e-02  1.88748982e-02  8.74792514e-03\n",
      "  2.73452885e-02 -7.63602172e-03 -3.47521972e-03 -1.24578198e-03\n",
      "  3.70398134e-02 -1.41574476e-02 -2.25257008e-02 -1.46899879e-02\n",
      " -2.62454905e-02  2.30540077e-03  1.80367029e-02 -1.16105121e-02\n",
      " -7.17923571e-03  3.00977768e-02 -6.60284663e-03 -2.86835718e-02\n",
      " -1.78613513e-03  1.36374241e-03 -1.59871112e-02  9.51619439e-03\n",
      "  1.09323634e-02 -9.65341104e-03  6.47105141e-03  7.68609539e-02\n",
      "  1.08593840e-02  1.96642258e-02  3.34117503e-02 -1.56548728e-03\n",
      " -2.11894940e-02  3.91835578e-02  6.22517381e-05  2.18613325e-02\n",
      " -4.37238472e-02  2.57474369e-03  2.81061795e-02 -1.27133392e-02\n",
      " -3.57685080e-02  2.80049133e-02  4.59182893e-02  1.46312372e-02\n",
      " -4.04070701e-02  3.54757437e-02  4.22867560e-02  5.00234706e-04\n",
      " -1.37221610e-02  2.61171347e-02  1.67171468e-02 -1.21752720e-02\n",
      "  2.53981746e-02  6.98024023e-02  8.44946918e-03 -5.77801096e-03\n",
      "  2.36957496e-03  1.91033844e-02 -1.59073340e-02 -1.27430500e-02\n",
      " -1.26180594e-03 -5.66708340e-03 -2.33702057e-02 -4.52045419e-02\n",
      "  1.08461268e-02 -3.97607351e-02 -7.02869894e-03 -3.15507248e-02\n",
      " -2.23239068e-02  2.98169400e-02 -8.39070196e-02 -6.23319387e-03\n",
      " -1.73249822e-02  4.57174778e-03  2.08531888e-06 -2.47479916e-03\n",
      " -4.31054633e-02 -3.57992459e-03 -2.45892281e-03  5.14501942e-03\n",
      "  5.22969568e-03  1.10645268e-02  3.35576950e-03 -2.37864962e-03\n",
      " -2.34235773e-03  3.57816126e-02 -2.90616743e-03  2.62032458e-04\n",
      " -1.41056814e-02  1.72509436e-02 -8.95075755e-03 -7.24076862e-03\n",
      "  2.83414192e-02  6.22180408e-02 -3.91686104e-02  2.27181911e-03\n",
      "  1.33978609e-02  2.49028850e-02  1.38513163e-02  1.39228792e-02\n",
      " -2.26518836e-02  2.99225245e-02  2.67510121e-02 -4.58290657e-02\n",
      " -6.86053314e-02  2.36550537e-03 -3.13848549e-02 -6.08229587e-03\n",
      " -3.61267676e-02  1.47503359e-02  3.44151200e-02  1.43340465e-02\n",
      "  9.93933481e-03  1.79798116e-02 -3.29799345e-02  4.39802708e-02\n",
      " -5.11038883e-03 -9.58884854e-03 -1.37880443e-02  1.56230809e-02\n",
      "  2.81034735e-02  1.41711369e-03  1.80024913e-02 -9.16909928e-04\n",
      " -1.87813478e-02 -2.96568877e-03  2.11735658e-02 -1.92059213e-03\n",
      " -1.46061901e-02  1.15125419e-02  1.12018223e-02  4.66836472e-02\n",
      " -5.41371226e-03  1.95071766e-02 -2.52272904e-02 -1.67997076e-02\n",
      "  1.32311049e-02 -1.25804762e-04 -2.86050660e-02 -3.40860671e-02\n",
      "  5.49401223e-03 -2.89843501e-02  4.37273858e-03 -3.41014630e-02\n",
      " -9.21799401e-03 -1.29067654e-02  1.95881390e-02 -4.39993172e-03\n",
      "  1.85549539e-02  1.10409896e-02 -6.77963966e-03  3.60555453e-02\n",
      " -5.94475017e-03 -1.21554666e-02  1.85280554e-03 -2.33566017e-02\n",
      "  4.54589616e-02  1.15243914e-02 -1.90154934e-02 -1.35676426e-03\n",
      "  2.36839625e-02  2.08320709e-03 -2.08068925e-02 -5.35823484e-03\n",
      "  2.48212592e-03 -4.87473173e-02 -1.99485022e-02 -4.35956803e-02\n",
      " -7.36054709e-03  2.80610842e-03 -2.20849282e-02 -2.33940538e-06\n",
      " -1.78957450e-03  3.73289060e-02  1.03528184e-02  5.43521913e-02\n",
      " -4.34619025e-02  1.72191035e-04  3.70466509e-03 -2.98831011e-03\n",
      "  1.28209390e-02  2.05234573e-02  2.13359481e-02 -3.68923458e-02\n",
      "  2.61653543e-03  5.45629123e-03  1.65046199e-02 -1.11207573e-02\n",
      " -1.72924786e-02 -2.75797439e-03  2.08879381e-02  3.94510973e-02\n",
      " -9.81786309e-03  8.47909319e-03 -3.12002825e-03 -3.01506600e-02\n",
      "  2.67931180e-03  2.45179642e-02 -2.28759963e-02 -1.46049121e-03\n",
      "  8.22804839e-03 -1.63806793e-04 -7.37225802e-03 -3.45644159e-02\n",
      "  5.38826369e-03  2.30148171e-02 -4.14680411e-02  3.95436693e-02\n",
      " -7.13870167e-03 -2.19928579e-02  1.23780667e-03 -1.63703987e-02\n",
      " -6.06752011e-03  1.20414677e-02  5.92881604e-03 -1.35631072e-02\n",
      "  8.28764208e-03  3.21242517e-03 -1.83941906e-02  7.61049344e-03\n",
      "  6.81838887e-03 -1.49727235e-02  2.03748775e-03 -1.42787514e-02\n",
      "  4.61424925e-02 -2.71278816e-02  1.07272221e-02 -1.36633796e-03\n",
      " -3.41552126e-03  4.37537582e-02 -2.90213786e-02 -4.77930941e-03\n",
      " -4.74128666e-03 -4.69454165e-02  3.49555737e-02 -2.29439379e-03\n",
      " -2.71782030e-02 -1.71872808e-02  3.73380895e-02 -1.54755577e-02\n",
      "  1.50676097e-02  1.26277585e-02 -4.18174243e-02  4.80308017e-02\n",
      "  3.01664116e-02  4.37896485e-03 -3.54490667e-02  1.45493484e-02\n",
      "  1.60169543e-02  6.60110028e-03 -2.00876705e-02  1.32466718e-02\n",
      "  8.02001409e-03  4.10716171e-02  4.33450191e-02  3.77222768e-02\n",
      "  2.33555367e-02  9.31710287e-03 -8.85004778e-03 -7.30306309e-03\n",
      " -8.55758135e-03 -1.62276401e-04  1.67844622e-03  1.40020719e-03\n",
      "  7.98455994e-03  2.88965081e-03 -5.64860432e-03  1.34928475e-02\n",
      " -5.91535492e-04  1.80991368e-02  8.56847663e-04  1.26380623e-02\n",
      "  2.59412441e-03 -1.65666325e-02 -2.39231877e-02 -2.96651208e-02\n",
      " -2.01938502e-02 -1.74536575e-02  2.25106300e-02 -1.36288938e-03\n",
      " -1.79448167e-02  1.25038356e-03 -1.30550909e-02  2.87975900e-03\n",
      "  1.01680404e-02 -1.60239451e-02  1.47757621e-02 -8.22364711e-03\n",
      "  4.06516389e-02 -6.66905787e-03 -3.15808135e-03 -2.32594514e-02\n",
      "  5.31429705e-02 -2.46720628e-02  6.36686982e-03 -3.54959905e-03\n",
      "  1.77846919e-02  9.53719993e-03 -1.54336112e-02  1.99722206e-02\n",
      "  1.30777602e-02 -3.50023925e-02  3.43407142e-03 -4.36180525e-02\n",
      "  2.74612779e-02 -5.78106253e-02  5.51031781e-03 -3.36883619e-06\n",
      "  1.80356210e-02 -1.90747036e-02 -2.42763379e-02 -1.23593257e-02\n",
      " -1.14478785e-02 -5.53004018e-03 -2.23482092e-02 -2.31128074e-02\n",
      "  3.30057249e-02  1.24786981e-02  1.38324157e-02 -2.61444893e-02\n",
      "  8.07560086e-03 -5.31173758e-02  9.24246054e-03  2.73149612e-02\n",
      " -2.89258522e-02 -1.65715476e-02  1.60782510e-02  1.95971580e-02\n",
      " -1.43427741e-02  3.27130075e-03 -1.52751361e-02  8.11916944e-03\n",
      "  2.34900737e-02  2.24372842e-02 -8.96686684e-03 -3.59318548e-02\n",
      "  7.97666700e-02 -8.33011427e-03 -1.96588519e-03 -1.23812106e-02\n",
      "  6.84163144e-03  1.46977956e-02 -3.60677736e-02  2.22712420e-02\n",
      " -3.22792418e-02 -2.71197518e-02  5.41141425e-02  1.09650211e-02\n",
      "  9.91997038e-03  1.37892164e-03  1.17732868e-02  9.04274284e-03\n",
      "  1.55660974e-02  2.56911051e-02 -3.24648825e-02  4.06844707e-02\n",
      "  3.94109669e-02  7.09403494e-03 -2.04050913e-02  1.03267099e-02\n",
      "  2.68605975e-02  1.88939046e-02 -2.47508074e-02 -2.00440275e-02\n",
      "  5.16732671e-03  1.63122291e-03  4.87545268e-02 -4.58769207e-02\n",
      "  1.44078573e-02 -2.55324856e-02  9.84487276e-03  2.18311332e-04\n",
      "  1.81416520e-02  1.80640671e-02 -3.22303261e-02  2.72224437e-02\n",
      "  2.96350111e-02 -1.73583297e-03 -3.44730529e-02  2.65399223e-02\n",
      "  7.92734485e-04  3.79717708e-02  4.94506401e-02 -2.64908717e-02\n",
      " -7.23733746e-03  1.67229832e-02  1.72702775e-02  2.57004255e-02\n",
      "  1.87215060e-03 -3.57227216e-03 -2.51918349e-03  3.49262807e-02\n",
      " -3.71446796e-02 -2.63809524e-02 -1.92617387e-02 -4.48594682e-03\n",
      "  1.67539103e-02  5.69142021e-03 -1.96929679e-03 -7.29104956e-03\n",
      "  1.04804251e-02 -7.07771084e-03  1.50422081e-02 -3.59266451e-02\n",
      " -5.64764517e-02 -1.58799226e-02 -3.14835701e-02  4.82160777e-03]\n",
      "408506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 30 testing: 21\n",
      "missed           16\n",
      "correctsource    14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           11\n",
      "correctsource    10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6  0.4  0.5  1.   0.75 0.25 0.75]\n",
      "Accuracy: 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.58      0.50      0.54        14\n",
      "       missed       0.61      0.69      0.65        16\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        30\n",
      "    macro avg       0.60      0.59      0.59        30\n",
      " weighted avg       0.60      0.60      0.60        30\n",
      "\n",
      "accuracy = 0.6190476190476191\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.60      0.60        10\n",
      "       missed       0.64      0.64      0.64        11\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        21\n",
      "    macro avg       0.62      0.62      0.62        21\n",
      " weighted avg       0.62      0.62      0.62        21\n",
      "\n",
      "(444,)\n",
      "[ 2.37875328e-03 -2.89841314e-02  1.82967571e-02  1.36174815e-02\n",
      " -3.88036941e-03 -2.45690480e-02 -1.85817299e-02  1.17436993e-02\n",
      " -4.35276621e-03  1.39048202e-03 -1.40475894e-02 -1.13423787e-02\n",
      " -1.85489833e-02 -1.86757269e-02  1.56080664e-02  5.21155025e-03\n",
      "  1.74266053e-02 -9.73010257e-03 -4.89541625e-04 -1.58379619e-02\n",
      " -4.70134612e-03  1.01896640e-02 -1.88279878e-02 -1.75263154e-02\n",
      "  4.89124380e-03  3.65393392e-02 -2.95031889e-02  2.56547641e-02\n",
      "  2.21640201e-02  2.04569003e-03  4.60809022e-02 -5.88922780e-03\n",
      " -7.74230592e-03  2.11810813e-02 -1.66809657e-02 -1.77014192e-02\n",
      "  4.77020502e-03  5.36597894e-03 -6.24841080e-03 -2.20534776e-02\n",
      " -1.93612523e-02  1.07033096e-02  7.75625360e-03  1.11308055e-02\n",
      "  6.08902976e-03 -2.07961918e-02 -1.69213415e-02  1.27274533e-03\n",
      "  1.96573133e-02  1.40905953e-02 -7.01133226e-03 -6.98182799e-03\n",
      " -5.26946851e-03  1.38724603e-02 -3.12364707e-02 -1.77689919e-02\n",
      " -6.82129828e-03 -6.31804181e-03 -1.00606741e-02  1.65630658e-02\n",
      " -1.30315015e-02 -5.81660999e-03 -2.52101895e-02  2.98214728e-02\n",
      " -3.85646438e-03  2.63353823e-02  1.28826413e-02  1.04732617e-02\n",
      "  7.95283868e-03 -1.57721086e-02 -2.41179584e-02 -1.68655580e-02\n",
      " -1.42351199e-02  4.58163363e-02  3.91168562e-03 -1.74451384e-02\n",
      "  8.64046026e-03  1.87815107e-02 -1.62347602e-02  8.02362479e-03\n",
      "  4.36354470e-03 -1.63249266e-02 -8.76958023e-03 -4.38680646e-03\n",
      " -1.34708357e-02  1.26871978e-02 -2.94783821e-03 -2.67417319e-02\n",
      "  6.32015579e-03  6.25550602e-03  2.86928062e-03  1.84059223e-02\n",
      " -1.45285567e-03  1.74614602e-02  2.31955554e-02 -2.18272016e-03\n",
      "  1.84396054e-02  1.77742388e-03 -2.44759978e-03  5.06183100e-03\n",
      " -1.81818954e-02 -2.46561477e-03  2.36673770e-02 -3.87796171e-03\n",
      "  1.06974235e-02 -9.95695394e-03  5.30888580e-03 -6.71781927e-03\n",
      "  6.63862966e-03 -1.31169381e-02  1.64155182e-02 -2.07439538e-02\n",
      "  2.58389392e-02 -1.24686976e-02  1.57010303e-02 -1.46644779e-02\n",
      "  2.71987355e-02 -8.87935811e-03 -1.65511460e-03  2.82358859e-03\n",
      " -2.08546348e-02 -1.51935300e-02  8.69352030e-05 -1.67846838e-02\n",
      "  1.39203011e-02 -6.81778802e-03 -2.66942561e-02 -2.46645062e-02\n",
      " -3.30244894e-02 -2.29226437e-02 -2.28577746e-02  1.17147747e-03\n",
      " -4.68183656e-03 -5.31277414e-03  3.04344258e-02  5.73746464e-04\n",
      " -2.60906345e-02 -1.47428708e-02 -2.21836560e-04  2.14004460e-02\n",
      "  9.74852741e-03 -4.02961016e-03 -9.16061208e-03 -3.33144988e-03\n",
      "  3.85286086e-04 -1.20223891e-02 -1.17608979e-02  1.81128141e-02\n",
      "  1.38791966e-03 -2.73272722e-02 -4.03412422e-03  5.45252408e-03\n",
      " -1.21256225e-02  5.96113534e-03  1.57392282e-02 -2.42195106e-02\n",
      " -1.87031647e-03  4.23200628e-02  6.52229722e-03  2.62008689e-02\n",
      " -8.42974793e-03 -1.01680946e-02  3.85785068e-02 -6.30051594e-03\n",
      " -7.21351891e-03  1.86029254e-02 -8.33199691e-03  1.89596138e-02\n",
      " -9.43772487e-03  6.00520050e-03  1.21424390e-02 -6.25693748e-03\n",
      "  3.61302649e-02 -5.64599941e-03  2.22309288e-02 -4.29120988e-02\n",
      "  6.22501054e-03 -3.49383104e-03 -1.62599942e-02 -1.30174570e-02\n",
      " -2.25390844e-02 -2.12458010e-02 -7.41477062e-03  5.81689017e-03\n",
      " -1.39820343e-02 -1.30398689e-02 -3.61578250e-03  1.24231589e-02\n",
      " -9.72510127e-03  2.41536322e-03 -1.76185608e-02 -3.90427027e-03\n",
      " -1.84365457e-02  2.28376488e-02 -2.26264543e-02  6.29800150e-03\n",
      " -2.06967586e-02 -2.74762875e-02 -2.29403525e-02 -4.11796814e-04\n",
      " -1.58819187e-02 -1.79064699e-02 -3.40004261e-03 -8.77465669e-03\n",
      " -1.57490090e-02 -4.30486998e-03 -1.64633256e-02 -1.24993319e-03\n",
      " -4.51063139e-02  8.91957900e-03  2.19173242e-03  1.21909997e-02\n",
      "  1.05352437e-02 -6.82081180e-03  4.16295161e-03 -4.34017774e-03\n",
      "  2.83608486e-02  1.42909050e-02  9.19876303e-03  1.43194145e-02\n",
      " -1.21709732e-02 -2.79527465e-02 -1.22473192e-02  1.40576707e-02\n",
      " -1.99532232e-03 -1.50077987e-02 -8.35675627e-03 -7.10004260e-04\n",
      " -6.26225430e-03 -2.01022867e-02 -3.07238515e-02 -8.09880658e-03\n",
      " -1.79373218e-02  2.62979063e-02 -1.30650838e-02  4.50628562e-03\n",
      "  1.40910859e-03  1.81933005e-03  8.37768305e-03 -1.73935783e-02\n",
      "  6.56301229e-03  6.21635244e-03  1.16830524e-02 -1.65350253e-02\n",
      " -1.09943379e-03  3.72767975e-03  9.03810526e-03  2.34849951e-02\n",
      " -1.56180647e-02 -1.39241592e-02  9.92798802e-03  3.96957125e-02\n",
      "  2.29512997e-02 -1.47713440e-02  1.23724744e-02  4.85688535e-03\n",
      "  3.67901493e-03 -4.44480235e-03  2.77585439e-04  1.39775930e-02\n",
      " -2.09490947e-02  2.91213186e-02 -7.12506072e-03  3.55128589e-03\n",
      "  1.72056823e-02 -8.29014238e-03  3.07729410e-03 -6.12910158e-03\n",
      "  9.32803686e-04 -2.82850224e-02 -5.60474980e-03 -1.10053921e-02\n",
      "  4.88805379e-04 -2.85564727e-04  2.62140182e-02  2.79824165e-02\n",
      " -1.41926795e-02  1.45477378e-02  3.57918396e-03 -1.60677628e-02\n",
      " -2.08430336e-02  3.96349017e-03  2.55641163e-03 -8.10581262e-03\n",
      " -8.02668971e-03  3.24817369e-02 -3.21348923e-02 -3.98720948e-03\n",
      " -5.75908945e-03  2.38633506e-02 -1.11630063e-02  2.20420772e-03\n",
      "  3.90766158e-03 -2.06378121e-02 -7.87195549e-04  2.30336405e-02\n",
      "  1.14796714e-02 -3.36008160e-02  3.54387522e-03 -3.72000647e-02\n",
      "  2.71336230e-03 -8.38057332e-03  6.48044726e-03 -8.29015152e-03\n",
      "  1.61509011e-02 -6.80506774e-03  1.74731325e-02 -1.04757101e-02\n",
      "  6.11391322e-03 -2.46291952e-02 -6.52181126e-03  1.54847411e-02\n",
      "  9.40385204e-03  7.80332440e-03 -1.71991381e-02 -6.98728555e-03\n",
      " -2.01298098e-02 -1.23676620e-02 -1.02786384e-02 -2.18843148e-02\n",
      "  5.35801966e-03  1.56340547e-02  9.31562247e-03 -3.95390681e-02\n",
      " -5.08062575e-03  1.27814695e-02 -1.59636819e-02  2.20328710e-02\n",
      " -1.64396651e-04 -9.57411233e-03  1.30803813e-02  1.44018875e-02\n",
      " -4.02772434e-03  2.10787178e-03  8.45890757e-03 -5.84974603e-03\n",
      " -1.72653544e-02  5.75385860e-03  1.51747178e-02 -3.75521571e-02\n",
      " -1.52684005e-02  1.88607440e-02 -4.99146063e-02  7.21067236e-03\n",
      " -1.37775266e-02  1.79854502e-02 -2.13218907e-02  2.25403618e-03\n",
      " -4.48022815e-03  1.70491198e-03  3.27000110e-02  2.41588269e-02\n",
      " -2.06475772e-02 -3.35121119e-02  5.82604804e-04 -7.33272992e-03\n",
      " -3.57552720e-02  3.18163805e-05 -1.73418606e-02  3.30621045e-02\n",
      " -4.71836755e-02  7.66577544e-03  1.99392141e-02  8.87175369e-03\n",
      "  8.85705347e-03  1.03989441e-02  3.97063092e-02  1.04297225e-02\n",
      " -1.21982350e-02  5.29669268e-03 -3.20733127e-03 -4.50544031e-03\n",
      " -3.07224112e-02 -5.42623327e-03 -2.86430618e-02 -2.53835193e-03\n",
      "  5.79741590e-03  1.28572907e-02 -2.20251587e-02 -1.09802301e-02\n",
      "  1.41795230e-02  2.59092831e-02 -1.21361056e-02 -2.30242638e-03\n",
      "  5.11729661e-03 -1.21407381e-02 -1.43252479e-03 -1.31611503e-02\n",
      " -2.78264616e-02  3.45326800e-03 -1.75998159e-02 -3.33303099e-03\n",
      " -1.10381878e-02  2.17601337e-02 -1.36236704e-02 -4.27170639e-03\n",
      "  1.75822247e-02  1.11382456e-02  1.31111337e-02  1.62802286e-02\n",
      " -2.21625328e-03 -3.25075238e-02 -1.49331506e-02  2.07872465e-03\n",
      "  1.62948761e-02 -4.98465354e-03  9.62985326e-03 -2.08500200e-03\n",
      "  3.23773326e-03  4.30571279e-04  1.41540690e-02 -3.75672226e-03\n",
      " -9.09882095e-04  1.13215350e-02 -2.27726771e-03  3.08939814e-02\n",
      "  2.41535509e-02  2.24921454e-02 -2.45098453e-02 -1.05303523e-02\n",
      "  1.88149476e-02 -8.29760039e-03 -5.49269912e-03 -4.84524858e-03\n",
      " -2.89249738e-02 -1.80212093e-02 -5.10539570e-04  1.66157898e-02\n",
      "  2.19689772e-02 -5.22830560e-03  7.86551243e-03 -1.56639964e-03\n",
      "  2.85875810e-02 -3.89823512e-04  8.49087900e-04 -1.23055382e-02\n",
      " -3.44462998e-02 -1.30702928e-02  1.48469151e-02  1.49485263e-02\n",
      " -7.69101485e-03  4.85955804e-02  1.10401143e-02  9.87049474e-03]\n",
      "413474\n",
      "training: 37 testing: 25\n",
      "correctsource    25\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    17\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.33333333 0.66666667 0.83333333 0.4        0.75\n",
      " 0.5       ]\n",
      "Accuracy: 0.5405405405405406\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.68      0.60      0.64        25\n",
      "       missed       0.33      0.42      0.37        12\n",
      "\n",
      "    micro avg       0.54      0.54      0.54        37\n",
      "    macro avg       0.51      0.51      0.50        37\n",
      " weighted avg       0.57      0.54      0.55        37\n",
      "\n",
      "accuracy = 0.44\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.64      0.41      0.50        17\n",
      "       missed       0.29      0.50      0.36         8\n",
      "\n",
      "    micro avg       0.44      0.44      0.44        25\n",
      "    macro avg       0.46      0.46      0.43        25\n",
      " weighted avg       0.52      0.44      0.46        25\n",
      "\n",
      "(444,)\n",
      "[ 0.02122653 -0.01956021 -0.0069232   0.01637333  0.05614188  0.00185922\n",
      "  0.05650575 -0.00254495  0.04321454 -0.00068717  0.00438632 -0.01263905\n",
      "  0.01115636  0.03312586  0.01031376 -0.0367325   0.03559369  0.05977639\n",
      " -0.00560662  0.01066488 -0.01359159 -0.04767188  0.00183729 -0.00138536\n",
      " -0.00829549  0.00402636  0.03170189 -0.01894935  0.02342123  0.04636919\n",
      "  0.01531045 -0.02285548  0.02152798 -0.04162523  0.06668577  0.00448559\n",
      " -0.00697482  0.01580031  0.04575271 -0.02754449  0.008611   -0.01143316\n",
      "  0.00086308 -0.01830455  0.01553338  0.01723297  0.00855771 -0.03409799\n",
      " -0.01222839 -0.01371164 -0.00627208  0.02985546 -0.02764117 -0.00048671\n",
      "  0.0162988  -0.03510618  0.01819346  0.05222114  0.03609419  0.02887194\n",
      "  0.01533405 -0.0139875  -0.01403076 -0.00880216  0.00550884 -0.02686609\n",
      "  0.01731498  0.00074134 -0.0072129  -0.01210749 -0.0304649   0.0268587\n",
      " -0.04405791  0.01334645  0.00881557 -0.07541426  0.00095114  0.01165878\n",
      " -0.02796633 -0.06521688  0.00504861 -0.01391568 -0.04712861  0.01043615\n",
      " -0.02361553 -0.01909167  0.03396059  0.01249462  0.02840234  0.00690916\n",
      " -0.0376816  -0.04254225  0.00146669 -0.01339594 -0.00038458  0.03472793\n",
      "  0.0131859   0.05536602  0.02154337 -0.01725423 -0.01443666  0.03536729\n",
      "  0.04006322  0.01356357 -0.00484837  0.00675881 -0.05586506 -0.04372798\n",
      " -0.01131794 -0.02140731  0.02626012 -0.00694399  0.01761496  0.01964201\n",
      "  0.06038704  0.01642492  0.03381784  0.03527469 -0.00427594 -0.04468038\n",
      " -0.01335601  0.03234063  0.02101061  0.04354405 -0.0392221  -0.02521872\n",
      "  0.02851589  0.00292005  0.01401     0.04139596 -0.010335   -0.04761132\n",
      " -0.02797136 -0.00387662  0.01249072 -0.01927527 -0.02939963 -0.03950467\n",
      " -0.01245477  0.00124147 -0.01400797 -0.01644154  0.00197688 -0.04323054\n",
      "  0.03924075 -0.00057501 -0.01512873  0.00811998 -0.0095368   0.03745931\n",
      "  0.0192403   0.03390039  0.03424821  0.00677237  0.02182231  0.00544857\n",
      "  0.01426017 -0.03365533  0.0155923   0.01822951  0.02928507 -0.02362291\n",
      "  0.02227775  0.0400521  -0.00595812 -0.01087436  0.01416377  0.0046095\n",
      " -0.00856954 -0.0243653  -0.01660103  0.00067785 -0.00153002 -0.00339512\n",
      "  0.02012813 -0.00831748 -0.0208129   0.0260689  -0.00674578  0.05082516\n",
      "  0.03204215  0.02856914 -0.04391777  0.00910544 -0.02130706 -0.02890774\n",
      "  0.02581434 -0.00372164 -0.02559012  0.01409547  0.03931377 -0.03140661\n",
      " -0.00481201 -0.00246491 -0.05238681  0.03797736 -0.00278069  0.01628366\n",
      " -0.00863689 -0.0031323   0.03020665  0.00727447 -0.00760436 -0.02531963\n",
      " -0.00908583 -0.03821906  0.01081186  0.02874136 -0.04286661  0.0049875\n",
      " -0.02966208 -0.02013932  0.01724098  0.00751346  0.00447947 -0.04859248\n",
      " -0.01126627 -0.003031   -0.02369997  0.01248292  0.0039511  -0.00941653\n",
      "  0.00952486  0.00522765  0.0194639   0.07010037 -0.0019114  -0.0122673\n",
      "  0.00056995 -0.02827016  0.04606894  0.01371468  0.00809868 -0.01166333\n",
      " -0.01977185 -0.01801542 -0.03782911 -0.02608403 -0.01839834  0.00210916\n",
      "  0.01740853 -0.06920062  0.00682518 -0.00204367  0.01581936  0.03202863\n",
      " -0.00939021 -0.03057759 -0.01466294 -0.03238613  0.01609181  0.0392436\n",
      "  0.02386528 -0.01645941  0.00353703  0.01771749  0.00611403  0.02794696\n",
      "  0.00233057  0.00795449 -0.01894642 -0.02440272 -0.02260814 -0.03851979\n",
      " -0.00614974 -0.00930425  0.02606953 -0.02825093 -0.00414596 -0.00640372\n",
      "  0.00153195 -0.01154847 -0.02107016  0.00339655 -0.00316292 -0.03166842\n",
      "  0.05592172 -0.02103025  0.00699841  0.00093332 -0.01474625  0.04099423\n",
      " -0.02420992 -0.0240072  -0.0607268   0.02960173  0.07784744  0.02638998\n",
      " -0.02403831 -0.01560249  0.00403904 -0.02637799 -0.03421263  0.0539843\n",
      " -0.00792714 -0.03552467  0.02630632  0.01486092  0.0060753  -0.01390677\n",
      " -0.00417632 -0.01427655 -0.00243512  0.00303329  0.01881987  0.0317178\n",
      " -0.0112008   0.02247824 -0.01220619  0.02453575  0.01448336  0.02474827\n",
      "  0.00985555 -0.01433683 -0.02781418 -0.02343082  0.00720534 -0.00643908\n",
      " -0.01118072 -0.00161386 -0.00097462  0.01584631 -0.00276037  0.03732874\n",
      "  0.01182238  0.01970654  0.00449645  0.00153949 -0.01358497  0.06984036\n",
      "  0.02956792 -0.03609047  0.01792163  0.02815383  0.00314684  0.00764665\n",
      "  0.00160533  0.03731742 -0.02483716 -0.02962872 -0.00497435 -0.01226261\n",
      "  0.00251506 -0.05004916 -0.00289014 -0.00740286 -0.01007655  0.00341736\n",
      " -0.01208269 -0.0395843   0.00895247  0.01749807 -0.00568337 -0.00126031\n",
      " -0.03976777 -0.01858746  0.02058605  0.01261127  0.00900112  0.01830205\n",
      "  0.00560515  0.00933193 -0.02611284 -0.02604352  0.01629237  0.01744952\n",
      " -0.01747553 -0.04755075 -0.01147536  0.00155615  0.0134008   0.00405918\n",
      " -0.00396883  0.02735493 -0.01823483 -0.03208552  0.00519988  0.03995738\n",
      " -0.00077539  0.05068923  0.00427618  0.00141078 -0.02104506 -0.00482104\n",
      " -0.0173869  -0.0181499  -0.01340577  0.00933204  0.03725357 -0.00645904\n",
      " -0.05244891 -0.03756549 -0.04488259 -0.00023427  0.00898666  0.0262922\n",
      "  0.02112557  0.00172868 -0.00657902  0.00824405 -0.03166269  0.00042369\n",
      "  0.01322487 -0.0158728  -0.00962434 -0.00740475 -0.00343644  0.02153472\n",
      "  0.00910534 -0.00103998  0.02482766 -0.00275104  0.02983838  0.01410377\n",
      " -0.02438271  0.01519747  0.01908059 -0.02177344 -0.00287758  0.01290046\n",
      " -0.00645084 -0.02881546  0.01108714 -0.03506269  0.02190291 -0.00353966\n",
      "  0.02518528 -0.02786216  0.00145625 -0.00492759 -0.01648468 -0.00561464\n",
      " -0.01997118  0.01018615 -0.03542685  0.04027176  0.02016315 -0.00491978\n",
      " -0.02666969 -0.00686794 -0.04696113 -0.03395756 -0.00115219  0.02365322]\n",
      "437101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 testing: 25\n",
      "correctsource    22\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.4        0.6        0.6        0.4        0.2\n",
      " 0.4       ]\n",
      "Accuracy: 0.4722222222222222\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.57      0.55      0.56        22\n",
      "       missed       0.33      0.36      0.34        14\n",
      "\n",
      "    micro avg       0.47      0.47      0.47        36\n",
      "    macro avg       0.45      0.45      0.45        36\n",
      " weighted avg       0.48      0.47      0.48        36\n",
      "\n",
      "accuracy = 0.4\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.53      0.50      0.52        16\n",
      "       missed       0.20      0.22      0.21         9\n",
      "\n",
      "    micro avg       0.40      0.40      0.40        25\n",
      "    macro avg       0.37      0.36      0.36        25\n",
      " weighted avg       0.41      0.40      0.41        25\n",
      "\n",
      "(444,)\n",
      "[-1.51365417e-02 -3.39673963e-03  1.17902205e-02  2.45041151e-02\n",
      " -8.49076091e-03  1.93118892e-02  1.67554800e-03 -2.51104295e-02\n",
      " -6.36443689e-03 -1.45630444e-02  1.59833666e-02  1.05340902e-02\n",
      "  1.56154800e-02  5.26821645e-03  1.29614672e-02  9.56139520e-03\n",
      " -3.63505816e-03  1.22070942e-02 -3.09697805e-05 -3.24105048e-02\n",
      " -1.08530708e-03 -2.33244893e-03 -9.75687063e-03  1.94442508e-02\n",
      "  2.43832020e-02  1.86230272e-02  3.37542703e-02 -2.48041636e-02\n",
      " -3.90717369e-02  1.32341167e-02 -1.22437217e-02  4.50305301e-02\n",
      "  1.06654847e-03  1.09191733e-02  3.20445694e-02 -3.48539595e-03\n",
      " -4.27506868e-02  4.25280315e-02  1.02300677e-02 -3.75158831e-02\n",
      "  1.33447137e-02 -2.46304676e-02  4.74391818e-02 -1.67905343e-03\n",
      "  9.79683309e-03 -2.01596888e-02  1.93555040e-03 -2.54002896e-02\n",
      " -2.11117956e-02  2.04530855e-02  2.07572817e-02  2.89100608e-03\n",
      "  1.66001384e-02  3.14285289e-02  5.48147531e-03  3.13732368e-02\n",
      "  3.34386515e-02 -3.52590686e-02  4.08831211e-03  1.21425094e-02\n",
      " -1.20951554e-02  2.98062562e-02 -1.76910743e-02  5.77854383e-02\n",
      " -2.55322484e-03 -4.51851498e-03 -7.79224081e-03  3.93652741e-02\n",
      "  1.53623890e-02  7.65379743e-03  1.19959091e-02 -6.25750502e-03\n",
      "  6.20896677e-02  1.08581348e-02  4.55606730e-03 -2.35992568e-02\n",
      " -5.69292636e-03 -3.28683843e-02 -8.31699396e-03  1.99387663e-02\n",
      "  1.22486455e-02  5.77229657e-02 -6.16782418e-03  3.33614466e-02\n",
      " -1.24476320e-03  1.53799972e-02  2.99906454e-02 -3.23492716e-02\n",
      "  1.59552442e-02 -1.46671496e-02 -3.53387994e-02 -4.95777359e-03\n",
      "  7.94680511e-06  1.54846762e-02 -1.48852933e-02 -8.13144454e-03\n",
      "  1.06788001e-02  1.03219653e-02  2.00842371e-03 -2.25671537e-02\n",
      "  1.31710459e-02 -9.74018722e-03  1.34420882e-02  1.93090678e-02\n",
      " -8.45930615e-03 -2.07908776e-02 -1.60550542e-02  2.91242199e-03\n",
      "  6.56185739e-02  4.15450019e-02 -2.34786214e-02  6.75800606e-03\n",
      " -1.37912880e-02  3.04831268e-02 -1.18328335e-03  5.08426579e-03\n",
      " -2.55466490e-03 -6.85068422e-03  2.06499904e-02  1.17039226e-02\n",
      "  1.61991320e-02  9.22210652e-03  1.17886790e-02  2.96977783e-02\n",
      "  3.06848124e-02 -4.57896229e-03  4.74454292e-02 -5.15777093e-03\n",
      "  4.62499941e-03  2.12411289e-02 -1.23157895e-02  2.11308186e-02\n",
      " -5.88942840e-03  7.17924264e-02  8.71107902e-03 -2.92258704e-02\n",
      "  1.52671031e-02 -2.39273437e-02  1.07299546e-02 -2.19666714e-02\n",
      "  1.53279556e-02  2.47823474e-02 -6.72863042e-03 -4.53583138e-03\n",
      " -9.81810743e-03 -2.28216719e-02 -2.03261900e-02  3.28933452e-02\n",
      " -5.78211811e-03  3.19526000e-03 -2.70291616e-02  2.23169030e-02\n",
      "  1.56901589e-02  2.30410099e-02  1.40448065e-02 -1.30820677e-02\n",
      "  3.05109279e-02  2.50589271e-02  2.08810941e-02 -1.10399654e-02\n",
      " -2.55312912e-02  1.25270075e-03  2.59227968e-03 -2.24059046e-03\n",
      "  2.71310357e-02 -2.72644136e-03  2.86955090e-02 -1.53880495e-02\n",
      " -1.75949721e-02  6.61755429e-03 -5.08518318e-03  3.13499465e-03\n",
      " -7.31444601e-03  4.01974593e-05  2.04230314e-03 -2.58227712e-02\n",
      " -6.79702177e-03 -1.15113351e-02 -1.43017957e-02  4.73477552e-03\n",
      "  2.63363607e-02 -8.55551846e-03  2.84170179e-02  2.57510408e-02\n",
      "  5.42953001e-05  1.26101885e-02  9.46491925e-03  1.56369524e-02\n",
      " -1.34230756e-03  2.18872062e-02 -5.00698009e-03  1.18371120e-02\n",
      " -2.41110748e-03 -2.13899680e-02  7.05113638e-03 -2.49572441e-03\n",
      "  1.54013925e-02 -3.72433489e-02  1.54158014e-02  2.00063375e-03\n",
      "  5.63796784e-03 -2.95208955e-02 -2.21378070e-02  2.26169124e-03\n",
      " -1.53342631e-02 -5.49303554e-02  8.46046190e-03  1.45042024e-02\n",
      " -1.93585290e-03  1.86583510e-02  1.56721706e-03 -1.99932172e-02\n",
      " -1.51736536e-02  2.63335201e-02  4.26614527e-02  3.34160128e-02\n",
      "  2.45725300e-02  1.55483323e-02 -7.98813591e-03 -1.31287626e-03\n",
      "  2.24152764e-02  5.65261394e-02 -3.48938453e-02 -4.39526258e-03\n",
      "  4.50113202e-03  2.72244033e-03 -2.26467682e-03 -2.50706359e-02\n",
      " -1.43734761e-02 -1.02085765e-02  1.34634454e-03 -6.97807847e-03\n",
      "  1.97585172e-02 -2.44960685e-02 -2.89066327e-02  1.72633316e-02\n",
      " -1.02186360e-02  9.83300543e-03  5.79507419e-02  2.51824460e-03\n",
      " -4.90231685e-02 -2.67193764e-02  4.39379356e-02 -1.02763905e-02\n",
      "  1.23915885e-03  1.10839710e-02 -3.96329146e-02  1.81410705e-02\n",
      "  3.98336376e-02  2.88804548e-02  3.17642985e-02 -1.68006423e-02\n",
      "  2.10912252e-02 -1.39330681e-02 -2.21795650e-02 -2.01254250e-02\n",
      "  2.80806977e-02  1.88123199e-02  7.86548115e-03  8.65082339e-02\n",
      " -1.89811512e-02 -1.83405076e-02 -3.76661485e-02  2.31253624e-02\n",
      "  2.79897004e-02  3.39879637e-02 -2.72368753e-02 -6.65231479e-02\n",
      " -1.63702975e-02 -6.94407654e-02 -1.85186175e-02 -1.39888946e-02\n",
      " -1.47923662e-02 -1.75641718e-02  2.04045506e-02 -1.43490222e-02\n",
      "  2.34844253e-02  4.18960204e-03  1.99414101e-02 -7.62609422e-03\n",
      " -4.46377498e-03  1.87990630e-03 -6.69398840e-02  1.89423780e-02\n",
      "  1.25290578e-02  3.15161505e-02  2.18723331e-02  1.84249276e-02\n",
      "  3.75014250e-02 -4.35974776e-03  3.73586584e-03  3.37433616e-02\n",
      "  1.81442738e-02  5.40782735e-02 -1.77479517e-02  6.30417735e-04\n",
      "  8.13358326e-03  1.99857623e-02  1.28547782e-02 -4.14687006e-03\n",
      " -6.99301064e-03  1.35120692e-02  3.35025153e-02  3.27008177e-02\n",
      "  1.64005491e-02  3.16375433e-02  2.60838972e-03 -3.94627102e-04\n",
      " -2.13181029e-03  5.07942609e-03 -1.75000784e-02 -1.78810478e-02\n",
      "  1.87793499e-02 -1.63627961e-02 -7.00885914e-03  3.45436193e-03\n",
      "  2.52204144e-03  2.43141560e-02 -1.52750047e-02 -2.16700009e-02\n",
      " -4.22671552e-03  2.34298364e-02  5.72894074e-03  1.39115053e-02\n",
      "  2.22549166e-02 -1.38111950e-03 -4.09340688e-02  4.57477508e-03\n",
      " -9.09577094e-03  1.47692052e-02  3.72814419e-02 -1.84308661e-02\n",
      " -4.00181606e-04  5.34327341e-02 -3.91187764e-03 -1.07410874e-02\n",
      " -2.00906963e-03 -2.52998777e-03  1.66930982e-02  1.79667200e-02\n",
      " -1.78439369e-02  1.66570812e-02 -6.14128301e-03 -1.47203796e-02\n",
      "  1.70390346e-02 -4.09124751e-03  7.40202975e-03  5.89479462e-03\n",
      "  3.30165038e-02 -9.97440757e-03 -1.50941988e-02 -1.76672953e-02\n",
      "  3.80909793e-02 -1.34837205e-03  2.59893206e-02  8.39663325e-04\n",
      " -9.83015853e-03 -2.17196676e-02  3.32804507e-02  2.70228976e-02\n",
      "  6.98675399e-03  1.65917046e-02  1.98960951e-02 -1.32977275e-02\n",
      "  7.41776376e-04 -4.68092738e-02 -8.35807298e-03 -2.37680345e-03\n",
      "  1.77340440e-02  6.41622453e-03  4.95477768e-02  2.40312773e-02\n",
      "  1.94748034e-02 -4.13621755e-02 -3.17658401e-03 -3.04346009e-02\n",
      " -1.80148965e-02  8.17996245e-03  9.56369260e-04  2.22153782e-02\n",
      "  1.89469432e-02 -2.35446288e-02  3.37991510e-03  2.76257554e-02\n",
      " -3.22837804e-03  6.04660731e-03  1.43350655e-02  1.62625359e-02\n",
      "  1.31933032e-02  7.53639176e-03 -4.18730494e-02 -3.52516103e-02\n",
      " -1.88362279e-02 -3.48606287e-03 -7.89166798e-03  1.32552342e-03\n",
      "  3.83339439e-03 -3.40061405e-02 -4.51520513e-02 -2.87402598e-02\n",
      " -2.88678873e-02 -3.41090639e-02  4.64738056e-02  1.76856557e-03\n",
      " -1.17131319e-02  1.90705847e-02  4.12901691e-03 -5.41240177e-03\n",
      "  2.46597655e-02  1.94361218e-02 -2.84950800e-02 -4.88501243e-02\n",
      " -2.30857746e-02  1.13320683e-02  1.12217262e-02 -3.04063209e-02\n",
      " -9.95114103e-03  1.26316924e-02 -1.19412495e-02  2.55946610e-02\n",
      "  1.19349191e-02 -2.60384118e-02  1.64861440e-02 -5.36788613e-02\n",
      "  1.70403389e-02 -3.03383971e-02  1.42947790e-02 -5.99798459e-02\n",
      "  1.77820689e-02  1.20955893e-02  1.13864347e-02  3.14416334e-03\n",
      "  1.95805716e-02 -1.73157027e-03  2.81657758e-02 -1.59912525e-02\n",
      " -2.82972260e-02 -1.84902195e-02 -5.08591582e-03 -1.78616966e-02\n",
      " -1.62093649e-02 -1.72069078e-02 -1.78382463e-02  1.93590685e-02]\n",
      "439776\n",
      "training: 44 testing: 30\n",
      "correctsource    33\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    23\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.85714286 0.71428571 0.42857143 0.57142857 0.66666667 0.8\n",
      " 1.        ]\n",
      "Accuracy: 0.7045454545454546\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.79      0.82      0.81        33\n",
      "       missed       0.40      0.36      0.38        11\n",
      "\n",
      "    micro avg       0.70      0.70      0.70        44\n",
      "    macro avg       0.60      0.59      0.59        44\n",
      " weighted avg       0.70      0.70      0.70        44\n",
      "\n",
      "accuracy = 0.6666666666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.84      0.70      0.76        23\n",
      "       missed       0.36      0.57      0.44         7\n",
      "\n",
      "    micro avg       0.67      0.67      0.67        30\n",
      "    macro avg       0.60      0.63      0.60        30\n",
      " weighted avg       0.73      0.67      0.69        30\n",
      "\n",
      "(444,)\n",
      "[-4.22864776e-02 -3.16726151e-03  2.27384518e-02 -3.71485551e-02\n",
      " -9.29384880e-03  1.13464130e-02 -8.14949840e-03  1.77666216e-02\n",
      " -2.00308974e-03 -3.53469958e-02  4.58381653e-02 -3.09092954e-02\n",
      " -2.00188634e-02  3.65138580e-03  2.67964588e-03  4.27454784e-02\n",
      "  1.60681684e-03  1.78559117e-02  1.21122359e-02  4.59387394e-03\n",
      "  1.70650182e-02 -4.22015574e-03  8.91357367e-03  1.18554931e-02\n",
      " -1.22110927e-02 -1.60700420e-02 -5.32758726e-03  1.35225586e-02\n",
      " -2.24917119e-02  3.43766322e-02  2.29315598e-02  4.26536520e-03\n",
      " -6.44114194e-03 -2.55193529e-02  2.79459000e-03 -8.99265505e-03\n",
      "  2.54813976e-02 -1.05594022e-02 -4.29584238e-03  3.68024493e-02\n",
      " -1.51369710e-02 -9.01794842e-04 -6.24713235e-02  3.61070920e-02\n",
      " -8.58018477e-03  8.52980061e-03  7.35928108e-03  2.01588148e-02\n",
      "  3.75479424e-02  8.15453710e-03  3.66366144e-02  2.24142707e-05\n",
      " -3.12216905e-02 -2.04912772e-03  1.77020701e-02  5.33507187e-02\n",
      " -2.47780590e-02  1.20267316e-02  1.31719256e-02 -1.05392515e-02\n",
      "  5.84581081e-02 -3.93734001e-03  1.34348895e-04  1.21480063e-02\n",
      "  1.58774650e-02  3.93660932e-03 -4.57760284e-03 -1.08643281e-02\n",
      "  5.54226784e-03 -3.44873363e-02  5.92705626e-03 -1.93306994e-02\n",
      "  7.83346399e-03 -3.98227178e-03 -1.41205767e-02  4.77740101e-03\n",
      "  1.12352775e-02  6.91803063e-02  1.90335488e-02  5.47315725e-03\n",
      "  4.32452055e-02 -1.65857803e-02  5.51137872e-02  5.22131348e-03\n",
      " -1.30738388e-02  2.25293814e-02  9.31010829e-03  2.63955190e-02\n",
      " -1.40492869e-02  1.01444774e-02  1.04876577e-02 -2.04607863e-02\n",
      "  2.64135982e-02 -1.09239836e-02  2.73658942e-02  2.66802542e-03\n",
      "  2.99879219e-02 -2.43605627e-02  1.30383110e-02  3.50966380e-05\n",
      "  6.09400305e-03  1.24692706e-03  2.43445342e-02 -7.62984187e-03\n",
      " -1.67342648e-02 -2.21996625e-02 -1.70948528e-03  2.98518773e-03\n",
      " -1.98718913e-03 -2.95338544e-03  3.15356818e-02  9.85560782e-03\n",
      "  5.41131241e-03  1.48646832e-02 -1.93493457e-02 -1.48244870e-02\n",
      "  2.32080148e-02 -1.59673422e-02  1.05174767e-02  3.11863002e-02\n",
      "  2.09621293e-02 -5.48806878e-03  1.57635147e-02 -3.27666193e-02\n",
      " -1.68732237e-02  1.24808986e-02 -3.26607764e-02 -1.83323487e-02\n",
      " -1.07510653e-02  9.43996997e-03  4.60679642e-02 -2.62924163e-03\n",
      "  1.44165622e-02 -4.25322938e-02 -2.56619755e-02  1.77541082e-02\n",
      "  3.10910095e-02 -1.70200085e-02 -6.96455328e-03 -8.95431268e-05\n",
      " -1.02059855e-02 -4.09109851e-02  2.85682063e-02  2.32947246e-02\n",
      " -9.24552434e-03  2.37852418e-02  2.56596795e-02 -3.59154053e-02\n",
      "  9.47280726e-03  6.12693595e-03 -3.62946789e-02 -4.48755819e-02\n",
      " -1.07863805e-02 -4.54776930e-03  2.40748351e-02  5.47988006e-05\n",
      " -1.38457908e-02  8.94866335e-05 -1.83463461e-02  1.27279226e-02\n",
      "  2.25829005e-02  8.91087383e-03 -1.52697669e-02 -7.33518321e-03\n",
      "  2.88026052e-02  2.16227616e-02  1.75472351e-02  1.59112234e-02\n",
      " -3.98725241e-03 -4.05414359e-02  3.91641096e-02  4.15887509e-02\n",
      " -1.02771272e-02 -4.70611276e-03  4.03622156e-02 -9.10895095e-03\n",
      " -2.99593035e-02  3.37741216e-02  1.29531775e-02 -8.17635559e-03\n",
      " -5.74111850e-03  2.62996525e-02 -1.23730012e-02  9.88372773e-03\n",
      "  2.56961078e-02 -1.89098453e-03 -1.60929403e-02 -1.73191661e-02\n",
      " -1.94409869e-03 -3.63309053e-03  2.58759594e-02 -3.68298997e-02\n",
      " -1.87507795e-02 -1.49272658e-02  2.56906019e-02 -7.98431867e-05\n",
      "  2.48776466e-02 -8.13970076e-03 -1.94063184e-02  3.66698615e-02\n",
      " -6.01865569e-03  3.48466259e-02  1.42306997e-02 -2.28835606e-02\n",
      "  5.63533777e-03 -4.37866912e-03 -3.11720514e-03 -3.25556746e-02\n",
      " -2.46011983e-03 -5.37135295e-03  1.23212050e-02 -4.60276596e-03\n",
      "  2.31933125e-02 -4.82353311e-02 -5.44171255e-02  4.42329498e-03\n",
      " -6.94992113e-03 -6.17013153e-03 -2.50506395e-02 -2.08870074e-02\n",
      " -2.16165465e-02  9.25597851e-03 -2.78756189e-02 -1.14561492e-02\n",
      " -4.88692996e-02 -1.50757327e-02  1.86744873e-02  2.08963806e-02\n",
      " -1.60536194e-02  1.92321943e-02 -9.02649906e-03  1.97963514e-02\n",
      "  5.84549623e-03 -2.69188174e-03  2.10893937e-02 -2.40541260e-02\n",
      " -3.96303682e-02  7.96522227e-03  9.89487052e-03  6.09273577e-04\n",
      "  2.03433182e-02  1.00973426e-02  3.99398016e-02 -4.85364809e-02\n",
      " -3.86685817e-02 -6.35060146e-03 -1.61753991e-02 -9.03512884e-03\n",
      " -2.40038211e-04 -1.64362797e-02 -3.86864773e-02  2.61627144e-02\n",
      "  3.10762055e-03  1.84540751e-02  1.89628085e-02 -5.06708819e-02\n",
      " -2.23481567e-02 -6.52257703e-02  1.95815130e-05 -4.19508643e-03\n",
      "  1.77001880e-02 -2.14544215e-02  2.90914738e-03 -5.39933148e-03\n",
      " -1.84698091e-02 -9.52316103e-03  2.55595139e-03 -2.08910575e-03\n",
      "  2.38751664e-02  1.61431083e-02 -1.37501516e-02  2.40558348e-02\n",
      " -2.46877963e-02 -1.14271453e-02  3.85792276e-03  2.93212704e-02\n",
      " -1.87128122e-02  1.80153275e-02 -1.22068873e-02 -2.58519031e-02\n",
      " -2.12781049e-02  2.98271550e-03  5.39686443e-04 -6.40604703e-02\n",
      " -1.00406425e-02  1.41290775e-02 -7.51652734e-03  2.62804348e-03\n",
      " -3.44669275e-04 -1.15518260e-02  2.40884098e-04 -3.12957177e-02\n",
      " -1.49260720e-03 -2.14748948e-02  3.15650584e-02 -1.65996456e-02\n",
      "  3.87530030e-03  2.13592663e-02 -1.15370235e-02  5.73391243e-03\n",
      "  4.24351612e-02  1.59770339e-02 -8.12972364e-03  3.03542981e-02\n",
      "  2.31788393e-02 -1.24202985e-02 -4.64593367e-03  2.18625929e-02\n",
      "  8.55882326e-04  2.46650543e-02  4.10627790e-02  1.95256750e-02\n",
      "  3.47808748e-02 -1.80486222e-02  3.99285351e-02 -1.88788225e-02\n",
      " -1.80999200e-02 -2.68212256e-02 -1.43346635e-02  1.31662701e-02\n",
      " -5.49484443e-03  6.67123014e-03 -3.27486283e-02 -1.47722333e-02\n",
      " -1.73915422e-02 -9.06567497e-03 -2.21010043e-02 -2.86121589e-02\n",
      "  3.99689502e-03  2.12738188e-02  1.60503992e-02  3.55453438e-03\n",
      "  1.84184443e-02 -2.34383356e-02  9.49333165e-03  3.23925431e-02\n",
      "  8.52011692e-03  3.88263339e-02  1.26597421e-02  5.11037390e-03\n",
      " -2.98701858e-02  2.07991252e-02 -2.69444981e-02  2.63609518e-02\n",
      " -9.23668592e-04 -6.47030800e-03 -2.61070898e-02  8.95060611e-03\n",
      " -1.12576368e-02  1.39391593e-02 -1.62450944e-02  3.14656843e-02\n",
      "  4.02182684e-02 -1.03182014e-02 -7.49040426e-03 -5.09478143e-03\n",
      "  2.39370762e-02  2.05619986e-02  5.56108026e-03 -4.36905932e-02\n",
      "  1.53111041e-02 -1.08480150e-02 -1.11887458e-03  2.14380992e-02\n",
      " -1.02348627e-02  1.22615796e-02  1.58943737e-02 -4.82126195e-02\n",
      " -2.10564782e-02 -1.26284458e-02 -1.02870013e-03 -3.34418506e-02\n",
      "  7.94482582e-03  2.24380019e-02  4.70315402e-02  7.26057067e-03\n",
      " -2.45244160e-02  2.36822896e-03  3.92175398e-02 -5.77673675e-03\n",
      "  6.22576176e-03  2.92642980e-02 -2.07284651e-02  3.10096752e-03\n",
      "  2.45944420e-02 -2.24870915e-02 -8.43652076e-03  5.64182349e-02\n",
      "  8.52767630e-04  5.19406157e-02 -3.15156490e-03  1.47416755e-02\n",
      "  1.08093890e-03  3.14388869e-03  4.44995364e-02 -1.55348038e-02\n",
      " -1.69066569e-02  4.84187955e-02  2.05910952e-02  1.38332802e-02\n",
      "  1.88010082e-02 -2.98635565e-02 -3.99905770e-02 -1.79929090e-02\n",
      " -3.15507818e-02  1.08798370e-02  1.19460874e-02  5.30671168e-02\n",
      "  9.45290299e-03 -1.14302530e-02  1.61026405e-02  2.37383568e-02\n",
      " -2.79025392e-02  5.88439993e-03  3.20371468e-03  3.89071426e-02\n",
      "  2.50958392e-02 -3.14268130e-02 -1.12995949e-02  2.02380697e-03\n",
      " -2.40174924e-04  6.83126708e-03  8.10930928e-03  5.88273935e-02\n",
      " -2.94593895e-02  5.35397686e-04  1.03102477e-02  1.87309866e-02\n",
      "  2.51071912e-02 -1.45676792e-02  2.08652227e-02 -3.84211507e-02\n",
      "  1.45540371e-02 -2.47935459e-02 -3.28697611e-02 -1.77658904e-02\n",
      "  4.98587086e-02  7.67198364e-03  5.82810764e-03  1.59107174e-02\n",
      "  2.51754538e-02  1.63115519e-02  5.86280108e-03 -4.89669293e-03]\n",
      "458807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 testing: 24\n",
      "correctsource    21\n",
      "missed           15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.6        0.4        0.2        0.2        0.6\n",
      " 0.4       ]\n",
      "Accuracy: 0.3888888888888889\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.48      0.52      0.50        21\n",
      "       missed       0.23      0.20      0.21        15\n",
      "\n",
      "    micro avg       0.39      0.39      0.39        36\n",
      "    macro avg       0.35      0.36      0.36        36\n",
      " weighted avg       0.38      0.39      0.38        36\n",
      "\n",
      "accuracy = 0.7083333333333334\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.77      0.71      0.74        14\n",
      "       missed       0.64      0.70      0.67        10\n",
      "\n",
      "    micro avg       0.71      0.71      0.71        24\n",
      "    macro avg       0.70      0.71      0.70        24\n",
      " weighted avg       0.71      0.71      0.71        24\n",
      "\n",
      "(444,)\n",
      "[-6.81646094e-03 -3.50652034e-02  2.03874194e-03 -4.85081607e-02\n",
      "  6.24298028e-03  9.82229466e-03  1.60198532e-02 -1.16201840e-03\n",
      "  2.45119322e-02  5.16442529e-02  7.67290919e-03  8.47655161e-03\n",
      " -3.96611614e-03  3.94972756e-03 -4.67962263e-02 -1.15204795e-02\n",
      " -4.23071248e-03  1.39174931e-02  9.38488717e-03  1.30020299e-02\n",
      " -2.56928365e-02  4.13167733e-03 -1.39253722e-02  2.04800017e-03\n",
      " -3.54544416e-02  1.79033804e-03  5.56371379e-02 -1.89525340e-02\n",
      "  4.05547818e-03 -2.53787284e-02  1.13676663e-02  2.14596414e-02\n",
      "  1.73814752e-02 -5.97940118e-02 -8.29579286e-03 -1.09437101e-02\n",
      "  4.12840616e-02 -1.69829879e-02 -1.78610268e-03  1.31804003e-02\n",
      " -9.04000646e-04  3.42190666e-02  1.93993961e-02  1.81735355e-02\n",
      "  1.37696167e-02 -5.90560319e-03  2.09918552e-02 -4.20969213e-03\n",
      " -2.52555323e-03  9.44082924e-03  2.76698507e-02 -1.90790482e-02\n",
      " -3.62854507e-02 -3.39413367e-02  3.59550370e-02  1.13868654e-02\n",
      " -2.55508268e-02  6.51342410e-05  1.13854811e-02 -2.04904225e-02\n",
      "  1.77933683e-02  2.33726098e-02 -2.06955618e-02  5.05900038e-03\n",
      "  1.41512217e-02 -3.89947600e-02  3.35756216e-02  2.99171688e-02\n",
      "  8.30148724e-03  2.46488528e-02 -4.06597144e-03  3.57609528e-02\n",
      "  1.26974219e-02  1.16540880e-02  8.35244009e-03 -4.41084453e-03\n",
      " -1.94219580e-03  1.04032591e-03 -1.40298732e-02 -1.70699842e-02\n",
      " -3.79056500e-02  3.51990954e-02  3.62636850e-02  2.55172152e-02\n",
      "  6.78549347e-03  7.88388339e-03 -1.56150002e-02  1.85880188e-02\n",
      "  1.88649715e-02 -7.59598735e-03 -1.36396725e-02 -1.93271637e-02\n",
      " -2.29462667e-02 -3.25491867e-02  1.43496637e-02 -2.34821006e-02\n",
      "  1.18676301e-02  5.27432392e-02 -3.68047821e-03  3.99832308e-02\n",
      " -5.66453724e-02  2.19040171e-02 -7.31097884e-03 -1.57943920e-02\n",
      "  9.80212235e-04  2.25279771e-03  5.40833812e-03  1.83092450e-02\n",
      "  4.20893684e-02 -6.86926980e-03 -3.58993453e-02  3.57239724e-02\n",
      " -1.50338286e-02  2.40971378e-02  1.95662823e-02 -5.45447532e-03\n",
      " -4.46806974e-02  1.48721622e-02  4.17545837e-03 -1.58211756e-02\n",
      " -3.92837181e-03 -3.91315641e-02  1.78371224e-03  6.39613912e-03\n",
      " -9.30937869e-03 -2.04057501e-02  1.50300025e-02 -6.59465371e-03\n",
      "  2.27283537e-02 -1.20474432e-02 -1.34855879e-02  7.17297866e-03\n",
      "  8.51856042e-03  7.55654443e-04 -2.12053636e-02 -1.27280239e-02\n",
      "  1.39248094e-02 -4.56327787e-03 -1.23398343e-02 -4.00336932e-02\n",
      "  1.29140245e-02 -7.31361050e-05 -7.12342716e-03  7.17591193e-04\n",
      "  4.59427557e-02  1.68865225e-02 -2.26561054e-03  2.90617661e-02\n",
      "  5.76404875e-02  4.86626962e-03 -6.29147040e-02  2.05012819e-02\n",
      "  3.32130005e-02 -1.96512241e-02  9.84050150e-03 -2.53987823e-03\n",
      "  2.83266564e-02  3.87197749e-02  2.68271371e-02 -4.19304853e-02\n",
      " -2.31029612e-02 -8.00032557e-03  3.70618417e-04  4.94940904e-02\n",
      "  4.04255912e-02 -1.00126979e-02  2.56300028e-02  4.91310676e-03\n",
      " -1.93722743e-02 -3.08547103e-02 -6.38185206e-03 -3.50572846e-03\n",
      " -1.45286501e-02 -2.18322167e-02 -9.26823065e-03 -1.98401472e-02\n",
      "  3.11227852e-02  4.06150373e-03 -5.66544860e-02 -8.63204397e-03\n",
      "  2.74865497e-02 -2.45781985e-02 -2.86881769e-02 -8.69341706e-03\n",
      " -1.74332486e-02 -2.61078484e-03  1.93948858e-04 -3.35833593e-02\n",
      "  2.36048135e-02  2.67023504e-02  2.45007842e-02 -7.54604527e-03\n",
      "  4.66406199e-02  3.95052407e-02  1.28053392e-02 -3.89744039e-03\n",
      " -4.34245241e-02 -2.28502362e-03  1.30663436e-02 -9.85820794e-03\n",
      "  3.79492515e-03 -9.27833993e-03  4.03021875e-02  2.01490526e-02\n",
      "  5.41304516e-02 -2.75467531e-02 -4.97797093e-02 -6.07463459e-03\n",
      "  9.73848650e-03  7.88264467e-03 -2.58863699e-02 -1.14276661e-02\n",
      "  0.00000000e+00  4.39796694e-04  1.43039278e-02  6.74340656e-03\n",
      " -4.61775340e-02 -6.16595380e-03  1.45534437e-04 -3.78924474e-02\n",
      "  4.17286371e-03 -1.71711453e-02 -5.35149431e-03  4.76772251e-03\n",
      "  2.25567377e-02  1.27270735e-02 -4.55452002e-02  1.29849865e-02\n",
      "  2.73687256e-02 -4.95219055e-03 -3.85242494e-03 -3.13155895e-02\n",
      "  1.78535020e-02  2.33618042e-02 -3.00661241e-02  2.21869669e-02\n",
      "  2.40880198e-02 -3.82664050e-02  3.35796892e-02  1.34342427e-05\n",
      " -5.24265302e-03  2.79521231e-02 -3.01598123e-02  3.12154820e-02\n",
      "  3.49684065e-02 -1.43466616e-02 -4.05119990e-02  2.21843321e-04\n",
      " -3.08865984e-02  1.37380691e-02  2.26404023e-02 -1.01358483e-02\n",
      " -2.06798339e-02  2.45969249e-02  1.34436933e-02  2.88144229e-02\n",
      "  3.14149077e-02 -1.13578957e-02  1.39621812e-02  2.85016148e-02\n",
      "  1.47996428e-02 -4.92398104e-02  3.97807967e-02  2.70778444e-02\n",
      "  7.92473077e-03  7.73028003e-03 -1.88626988e-02 -2.47371039e-02\n",
      " -7.94424723e-03 -2.93254163e-02 -5.12466261e-03  1.80061213e-02\n",
      " -7.76238484e-03  1.18364182e-02  2.56108956e-02 -2.42246827e-02\n",
      " -2.39006536e-02  5.54124126e-03 -1.98709126e-03  7.99159205e-03\n",
      " -1.53725739e-02  1.61339875e-03 -4.22250309e-02  1.24866776e-02\n",
      "  1.14563701e-02 -5.01279587e-03  9.94436778e-03 -2.49621249e-02\n",
      "  1.79637967e-02 -1.09032878e-02  4.85727177e-02  9.60497520e-04\n",
      " -1.22111271e-02  2.83934829e-02 -1.14100443e-02 -1.44269487e-02\n",
      "  1.76779240e-02  5.72148312e-03 -1.06664221e-02 -1.54423980e-02\n",
      " -9.03324945e-03  2.23773355e-02 -9.53390340e-03  2.53046962e-02\n",
      "  8.92284730e-03  3.13099281e-02  3.47708299e-02 -7.33730444e-03\n",
      " -4.31433038e-03  3.35204655e-02 -2.48566370e-02 -4.97567527e-03\n",
      "  1.91811494e-02 -1.58010831e-02  1.08441555e-02 -1.70947626e-02\n",
      " -1.35985243e-02  3.73695593e-03  3.13943483e-02  8.50746878e-03\n",
      "  2.56653082e-02  1.62310955e-02 -1.80799038e-02  6.49528932e-03\n",
      "  7.44825434e-03  9.07328896e-03  6.40587230e-03  8.31992247e-03\n",
      " -2.55567613e-03  4.59092565e-02  4.96095554e-04  1.62225891e-02\n",
      "  1.71493890e-02 -2.24429603e-02 -1.02391655e-02 -4.77788686e-03\n",
      "  2.06887346e-04  5.21118889e-02  2.10863453e-02 -3.23390596e-02\n",
      " -2.30278073e-02 -3.97264620e-03 -2.99768409e-02  2.61269819e-02\n",
      " -4.90514176e-03 -2.22165971e-02 -2.55784795e-02  4.77635132e-02\n",
      "  1.57648315e-02 -7.40702339e-03  1.30067550e-02 -2.66474043e-02\n",
      " -1.96868493e-02 -1.78341930e-02  9.83982895e-03  8.76250266e-03\n",
      "  1.04808958e-02 -1.58486468e-02 -1.24641460e-02 -3.30397728e-02\n",
      " -1.37350167e-02 -8.43422676e-03 -3.41152125e-02 -2.18760061e-02\n",
      "  1.65430374e-02 -2.07652310e-02 -3.32099714e-02  4.88648723e-03\n",
      "  1.67894531e-02 -3.05341092e-03 -2.71188097e-03 -5.09148473e-03\n",
      " -3.52049234e-03 -2.07268906e-03  5.36288337e-03  5.80726034e-03\n",
      " -2.36434787e-02  2.38237444e-02 -3.62205210e-02  1.46961327e-02\n",
      " -1.71526579e-02 -1.09533123e-02  1.66969471e-02 -1.66470187e-02\n",
      " -3.01744338e-02  6.82402685e-03  2.50383772e-03  1.76161222e-02\n",
      "  1.34244253e-02 -3.03795112e-03  6.61295480e-03 -1.75687242e-02\n",
      " -2.29081051e-02  1.85512162e-02 -1.94462799e-02 -4.36695234e-03\n",
      "  2.24199023e-02  2.93128630e-02  4.54265741e-02  1.77702029e-02\n",
      " -1.07926714e-02 -9.30553772e-03  7.47861810e-02 -1.63779625e-02\n",
      " -7.80259028e-03 -1.69505450e-02 -3.23670541e-02 -1.53130621e-02\n",
      " -1.52007190e-02 -2.06510264e-02 -2.21328942e-02  3.24995349e-03\n",
      "  4.54873519e-02  1.38108484e-02  2.41249146e-02 -4.04434360e-02\n",
      " -4.76871262e-03 -3.41648937e-02 -3.35082633e-02  1.56359588e-02\n",
      "  2.31283240e-02 -1.79338133e-02  3.85554398e-02 -3.61442363e-02\n",
      "  3.26230304e-03  3.59035891e-03  3.72735996e-02 -3.75142150e-02\n",
      "  4.16381155e-02  4.11062190e-04 -1.32376564e-02  1.94028009e-02\n",
      "  1.62972380e-03 -1.00629687e-02  1.97607841e-02  3.94453104e-02\n",
      " -1.41935460e-02  2.45907429e-02  6.72820819e-02 -5.44535740e-02\n",
      "  1.66784442e-02  8.17137529e-03 -3.90748193e-03 -5.30054645e-02]\n",
      "459801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 31 testing: 21\n",
      "correctsource    20\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    13\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6  0.8  0.6  1.   0.75 0.75 1.  ]\n",
      "Accuracy: 0.7741935483870968\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.76      0.95      0.84        20\n",
      "       missed       0.83      0.45      0.59        11\n",
      "\n",
      "    micro avg       0.77      0.77      0.77        31\n",
      "    macro avg       0.80      0.70      0.72        31\n",
      " weighted avg       0.79      0.77      0.75        31\n",
      "\n",
      "accuracy = 0.6190476190476191\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.65      0.85      0.73        13\n",
      "       missed       0.50      0.25      0.33         8\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        21\n",
      "    macro avg       0.57      0.55      0.53        21\n",
      " weighted avg       0.59      0.62      0.58        21\n",
      "\n",
      "(444,)\n",
      "[-3.73293768e-03  1.31464154e-02  2.10190931e-02  5.56420835e-03\n",
      " -2.64130526e-02 -1.54614724e-03 -1.60040920e-03 -4.34832375e-03\n",
      "  1.43830172e-02  1.35563027e-02  9.24646837e-03  2.78348414e-02\n",
      "  1.79963176e-02 -7.05934410e-03 -6.18727404e-03 -1.69630377e-02\n",
      " -2.02821168e-03  1.78140920e-02 -4.12003908e-03 -3.24012257e-02\n",
      " -4.47096536e-03 -2.68986594e-02  1.75380437e-02 -3.95932761e-02\n",
      " -4.06824888e-04  2.15439197e-03  9.21643473e-03 -1.08700923e-02\n",
      " -1.06301506e-03 -1.65178678e-02  9.07943535e-03 -1.07242003e-02\n",
      "  4.12907010e-02  3.58414855e-03  2.48689488e-02 -7.56348785e-03\n",
      "  1.08190767e-02  2.32136328e-02  5.43998557e-03 -1.87244758e-02\n",
      " -6.75559500e-05  7.43105146e-03 -1.16673819e-02  1.00837731e-02\n",
      " -1.03224298e-02 -1.50189413e-02  6.38451368e-03 -6.57484062e-03\n",
      " -4.18675823e-04  3.67810375e-03 -9.50874352e-03  4.26503402e-03\n",
      " -2.45907589e-02  1.27601139e-02  2.71739141e-02  8.27735112e-03\n",
      " -2.03419710e-02 -1.11712590e-02 -1.85186481e-03  2.42541896e-02\n",
      "  1.23977232e-02  3.74863933e-02 -4.20425313e-03  1.28316776e-02\n",
      "  3.31492639e-02  7.55043651e-03  3.11346329e-03  2.04213548e-02\n",
      " -2.90818968e-03 -2.09402647e-02 -9.64630820e-03 -2.33108540e-02\n",
      "  9.52811084e-03  2.26281835e-02  2.15885694e-02 -1.21994140e-02\n",
      " -1.20715495e-02  3.69950748e-03 -4.44440715e-02  3.48479454e-04\n",
      "  1.83609426e-02  1.11619322e-02  1.74515552e-02  4.43740639e-03\n",
      " -9.24104592e-04 -2.85241600e-02 -1.25531595e-02 -4.81064835e-03\n",
      "  1.68147601e-02 -2.14267393e-04 -2.91219728e-02  1.97804558e-02\n",
      " -1.60679692e-02 -5.84845362e-03 -1.04990128e-03  3.58218315e-03\n",
      "  1.04390356e-02 -2.42270630e-03 -4.56737929e-03 -1.93367980e-03\n",
      "  2.47951727e-02 -1.50508855e-02  2.21600921e-02  3.45979564e-02\n",
      "  1.31184008e-02 -3.04266805e-02 -1.73109103e-03 -8.29292612e-03\n",
      "  3.29768042e-02  1.83405322e-02  1.80980239e-02 -1.54641731e-02\n",
      "  5.93578668e-03  2.16594518e-02  2.55346825e-02 -8.42664157e-03\n",
      "  1.08305030e-03  1.44994469e-02  4.70585921e-03  4.72845214e-03\n",
      " -1.43815613e-02 -4.93710800e-03 -1.26858513e-02 -5.82568285e-03\n",
      " -2.65417786e-02 -6.03744770e-03 -3.65141693e-03 -1.35463648e-02\n",
      " -1.81507099e-02  8.40815910e-03  1.00570088e-02  3.05233575e-03\n",
      "  2.00324064e-02  3.09630712e-03 -1.16367131e-02 -5.86694129e-03\n",
      "  2.09908405e-02 -3.14187023e-03  3.94042142e-04  1.07460689e-02\n",
      " -1.80231754e-03  1.32743338e-02 -1.61472025e-02 -1.50785026e-02\n",
      " -9.55800501e-03  2.30759892e-02 -4.77710341e-04  4.94466307e-03\n",
      "  3.27150477e-02  1.65778665e-02 -1.31775716e-02 -6.31077804e-03\n",
      " -3.26198940e-04  1.56659204e-02 -1.06060297e-02 -2.69497412e-02\n",
      "  1.80794437e-02  8.15120744e-03  2.03871867e-02  4.42134473e-03\n",
      " -2.01184931e-02 -3.53261402e-03  5.44766461e-03  5.59039244e-03\n",
      "  1.67039568e-02 -2.43111388e-02  5.18664339e-03 -1.34989706e-02\n",
      " -4.53731326e-03 -1.65086301e-02 -9.23821370e-03 -1.86917399e-02\n",
      " -1.16998482e-02 -1.90981958e-02  5.94952271e-03 -5.94660180e-03\n",
      "  6.74181248e-03 -2.72402772e-03  6.22488383e-03  1.45504487e-02\n",
      "  2.50432023e-02 -1.80195370e-02  1.31004666e-03  5.26351759e-03\n",
      "  2.83297939e-02  3.26959027e-02  1.44561214e-02 -8.37083907e-03\n",
      "  2.03384802e-02  1.86403500e-03 -6.01060195e-03 -6.40109923e-03\n",
      "  8.96168205e-03 -1.18803475e-02 -2.87023030e-02 -2.28842734e-02\n",
      "  9.49708923e-03 -1.29774465e-02  3.55957978e-02 -1.12567081e-02\n",
      "  1.18372568e-02 -1.60502281e-02 -1.12262908e-02 -2.52248160e-02\n",
      " -4.76000836e-03 -9.97155313e-03  1.45353130e-02 -1.55056049e-02\n",
      "  7.64692485e-03 -1.59521545e-02 -4.98241574e-03 -3.61489680e-02\n",
      " -3.29584698e-02 -2.52046117e-02 -8.55328874e-03  1.70935561e-02\n",
      " -1.30580276e-02 -1.31391948e-03  5.81365619e-03 -1.59651830e-02\n",
      " -2.34934919e-02 -8.34581611e-03 -1.65088267e-02 -3.77417204e-03\n",
      "  2.13537219e-02  1.24094353e-02  3.00955558e-02  2.70331866e-04\n",
      "  9.83089487e-04  1.14999338e-02 -8.72371738e-03  1.89160392e-02\n",
      "  3.71204332e-02 -4.93299128e-03  1.81750906e-03 -2.83841511e-02\n",
      " -1.90051506e-02 -5.14885825e-03 -5.63118916e-03 -1.96303284e-03\n",
      " -5.61915093e-03 -1.76123543e-02  2.68808410e-02  1.48707811e-02\n",
      "  2.00379509e-02 -2.91321267e-03  6.83748716e-03 -1.39710153e-03\n",
      " -2.83526832e-02  2.28215991e-03  2.40164761e-03 -9.42819365e-03\n",
      "  2.36713930e-02  2.90528085e-02 -2.81725447e-02 -6.08939478e-03\n",
      " -1.94874199e-03  1.30329423e-02  2.58341807e-02 -1.63825335e-02\n",
      "  5.35347124e-04 -3.35405231e-03 -8.58183302e-03  1.71951272e-02\n",
      "  2.41405907e-02 -1.22205193e-02 -9.61660945e-03  1.80815411e-02\n",
      " -9.49406058e-03  2.44759426e-02 -8.88623923e-03 -2.03596119e-03\n",
      " -4.57633093e-03  5.67626292e-03  2.48138817e-03  9.92310600e-03\n",
      "  1.36329890e-02 -1.31865821e-02 -1.20196959e-02 -2.37144049e-03\n",
      " -7.58210186e-03 -2.42584795e-02  8.00168057e-03 -2.82945195e-02\n",
      "  1.16379417e-02  2.13483434e-02  1.77762230e-02  1.39530677e-03\n",
      "  9.59522246e-03 -1.72216460e-02 -2.63690213e-02  2.20066126e-02\n",
      " -2.20784307e-02  5.46974978e-03 -1.86870226e-02 -5.84747604e-03\n",
      "  1.37852211e-02 -3.04650260e-03 -2.08562701e-02  8.58171890e-03\n",
      " -1.27866420e-02  1.83348962e-02  2.41255036e-02  2.55933444e-02\n",
      "  6.17830774e-03  1.95180034e-02  1.23617834e-02  4.58733925e-02\n",
      "  8.81785525e-03  1.70031007e-02  2.96223857e-04  3.95448863e-03\n",
      "  2.53763240e-02  8.53155600e-03  4.73567816e-03  1.79587850e-02\n",
      "  1.90046740e-02 -5.82048295e-03  8.86667271e-03  1.42884774e-02\n",
      "  2.94136309e-05 -1.52337920e-02  1.52718094e-02  1.05474024e-02\n",
      "  1.38315188e-02  1.84823270e-02 -6.72126415e-03 -2.22421329e-02\n",
      " -8.28375187e-03 -3.69600127e-02  9.52143043e-03 -2.33610747e-02\n",
      "  2.89697973e-03  2.27798809e-02  9.84725358e-03  2.55351678e-02\n",
      "  3.87015748e-03 -4.81105578e-03  2.43820073e-02  5.45624827e-04\n",
      " -1.53986376e-02  1.10624839e-02 -6.83026072e-03 -2.78470669e-02\n",
      "  8.52626753e-03 -1.97328723e-02  1.27542122e-02 -9.30329325e-03\n",
      "  2.01820299e-02 -7.62038762e-03 -8.80539345e-03  3.88138243e-03\n",
      "  2.28697437e-02  3.22409945e-02  1.74238920e-02 -8.10156504e-03\n",
      " -7.68385706e-03 -5.11584912e-03  1.89111258e-03 -1.44172835e-02\n",
      "  7.06861702e-03  5.86972481e-03 -8.26328862e-03 -4.95832775e-03\n",
      " -1.86260895e-02  8.40396130e-03 -6.67058365e-04 -1.79185159e-02\n",
      "  3.71498365e-02  1.07292608e-02  1.43861985e-02 -1.82319380e-02\n",
      " -3.81123502e-03 -1.93912029e-03  1.91710564e-03  2.49424803e-03\n",
      "  3.95609375e-03  2.63921109e-02 -7.82070034e-03  1.63304206e-03\n",
      "  3.40819856e-02  3.20570200e-02 -1.51396618e-02  4.12989477e-02\n",
      " -2.53308067e-03 -1.26537433e-02  6.90442253e-03  8.90583865e-03\n",
      "  8.19242528e-03 -1.03592076e-02  5.49313818e-03  3.01508745e-02\n",
      "  1.73243734e-02 -5.51373634e-03 -1.31134451e-03 -2.69418159e-02\n",
      "  1.40741907e-02  1.53562386e-02  2.89821969e-02 -1.96953015e-02\n",
      " -1.43536785e-02 -6.62513353e-03  2.06004571e-02 -3.56132105e-04\n",
      "  7.35135020e-03  3.20680145e-03 -1.97366631e-02 -5.56130381e-03\n",
      "  2.56849255e-03  4.67472101e-02 -3.73091840e-03  1.36955332e-02\n",
      "  2.12247961e-02  9.18695116e-03  9.80163431e-03  6.41309249e-03\n",
      "  9.90887974e-03  5.88837646e-04  8.96151730e-03  1.02861230e-02\n",
      "  1.74932921e-02 -1.33007717e-02 -1.86720485e-03  6.51075091e-03\n",
      "  5.51210064e-04  1.39934036e-02  6.97876855e-03  9.08814199e-03\n",
      " -9.12483301e-04  8.54250573e-03  3.66996342e-02  1.22216104e-02\n",
      " -9.67330137e-03  1.02962988e-02 -1.10307025e-02  2.02641208e-02\n",
      "  2.62953027e-02 -1.20301289e-02  2.19521670e-02 -1.13978485e-03\n",
      " -1.42650369e-02 -1.18977042e-02  1.01774219e-02 -1.69327742e-03]\n",
      "484204\n",
      "training: 39 testing: 26\n",
      "correctsource    22\n",
      "missed           17\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    15\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.42857143 0.66666667 0.66666667 0.8        0.         0.4\n",
      " 0.6       ]\n",
      "Accuracy: 0.5128205128205128\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.57      0.59      0.58        22\n",
      "       missed       0.44      0.41      0.42        17\n",
      "\n",
      "    micro avg       0.51      0.51      0.51        39\n",
      "    macro avg       0.50      0.50      0.50        39\n",
      " weighted avg       0.51      0.51      0.51        39\n",
      "\n",
      "accuracy = 0.5769230769230769\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.67      0.65        15\n",
      "       missed       0.50      0.45      0.48        11\n",
      "\n",
      "    micro avg       0.58      0.58      0.58        26\n",
      "    macro avg       0.56      0.56      0.56        26\n",
      " weighted avg       0.57      0.58      0.57        26\n",
      "\n",
      "(444,)\n",
      "[ 6.46176341e-02  2.07802777e-03  6.17497542e-02 -6.35428586e-03\n",
      "  5.54793995e-02 -5.46777115e-02  3.68574354e-02  3.35850941e-02\n",
      "  1.01107759e-02  2.63324626e-02  1.16970918e-02  3.22556512e-02\n",
      " -2.64699618e-02  1.45795718e-02  8.22842241e-03 -6.49582824e-03\n",
      "  2.55903570e-02  7.47417787e-03  1.05708715e-02 -2.59465107e-02\n",
      " -4.10838035e-03  3.53396201e-02 -3.54895861e-03 -2.12125240e-02\n",
      "  2.39085718e-02 -9.77296843e-03  2.68205590e-03 -5.12553835e-02\n",
      "  4.84849018e-02  1.30073360e-02  2.37751014e-02 -1.49573626e-02\n",
      "  2.11471203e-02  2.88284442e-02  3.88469104e-02  1.69182086e-03\n",
      " -4.61632398e-02 -7.73265657e-02 -5.67555271e-03  3.58659063e-03\n",
      " -8.07762887e-03  1.29594415e-02  3.89366516e-03 -6.11772926e-03\n",
      "  4.49111151e-03  4.03741426e-03 -2.03802996e-02 -1.68878850e-02\n",
      "  1.43158744e-02 -1.40696654e-02  2.87993475e-02  1.86687058e-02\n",
      "  2.87643022e-02  2.80947110e-02 -4.22047117e-03 -3.78309947e-03\n",
      "  3.52862448e-02  3.75662430e-02 -5.22065162e-02  1.98809569e-02\n",
      " -2.84011052e-02  1.31788503e-03  4.03265634e-02  5.83848837e-03\n",
      "  1.70119703e-02 -3.89719354e-02  2.76859149e-02  1.14805041e-02\n",
      "  2.50321714e-02  6.75899186e-04 -2.70728795e-03 -2.48941449e-03\n",
      " -7.88589424e-02  3.47388997e-03  1.53430331e-02  4.48673014e-02\n",
      " -4.08212682e-02 -4.91955108e-03  1.72882109e-02 -3.27893852e-03\n",
      " -2.03517654e-02 -3.40855551e-03  1.41339911e-03  1.14578668e-02\n",
      " -1.38338692e-02 -5.02217809e-02 -1.73396820e-02  1.08951396e-02\n",
      " -6.82077930e-03  1.66637340e-02  2.14984256e-03  1.00751110e-02\n",
      " -7.53690106e-03  2.86546942e-02 -1.63862007e-02  1.52458211e-02\n",
      "  1.99313977e-02  5.10358705e-03 -1.80445167e-02 -4.76714727e-02\n",
      "  3.66138690e-02  2.51062372e-02 -2.93632370e-02 -3.10517714e-02\n",
      " -1.48713092e-03  2.56613188e-02 -7.58903358e-02  1.11757720e-03\n",
      " -1.49908386e-02 -4.76848629e-03  3.30210355e-02 -9.87899656e-03\n",
      " -2.65524457e-02 -3.13827703e-03 -3.04668199e-02  3.69879675e-02\n",
      " -6.35487352e-02 -1.26880738e-03  2.52943662e-02  5.23060927e-02\n",
      " -8.42894664e-02  1.38421257e-02  4.36870123e-02  3.09701429e-04\n",
      " -4.71828495e-02 -5.42308203e-04 -2.61726964e-02 -1.86241461e-02\n",
      " -3.69721607e-03  8.40976489e-03 -6.19409208e-02  3.88591092e-02\n",
      " -1.19315904e-02 -2.17143076e-02  1.81380249e-02  3.28470723e-02\n",
      " -1.85947438e-03  8.36251013e-04  1.43514492e-02 -5.82974053e-02\n",
      " -4.12219283e-02  3.62871500e-02  2.01897536e-02  5.88607144e-03\n",
      "  2.41089041e-02 -2.70082295e-02  7.34081823e-02 -1.32190894e-02\n",
      " -1.25479779e-02 -3.71404909e-02 -8.71854699e-03 -2.96592978e-03\n",
      "  9.19964155e-03 -2.09990970e-02  6.60222653e-02  1.41600785e-02\n",
      " -1.09871138e-03 -1.32254543e-02  2.94712719e-02 -6.74460075e-03\n",
      " -4.14039213e-02 -1.59068803e-02 -7.68031486e-03 -1.96789658e-02\n",
      "  2.80078619e-02 -3.55083917e-03  1.84879900e-02 -1.73426467e-02\n",
      " -3.09432609e-02  4.23513931e-02 -5.68439124e-03  1.59684115e-02\n",
      " -2.58736676e-02  3.88661887e-03 -6.83701599e-02  3.94871874e-02\n",
      " -2.03493220e-02 -6.51410697e-04  1.75037500e-02  9.63577019e-03\n",
      " -2.41994973e-02 -5.06385400e-02  1.11383361e-02 -3.28674948e-02\n",
      " -1.24302847e-02 -1.19865779e-02  4.88082738e-02  1.90069241e-02\n",
      "  9.04364725e-03  2.27365946e-03 -1.05482086e-03  3.23979045e-02\n",
      "  6.41039031e-02 -1.08751577e-02 -6.36617586e-02  3.43232368e-02\n",
      " -1.12586649e-02  5.07144038e-02  1.84888111e-03  5.32967472e-02\n",
      " -7.80777662e-03 -1.43911830e-02  4.78738291e-02 -9.87330031e-02\n",
      " -8.67286850e-03 -3.39391400e-02  1.13765536e-02  1.12866434e-02\n",
      " -2.78144626e-02  2.08202850e-02  3.10430645e-02 -1.55234936e-02\n",
      "  3.58441633e-02 -2.68781841e-02 -6.07270591e-03  7.20030215e-03\n",
      " -1.74884106e-02  1.72063908e-02  8.35750238e-03 -3.14368449e-02\n",
      "  2.48380173e-02 -1.06806057e-02 -1.46410730e-03 -1.28783603e-02\n",
      "  3.16395815e-02  2.23413069e-02  1.05440205e-02 -5.06576083e-02\n",
      "  2.73709736e-02  5.41441041e-02  9.11046112e-03  3.04222437e-02\n",
      "  7.66017471e-03  3.66404148e-02  3.52287964e-02  2.78530250e-02\n",
      "  7.19701658e-03  3.11629267e-02  2.29428821e-02 -1.13102131e-02\n",
      "  3.00514035e-02 -5.48970745e-02 -1.50016116e-02 -5.24288526e-02\n",
      " -3.57756876e-02 -1.21175609e-02 -1.64336238e-02 -4.26576085e-03\n",
      "  4.54199187e-02 -1.24817351e-02 -1.13247518e-02  6.36251351e-02\n",
      "  1.63119095e-02 -8.02641461e-02  1.11268802e-02  3.03160046e-05\n",
      " -3.92273604e-02  1.80949205e-02 -2.31928075e-03  3.38106902e-02\n",
      "  2.04866086e-02 -7.29829915e-02  2.16659900e-02  9.97702592e-03\n",
      " -4.41615463e-02 -1.68189733e-02 -9.33976681e-03 -9.87375197e-03\n",
      "  2.70875762e-02  1.56580354e-03  8.46846493e-03  4.47047258e-02\n",
      " -3.04007141e-02  4.25794500e-02 -1.86589274e-02  8.01464557e-03\n",
      "  4.27226943e-02  7.23115092e-02  8.25671399e-03 -9.06775044e-03\n",
      " -5.14809433e-02  3.73362689e-02 -3.34568292e-02 -6.11407255e-03\n",
      " -2.12761706e-03  9.90944619e-03 -8.93523555e-03  6.80827769e-02\n",
      "  4.56401079e-03 -4.50083477e-02 -5.03206601e-03  7.89138012e-04\n",
      " -1.03977490e-02 -4.80226646e-03  4.88592918e-02 -2.78901948e-02\n",
      " -9.10857932e-03 -8.70849405e-03  5.26227494e-03  4.16036363e-02\n",
      "  3.81031539e-02 -3.35278362e-02 -4.22899667e-04  1.99793332e-02\n",
      "  1.78746792e-02  6.68049783e-04  4.04098782e-02  6.36620300e-03\n",
      " -3.30530003e-02 -6.53679979e-03  7.22385610e-03 -4.45974891e-03\n",
      "  2.13790030e-02  4.00297591e-02  2.97357872e-02 -4.88751811e-02\n",
      "  3.47803694e-02 -3.38046409e-03 -3.05337514e-02  4.06794242e-02\n",
      " -5.98454263e-03  5.61673108e-03  7.15831947e-02 -2.66891332e-02\n",
      "  1.00303372e-02 -9.01995658e-03  3.74182671e-02 -2.70994821e-02\n",
      "  3.12717751e-02  2.11060089e-03  2.39362486e-02 -7.15724931e-05\n",
      " -3.44861228e-02  5.22777169e-02 -1.70351981e-02  7.42563118e-03\n",
      "  2.75721247e-02 -5.54351051e-02 -9.78247296e-03 -1.19742430e-02\n",
      "  2.01474150e-02 -2.37253430e-02 -5.19709852e-02 -5.36699523e-02\n",
      " -1.42300007e-02  2.56063631e-02  8.02942934e-02  3.54683520e-02\n",
      " -2.50055779e-02  5.03361468e-02 -5.87385824e-02  8.66798400e-02\n",
      " -3.68920926e-03 -2.28818670e-02  3.22108164e-03  3.33554800e-02\n",
      "  4.96297870e-02 -1.36778951e-02 -2.59656403e-02  1.27452928e-02\n",
      " -2.34836531e-02 -3.87430797e-03 -7.31343790e-02 -1.91909899e-03\n",
      " -4.53975366e-02 -1.39805577e-03 -5.21614337e-02 -2.33181009e-02\n",
      " -3.58327633e-02 -1.32551988e-02  1.20903574e-01  2.07682737e-02\n",
      "  3.77400525e-02 -1.68782665e-02 -1.27575642e-02  1.29084584e-02\n",
      " -1.91394590e-02  8.00270752e-02 -3.23445748e-02  1.86955718e-02\n",
      "  5.78170896e-02 -4.71985417e-02  5.14164464e-03 -1.08784971e-02\n",
      " -1.09109031e-02  3.56947295e-03 -5.58929867e-02  8.24982082e-03\n",
      "  2.30455725e-02  1.42993221e-02 -1.57785704e-02 -4.52336832e-03\n",
      "  2.86827670e-02  1.13671755e-02 -6.53937089e-03  3.28500063e-02\n",
      "  1.06634679e-02 -6.25887083e-02 -3.98240577e-02  2.41159272e-02\n",
      " -1.19996752e-01  9.21613717e-02  5.05096926e-02 -5.40951393e-02\n",
      " -8.58315563e-03 -5.09180811e-02 -2.05226590e-02 -1.62974771e-02\n",
      "  5.88270383e-02  5.19389968e-02  4.78492144e-02  1.53017036e-02\n",
      "  3.58018213e-02  6.49340463e-03  1.29673549e-02  3.50055262e-03\n",
      " -4.91095292e-02 -1.21975557e-02 -2.57628614e-02  2.05253526e-02\n",
      " -9.42746984e-03 -2.56594301e-02  4.10689168e-02  2.97567539e-02\n",
      "  2.37393971e-02  2.01009829e-02  2.69770947e-02  4.56788795e-04\n",
      " -2.12133583e-02  2.76780315e-02 -1.63892853e-02  1.21349298e-02\n",
      "  2.27027526e-02  3.03820141e-02 -3.63957504e-02 -7.93297508e-03\n",
      "  1.00132300e-02  1.08090424e-02  1.32119618e-02  9.41867999e-03\n",
      " -5.42547075e-02 -1.02040566e-02 -6.17474516e-02 -1.79140328e-02]\n",
      "502616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 31 testing: 21\n",
      "correctsource    21\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    15\n",
      "missed            6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.4  0.8  0.8  0.75 0.25 1.   0.  ]\n",
      "Accuracy: 0.5806451612903226\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.72      0.62      0.67        21\n",
      "       missed       0.38      0.50      0.43        10\n",
      "\n",
      "    micro avg       0.58      0.58      0.58        31\n",
      "    macro avg       0.55      0.56      0.55        31\n",
      " weighted avg       0.61      0.58      0.59        31\n",
      "\n",
      "accuracy = 0.6190476190476191\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.71      0.80      0.75        15\n",
      "       missed       0.25      0.17      0.20         6\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        21\n",
      "    macro avg       0.48      0.48      0.48        21\n",
      " weighted avg       0.58      0.62      0.59        21\n",
      "\n",
      "(444,)\n",
      "[ 1.61617041e-02 -4.10926371e-03  2.62595486e-03  2.61494644e-02\n",
      " -5.95563770e-03 -2.32379917e-02  1.21178505e-02 -2.36394370e-02\n",
      "  1.52069304e-02  5.89311313e-03 -5.70769298e-03 -8.14293161e-04\n",
      "  1.19175886e-02  3.46300615e-03  1.20807603e-02  3.36896186e-02\n",
      " -1.43504137e-02 -2.31104652e-02 -1.01206620e-02  1.94986207e-02\n",
      " -2.23755675e-02 -3.17918405e-03 -2.62080844e-03 -1.33008296e-02\n",
      "  1.86230421e-02  3.23170542e-02 -1.58107777e-02 -5.88125700e-03\n",
      " -6.36627862e-02 -2.68168571e-02 -2.13393330e-02  7.51058456e-03\n",
      "  1.04483072e-02 -1.51912598e-02  9.89839891e-03 -1.56335009e-02\n",
      "  1.23238282e-02  6.88403733e-03 -2.74142422e-03 -1.01780924e-02\n",
      "  2.29305400e-02  6.99870774e-03  1.10432083e-02  2.73978684e-02\n",
      "  1.27703560e-03  1.04623803e-03  3.09478790e-02  4.71810969e-03\n",
      "  1.15114138e-02 -2.45932032e-02  8.25339880e-03 -1.40593833e-03\n",
      " -5.00410592e-03 -2.53573892e-02  8.19412303e-03  7.14225283e-03\n",
      "  8.14237020e-03  2.40517839e-02  6.60370904e-03 -9.58281603e-04\n",
      " -5.07920583e-04  3.04037841e-02  7.70345558e-03  2.89693454e-02\n",
      "  2.50154786e-02  5.02554574e-03 -1.38811944e-02  3.83082062e-03\n",
      " -2.02024641e-02  4.88406178e-03  5.70123812e-03  1.81595390e-02\n",
      " -3.77989760e-02  2.01483866e-02 -1.96749213e-02  5.21503141e-03\n",
      " -2.22931772e-02  2.60643505e-02  9.39141073e-03 -4.46465365e-03\n",
      " -2.28133588e-02  2.45466370e-03  3.56340776e-02 -1.47260142e-02\n",
      "  1.84362609e-02 -4.47251305e-02  2.42303072e-02  1.23118197e-03\n",
      "  1.89637188e-02  4.39729866e-03 -2.00964999e-02 -3.12263317e-04\n",
      " -1.65636741e-02  3.78557815e-03 -3.85846503e-03  2.45244398e-02\n",
      " -3.30577962e-02  1.77597732e-02 -8.40692771e-03 -1.99785314e-02\n",
      " -1.98475714e-02 -2.35876890e-02  1.16708599e-02 -8.96403593e-04\n",
      "  3.91560873e-02 -4.62861229e-03 -2.12026999e-02 -3.52581181e-02\n",
      "  2.35165609e-02 -5.48325048e-03  3.07665818e-02 -5.58032672e-03\n",
      "  3.57082695e-04 -1.15305117e-02  2.68866981e-02 -1.26649132e-03\n",
      " -2.41842209e-02  9.60080917e-03  7.41415134e-04  1.00581056e-02\n",
      "  7.38980140e-04 -2.04286351e-02 -8.95585550e-03 -1.45676614e-02\n",
      " -1.82124958e-02  1.03238607e-02  1.75005677e-02 -1.55584610e-02\n",
      "  4.38226194e-03  1.81770754e-02 -7.72729310e-03 -7.29622400e-03\n",
      " -2.24633685e-02 -3.59537496e-02  1.22159253e-02  1.62180421e-02\n",
      "  2.46210735e-02  8.53064105e-03  1.58056046e-02  2.13134806e-02\n",
      "  5.03846299e-03 -2.85367004e-02  1.39240613e-02 -2.03176955e-03\n",
      "  5.36223587e-03  3.28974357e-02 -4.36118433e-03  3.30607372e-03\n",
      "  2.53186372e-02 -2.24507622e-02 -1.22904979e-04 -4.94411507e-03\n",
      "  1.66159845e-02  8.30619055e-03  1.10661099e-02  1.44414481e-02\n",
      "  1.23865949e-02 -1.27606813e-02 -2.49556986e-02 -1.49385288e-02\n",
      " -6.06502060e-03  5.77823108e-03  1.62889843e-02 -1.73954755e-02\n",
      " -1.45739524e-02 -2.04646428e-02 -2.46369816e-03 -9.33782686e-03\n",
      "  7.14574546e-03  4.28938324e-03 -8.36500871e-03 -4.40883414e-03\n",
      " -1.19848712e-02  6.11908291e-03  1.45605557e-03  3.04211373e-02\n",
      " -2.55451753e-02 -1.42718761e-03 -1.63181941e-02 -2.13403699e-02\n",
      " -3.60428855e-02  4.43069108e-02 -1.97811158e-02 -2.99228272e-02\n",
      " -1.36387337e-02 -1.44475564e-02 -4.86237104e-03  2.33791486e-02\n",
      " -1.25016043e-02 -1.76514405e-02 -7.70763424e-03  7.26285129e-03\n",
      " -2.79794286e-02 -1.17483017e-02  1.75041829e-02 -3.20522017e-03\n",
      "  3.18329381e-03 -5.46940705e-03 -1.95652377e-02 -3.06193620e-02\n",
      "  1.40898260e-02  2.08735910e-03  3.27945646e-03  1.77505856e-02\n",
      "  7.19803340e-03  1.99274511e-03  9.54376763e-03  1.08372480e-02\n",
      "  2.43061976e-02  1.35381203e-02 -3.03226916e-02  4.67147811e-02\n",
      " -6.78362017e-03 -1.00260374e-02  5.46625557e-03  2.24515844e-03\n",
      "  7.18207857e-03 -3.03753192e-02 -1.08582543e-03  3.15873751e-03\n",
      " -1.74333721e-02 -3.07689917e-02 -2.24626599e-02 -4.44597666e-04\n",
      " -3.06056572e-03 -6.43115235e-03 -2.66502255e-02  2.05470368e-02\n",
      "  1.07551016e-02 -1.02566039e-02 -2.30434089e-03 -1.08557367e-02\n",
      "  9.63685055e-03  1.45060346e-02 -2.22932862e-02  1.68294734e-02\n",
      " -9.31321528e-03  2.05634600e-03 -1.78077956e-02 -4.72380421e-03\n",
      "  9.76489385e-03  8.78129606e-03  3.23246145e-02  2.20414037e-02\n",
      " -7.96077799e-03 -7.61043694e-03 -2.70373183e-02  1.91419744e-03\n",
      "  2.79228911e-02  8.32228982e-03  2.03160933e-03 -4.81218600e-03\n",
      "  9.54530137e-03  2.26838156e-02  1.28445456e-02  9.95883492e-03\n",
      "  1.51770203e-02 -9.97528337e-04 -8.99361289e-03 -2.84693795e-02\n",
      "  9.31726078e-03  2.89231509e-02 -9.86343254e-03  1.67105014e-02\n",
      "  2.93866840e-02 -7.03957143e-03 -9.10485870e-03 -6.68582424e-03\n",
      " -1.17975212e-02 -1.14336629e-02  1.54792065e-02  1.17711495e-02\n",
      "  4.11925590e-02 -7.89789028e-03 -2.20922500e-02 -2.33894387e-02\n",
      "  7.69517468e-03 -4.40411622e-04  6.51217204e-03  2.89639065e-02\n",
      "  2.47828319e-02  8.69691404e-03  2.75127778e-02  4.55854220e-04\n",
      " -3.23191089e-03  1.64122630e-02  2.42080825e-02  2.43988694e-03\n",
      "  3.44273664e-03  1.27361532e-02 -1.27362840e-02  1.66683767e-03\n",
      " -2.21249053e-02  1.34914722e-02  3.20518704e-03  7.40877373e-03\n",
      " -2.18027859e-02 -5.82141661e-03 -1.99291750e-03 -2.19990788e-02\n",
      "  9.65197618e-03  3.21428733e-03 -3.41528909e-03 -8.14325874e-03\n",
      " -2.39261629e-02  7.46923335e-03 -3.01472382e-02  1.98039479e-03\n",
      " -4.57788510e-03  1.37440591e-02 -1.73130870e-02  9.02138610e-03\n",
      " -3.62553704e-02 -1.20796153e-03  6.32673395e-03 -5.29425062e-02\n",
      "  6.82926533e-05 -1.77799334e-02  9.45710720e-03  9.54541389e-03\n",
      " -1.42264655e-02  6.91360295e-03  3.51231494e-03 -7.30033586e-03\n",
      "  1.57382145e-02  1.86452871e-02 -1.38682482e-02  5.85620566e-03\n",
      "  2.65075556e-02 -1.09369261e-02 -7.28275587e-03  4.73789775e-03\n",
      " -6.68778320e-03 -4.72380537e-03 -2.16109722e-02 -4.58767898e-02\n",
      "  3.83091758e-02 -6.64178372e-04  5.13688868e-03 -2.28827971e-02\n",
      "  3.44889726e-02  2.12317804e-02  6.13847900e-03  1.76032984e-02\n",
      "  6.46854037e-03 -1.92567832e-03  9.12937982e-03 -1.10925678e-02\n",
      "  3.43148812e-02  1.00538793e-02  1.55517221e-02 -2.30377956e-03\n",
      " -1.06417410e-02 -1.82552755e-02  1.40753469e-03 -1.65339771e-02\n",
      "  2.81634872e-02 -3.85308773e-03 -1.52876581e-02  1.98674847e-02\n",
      "  1.41423949e-02  1.78150537e-02  3.89533139e-02 -1.07370405e-02\n",
      "  2.35408654e-02  2.12175261e-02 -3.72627521e-03  7.71230586e-03\n",
      " -6.56233914e-03 -1.89269695e-02  1.80331682e-02 -9.33001192e-03\n",
      "  1.41946430e-02  4.10492512e-02  2.90355383e-04 -1.20430930e-02\n",
      "  3.46993443e-03 -3.60376330e-02 -1.41899013e-02 -6.52307975e-03\n",
      "  2.22420001e-02  1.40480633e-02  1.41843169e-02 -9.86360144e-03\n",
      "  9.68037229e-03  1.94500836e-02 -1.45356791e-02 -1.32572950e-02\n",
      "  4.36303614e-03 -2.14381331e-02  2.16198001e-02 -1.93086786e-02\n",
      "  2.64899337e-02 -1.40183902e-02 -2.11650075e-02  1.34373579e-03\n",
      " -2.84563257e-02  1.55924975e-02  2.51644181e-03 -1.80971010e-03\n",
      " -1.47780714e-02 -1.69879793e-02  1.98205393e-02  1.88966473e-02\n",
      "  4.63513342e-03 -1.91538262e-02  4.04463564e-03 -1.98979058e-02\n",
      " -4.26731363e-02 -1.71348641e-02  4.84603849e-03  2.79966299e-02\n",
      "  6.54591582e-03 -7.93315202e-03 -2.27693207e-03 -1.31693628e-02\n",
      " -1.76947939e-02 -2.02170774e-02  6.89831938e-03 -2.48374139e-02\n",
      " -3.02036124e-02  5.20317572e-03 -4.27885920e-02 -2.23984140e-02\n",
      "  7.99650890e-03  9.84263645e-03 -1.89409618e-02 -2.95290890e-02\n",
      " -6.67577403e-03 -8.26495827e-03 -2.73536975e-02 -7.52025493e-03\n",
      "  1.86680537e-02  6.03495926e-03 -2.07111837e-02 -8.25892688e-03\n",
      " -1.76493909e-02  1.22025104e-03  2.12310590e-02 -1.94917136e-02\n",
      "  3.13103065e-03  2.90466403e-03  1.03118757e-02 -2.28349876e-02]\n",
      "567214\n",
      "training: 29 testing: 20\n",
      "correctsource    17\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    12\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6        1.         0.2        0.75       0.75       0.33333333\n",
      " 0.33333333]\n",
      "Accuracy: 0.5862068965517241\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.65      0.65      0.65        17\n",
      "       missed       0.50      0.50      0.50        12\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        29\n",
      "    macro avg       0.57      0.57      0.57        29\n",
      " weighted avg       0.59      0.59      0.59        29\n",
      "\n",
      "accuracy = 0.55\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.60      0.75      0.67        12\n",
      "       missed       0.40      0.25      0.31         8\n",
      "\n",
      "    micro avg       0.55      0.55      0.55        20\n",
      "    macro avg       0.50      0.50      0.49        20\n",
      " weighted avg       0.52      0.55      0.52        20\n",
      "\n",
      "(444,)\n",
      "[-7.43599772e-03 -1.63383290e-02 -6.14366540e-02  5.76511942e-04\n",
      " -1.22231112e-03 -2.19208941e-03 -2.43410915e-05  8.69075933e-03\n",
      "  8.89739604e-04  3.49664478e-02 -1.96622922e-02  4.47355964e-02\n",
      "  1.81458976e-02  1.49268450e-02 -1.17010399e-02  1.42098467e-02\n",
      "  1.39043770e-03 -8.67872483e-03  1.09313888e-03  2.41930254e-02\n",
      " -8.89102298e-03 -4.91589034e-02  4.89749487e-03 -2.63988984e-02\n",
      "  2.17428119e-02  1.18292370e-02 -6.56313104e-03  2.00234995e-02\n",
      "  6.81392305e-03  1.84133785e-02 -1.81593875e-02 -1.13569305e-02\n",
      " -1.51026026e-02  1.49889286e-02  1.41446545e-02 -3.36276221e-02\n",
      "  1.34966924e-02 -6.69930009e-03 -2.33094859e-04 -1.51532084e-02\n",
      " -1.50637280e-02  7.60092134e-04  1.81100662e-02 -2.95234064e-02\n",
      " -7.14593329e-03  3.96463591e-03  1.85210906e-02  3.67644818e-02\n",
      " -2.79123752e-02 -5.91293455e-03  1.16186272e-02 -2.23660651e-02\n",
      "  1.18917252e-02  1.18836437e-02  3.29929420e-04  1.56128480e-02\n",
      " -3.45128516e-03  6.10723944e-03 -1.37247779e-02  2.83613972e-02\n",
      "  8.71310957e-03  1.36082694e-03  8.14659416e-03 -5.99589312e-03\n",
      "  1.59578174e-02  1.07888993e-03  3.41386729e-02  1.37154992e-02\n",
      " -1.59827835e-02 -1.25724945e-02  1.08314776e-02  4.30086769e-02\n",
      " -2.31330300e-04  3.86171843e-03 -1.23882816e-02 -1.50885736e-02\n",
      " -2.73110461e-04 -1.65832140e-03 -2.47728656e-02 -1.89412126e-02\n",
      " -9.66737234e-03 -8.72067538e-03 -7.25251850e-03  4.22190492e-03\n",
      " -7.95364303e-03 -4.10604777e-03 -2.67177101e-02  1.18636917e-02\n",
      "  2.05281567e-02 -1.94157265e-02 -4.32158833e-02 -9.51710925e-03\n",
      " -3.15292821e-02  5.57937462e-03 -9.77174055e-03  9.12331801e-04\n",
      " -3.04647131e-03  7.35600903e-03 -2.62253942e-02  9.05117631e-05\n",
      " -2.06460217e-02  1.40895361e-02  2.96246977e-03 -5.72645713e-03\n",
      "  7.79783296e-03 -2.04583297e-04  5.38321223e-02 -1.59421368e-02\n",
      " -1.44915360e-02 -4.54538461e-03 -2.70118067e-03 -9.15201089e-03\n",
      "  2.37959379e-02 -1.71763152e-02  4.47110016e-05  5.30640234e-03\n",
      " -5.90791856e-03  2.83573402e-04 -2.93183786e-02  3.52479213e-02\n",
      " -1.92339270e-02  1.16881469e-02  3.13896125e-02  7.41963744e-03\n",
      " -8.56549669e-03 -2.19950255e-02 -4.95958966e-04 -1.36835456e-02\n",
      "  8.74075433e-03 -3.83071039e-02  1.63012570e-02  1.12094238e-02\n",
      "  1.71001152e-03  1.38986303e-02  1.53689229e-02  2.71970397e-03\n",
      "  5.90627133e-03  6.00393375e-02  3.03900410e-03  2.44287599e-02\n",
      "  7.79823910e-03  2.34587059e-03 -4.88677299e-03 -3.13889972e-02\n",
      "  8.30715565e-03 -2.03913293e-02  1.54188954e-02 -1.04002120e-02\n",
      " -2.80648703e-02 -1.37910124e-02 -1.28122425e-02  1.18078156e-02\n",
      "  1.24383436e-02  8.46650377e-03 -8.55950562e-05  3.46807636e-02\n",
      " -8.48821968e-03 -2.93458468e-02  1.44922722e-02 -5.23384306e-04\n",
      " -1.59949708e-02  2.45226841e-02  7.23765051e-03 -4.77437482e-03\n",
      "  2.74002072e-02 -1.79228616e-02  4.56969084e-03  4.34304531e-03\n",
      " -9.06539893e-03 -1.24840413e-02 -5.58118225e-03 -7.65391785e-03\n",
      "  1.50975227e-02 -2.50932354e-02  1.41203674e-02  2.70750185e-02\n",
      "  3.88014236e-02 -1.74341034e-02 -2.78741037e-02  1.01655977e-02\n",
      " -9.51412791e-03  2.01512001e-02  1.11118648e-02  9.69406529e-03\n",
      " -3.82483480e-02  7.05589469e-03  1.14637809e-02  1.78326520e-02\n",
      "  5.33727503e-05  2.61807514e-02 -3.37009713e-02 -7.96735900e-03\n",
      "  3.06801080e-02  1.84549398e-03  6.33750241e-03  1.05773278e-02\n",
      " -1.94379164e-02  8.80240656e-03 -1.10524216e-02  2.66991040e-02\n",
      " -1.37425530e-02 -1.71915324e-02 -1.68222595e-03 -2.58487298e-02\n",
      "  1.24419865e-02 -3.18730366e-02 -5.03434112e-03 -1.36726781e-02\n",
      "  2.28438344e-03  1.34765974e-04  3.10322062e-02 -4.68140197e-03\n",
      " -1.27406973e-02  8.79514410e-03  2.37189360e-02  2.59367708e-02\n",
      "  2.69815776e-02 -2.00135701e-03  2.31187499e-02 -7.45400520e-03\n",
      " -1.26823489e-02 -1.85824718e-02 -1.23486397e-02  1.84842777e-02\n",
      "  3.36419331e-04 -1.53231742e-02 -2.42419167e-02 -1.95376144e-02\n",
      "  2.14733026e-03 -7.38721811e-03  5.97381920e-03  1.78899691e-02\n",
      "  1.10806522e-02 -5.70989087e-03  4.93440354e-04 -9.48101628e-03\n",
      " -8.77361584e-03 -2.56263193e-02 -1.35841894e-02 -7.52888763e-03\n",
      " -1.38115121e-02 -2.30970813e-02 -3.82601760e-02  2.42649505e-02\n",
      "  9.20215817e-03  3.40798356e-02 -1.35659442e-02  5.37811657e-03\n",
      "  2.08851875e-03  1.63770853e-02 -7.99427353e-04 -8.66332509e-03\n",
      "  4.12600984e-03  3.75890142e-02  1.27907818e-02  8.91853347e-03\n",
      "  2.09095117e-02 -8.56741610e-03 -2.43936579e-03  1.67973270e-02\n",
      " -1.01976283e-02  1.46299804e-02 -2.39066424e-02  3.41057212e-03\n",
      "  3.25633070e-02  1.47365627e-03  9.80102817e-03 -6.94388574e-03\n",
      "  1.07546693e-03 -3.18923640e-03  8.69663574e-03  3.51664767e-02\n",
      " -1.30402144e-02  2.77724474e-02  9.94606283e-03  1.28823365e-02\n",
      " -1.19086245e-02  1.46045807e-02 -3.83588990e-03 -1.53063380e-02\n",
      "  1.40378579e-02  2.22248707e-02  8.62863957e-03  5.09992948e-03\n",
      "  5.83518319e-03  4.36361789e-03 -7.87560741e-03  6.38494053e-03\n",
      "  1.47189423e-02 -6.19235679e-03  6.94683574e-03 -2.32463095e-02\n",
      " -3.60860346e-03 -1.59806533e-02  1.85803703e-02  2.50746846e-02\n",
      " -1.96001179e-02  1.97990126e-02  5.21586521e-04  1.51152675e-03\n",
      " -1.36227401e-02  4.39039319e-02 -1.98247209e-03  2.94830150e-02\n",
      " -1.34256757e-02 -5.34242158e-03  1.66340682e-02 -1.29018848e-02\n",
      " -7.88763044e-03  9.63078785e-03 -3.46097486e-03  6.70614121e-03\n",
      "  1.44771379e-02  1.00507242e-03  1.27831644e-02 -8.39303776e-03\n",
      "  5.54271539e-03  8.09212146e-03  2.83775351e-02  2.55853988e-02\n",
      " -6.16162507e-04  2.74342643e-02  2.81103051e-02  1.37506804e-02\n",
      " -1.88240722e-02  6.28736618e-03 -1.32262542e-02 -9.23637323e-03\n",
      " -5.71446557e-03  4.11182384e-02  1.81494637e-02  2.10613770e-02\n",
      "  1.01928389e-02  1.42394800e-02 -6.20152845e-03 -2.11265634e-02\n",
      "  4.27719520e-02 -1.12922961e-02 -1.74349387e-03  4.34349750e-03\n",
      "  2.91827416e-02 -3.90437121e-02 -1.15963858e-02  4.89538287e-02\n",
      "  1.77669690e-02  3.35449423e-02 -5.38874121e-03  1.05381136e-02\n",
      " -2.71606600e-02 -2.00801560e-02 -3.16239931e-02 -3.82952058e-03\n",
      "  3.40804109e-03  1.17118337e-02 -1.32738617e-02 -1.82837882e-02\n",
      "  7.68326796e-03 -1.49395724e-02  2.02452066e-03  1.66008235e-02\n",
      " -2.10473261e-02 -3.21236853e-02 -6.66713579e-03 -8.24189866e-03\n",
      "  6.50078448e-03  6.21730584e-03 -2.24481373e-02 -1.87293746e-03\n",
      "  1.86899914e-03 -1.13870037e-02  5.70187652e-04  5.77127373e-03\n",
      " -8.82669062e-03  3.14069669e-02 -1.38078650e-02 -1.00362608e-02\n",
      " -4.09841613e-03 -2.71207458e-03 -1.95312185e-02 -2.42614153e-03\n",
      "  6.85531279e-03  4.16851684e-02  8.23995655e-03 -2.36428140e-02\n",
      " -2.07739128e-02  1.26411918e-02 -7.10191315e-03 -2.19195976e-02\n",
      " -1.59728288e-02 -6.82508974e-04  6.00358953e-03 -7.10459238e-03\n",
      "  1.18543990e-02 -4.80074121e-03  2.14579715e-02 -1.49801565e-02\n",
      "  4.31415171e-02  1.44376979e-02 -1.98522027e-02 -1.71903061e-02\n",
      " -1.37714149e-02  9.63687003e-03  1.94169421e-02 -6.18369400e-03\n",
      "  3.39438733e-03  1.96545843e-02  7.28036574e-03 -3.49047588e-03\n",
      " -2.84503902e-02  6.60199767e-03  2.92132803e-02 -4.25503147e-03\n",
      "  8.11450557e-03 -2.56485050e-03  1.13903825e-02 -1.57330673e-02\n",
      "  6.96352717e-03  1.71080540e-02 -3.16669584e-02  1.34986643e-02\n",
      "  4.75030909e-03 -1.94887938e-02  2.05732164e-03  1.11156582e-02\n",
      "  3.89658911e-03  1.42026828e-02 -9.77669543e-03  1.93031044e-02\n",
      " -4.29960374e-02 -2.18271000e-02  2.89667944e-02  6.98553124e-03\n",
      " -1.22419704e-02 -2.09073646e-02  2.09136431e-03 -3.38747645e-03\n",
      " -9.51245419e-03 -2.50082966e-03  2.00297283e-02  1.55387544e-02\n",
      " -1.59436543e-02  1.66747000e-02  1.52644210e-02 -1.65115042e-02]\n",
      "597569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 39 testing: 27\n",
      "correctsource    28\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    20\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.33333333 0.66666667 0.33333333 0.2        0.6\n",
      " 0.4       ]\n",
      "Accuracy: 0.41025641025641024\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.59      0.57      0.58        28\n",
      "       missed       0.00      0.00      0.00        11\n",
      "\n",
      "    micro avg       0.41      0.41      0.41        39\n",
      "    macro avg       0.30      0.29      0.29        39\n",
      " weighted avg       0.43      0.41      0.42        39\n",
      "\n",
      "accuracy = 0.7037037037037037\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.80      0.80      0.80        20\n",
      "       missed       0.43      0.43      0.43         7\n",
      "\n",
      "    micro avg       0.70      0.70      0.70        27\n",
      "    macro avg       0.61      0.61      0.61        27\n",
      " weighted avg       0.70      0.70      0.70        27\n",
      "\n",
      "(444,)\n",
      "[-0.03424687  0.02787332  0.0268653  -0.03116027  0.00969964 -0.0002039\n",
      "  0.00976546  0.02577781  0.00327953 -0.00807668  0.00587842 -0.01632967\n",
      " -0.00914959 -0.00503422 -0.00825021  0.03641705 -0.00268768  0.00264844\n",
      " -0.02068899  0.03529328  0.02501685  0.00678077 -0.00057106  0.01264777\n",
      "  0.01290533 -0.02593549  0.00737038  0.03751913 -0.01139288 -0.01011044\n",
      "  0.00062473  0.01793659  0.03915531  0.00537719  0.00442756 -0.0270675\n",
      "  0.0174515   0.0028289   0.00360116 -0.01554032  0.01968704  0.01731693\n",
      "  0.01894872  0.00507407  0.0183538  -0.01257641 -0.00721075 -0.03519202\n",
      "  0.01841332 -0.0054571   0.02475713  0.00868208  0.00923632  0.00604492\n",
      "  0.00739379 -0.01290236  0.02351713 -0.00154712 -0.02790111 -0.0131169\n",
      "  0.00674524 -0.02706428 -0.0423739  -0.00633417 -0.03116363 -0.00244066\n",
      " -0.00783744  0.02070724  0.02735432 -0.00528803  0.01224364  0.03250796\n",
      "  0.00940253 -0.00644638  0.05825463  0.00600866  0.02269857 -0.00931654\n",
      "  0.00035392 -0.00145071  0.01565227  0.00355232 -0.01724091  0.0092851\n",
      "  0.03136597 -0.03988615 -0.0438358   0.02352556 -0.0331131  -0.0358158\n",
      " -0.01142725 -0.00901809 -0.0444125   0.00770633  0.00530819  0.01235288\n",
      " -0.01977392  0.02338725  0.02601881  0.00595877  0.0398757   0.00370536\n",
      "  0.02736787  0.00215243  0.03067109 -0.01527275 -0.01612223 -0.00095167\n",
      "  0.02898139 -0.00023645  0.01022229 -0.0416477  -0.0126502  -0.01016171\n",
      " -0.00107467  0.01249658  0.03444599  0.02115249  0.00667776  0.00838973\n",
      " -0.02859533 -0.03145471 -0.0177662  -0.02353625  0.01545889 -0.04050052\n",
      " -0.01652959  0.01395938  0.02215709 -0.01522101 -0.00358697 -0.03870251\n",
      " -0.00753423 -0.00040932  0.00409684 -0.02050398 -0.00493375 -0.00562514\n",
      "  0.00729826  0.00477497 -0.01606821  0.01488598  0.01700416  0.03025313\n",
      " -0.00610053 -0.03888965 -0.01825926 -0.01799217  0.02844253  0.00678828\n",
      "  0.01304939 -0.00621512  0.01339552 -0.01004773  0.00974979  0.00496902\n",
      " -0.01140363  0.00472685  0.0346031   0.03990968  0.00346561 -0.03036564\n",
      " -0.01915322  0.00535803 -0.01529077  0.01911976 -0.00846565 -0.00468592\n",
      " -0.03779417  0.00414798 -0.00490515  0.01010484 -0.03033449 -0.02699031\n",
      " -0.01882816  0.04003551 -0.03212586  0.00901235  0.00282891 -0.00249128\n",
      "  0.00015367  0.01102007 -0.04785511 -0.01137187 -0.02609263  0.02950352\n",
      "  0.03703518 -0.02395854 -0.0055551   0.00506467 -0.03593115  0.00960728\n",
      " -0.02946629  0.01672964 -0.00771692 -0.00109787  0.02069768  0.01867395\n",
      "  0.00396462 -0.02794392 -0.01585881  0.01920673  0.00737288 -0.0050064\n",
      " -0.01786799 -0.0168669   0.02452808  0.00693704  0.01428956 -0.02953456\n",
      "  0.02536909 -0.04006657  0.00378187 -0.02709977  0.02698422  0.00343959\n",
      " -0.00020279  0.00356138  0.02163258 -0.00388417  0.0066084   0.00128491\n",
      "  0.0457447   0.00733767  0.00265555  0.02238575  0.05551172 -0.00968896\n",
      "  0.03580506 -0.00346808 -0.01595602  0.02197069 -0.06549024  0.02463252\n",
      "  0.00208187 -0.01660299 -0.04214198 -0.03667077  0.00764934 -0.00263604\n",
      "  0.0096794   0.04199378 -0.00735524 -0.00534837  0.00594008 -0.00588468\n",
      "  0.00380073 -0.0443904   0.01181659 -0.02933465 -0.00818775 -0.01085651\n",
      "  0.00610099  0.01351402 -0.02018116  0.00514069 -0.0228498   0.0424024\n",
      " -0.02475136  0.01017892 -0.00969378 -0.0164426  -0.02251373  0.02462276\n",
      " -0.00483621  0.03526928  0.0104104  -0.02930798  0.01144614  0.00427229\n",
      " -0.06992392 -0.00191732 -0.03524777 -0.01802337 -0.01908062  0.03777955\n",
      " -0.03527075 -0.03230221 -0.00666887  0.00347917  0.0008822  -0.01189367\n",
      " -0.0325025  -0.01416903 -0.01316234  0.00306197 -0.03045093  0.01558543\n",
      " -0.00869981 -0.00695181  0.01349392  0.02988932 -0.01757322  0.01700994\n",
      "  0.01944354 -0.00245367  0.0307223   0.02073727  0.02096147 -0.01392196\n",
      " -0.01596121 -0.04577439  0.00476361  0.00998888 -0.02376307  0.02488095\n",
      "  0.02272651 -0.00158898 -0.00369441 -0.0257048   0.02338013  0.00484523\n",
      "  0.0012816  -0.00785055 -0.02479216 -0.01318125  0.02218435 -0.01405755\n",
      " -0.0104176  -0.00716822 -0.00481847 -0.02133683 -0.00867041 -0.00502301\n",
      "  0.00948102 -0.0266508  -0.00458126 -0.00371172 -0.01288464  0.02600091\n",
      "  0.02096523 -0.00590615  0.00509867  0.02406421 -0.00242753  0.01659152\n",
      " -0.03608411 -0.01284065 -0.0116448  -0.01256707 -0.04878364 -0.01288432\n",
      "  0.0352261   0.00445508 -0.00232619 -0.00813917  0.01541638  0.02680458\n",
      "  0.00366706 -0.03234783 -0.02634168  0.01956485  0.00781808  0.0030967\n",
      "  0.00991957 -0.01542943  0.00014284 -0.01956161 -0.00242141 -0.00797677\n",
      " -0.01624768 -0.00093043 -0.00027139 -0.04417939 -0.02528985 -0.00837333\n",
      " -0.04160208 -0.01953851  0.00756079  0.02059664  0.00288426 -0.00519408\n",
      " -0.04841235  0.01830531 -0.00979273  0.00826545 -0.0146766   0.05547203\n",
      "  0.02212695 -0.00379527  0.00039597 -0.00089169 -0.02297872  0.01343958\n",
      "  0.02485256 -0.01009917 -0.02130731  0.02291585  0.0258726  -0.03688727\n",
      " -0.0072338   0.02099315  0.01631079 -0.00356231 -0.00671816  0.02032307\n",
      " -0.02355778  0.00960263  0.00891368  0.00718891 -0.03156497  0.0176067\n",
      "  0.00317917 -0.01153054  0.02265119  0.00864645  0.02147733 -0.00409215\n",
      "  0.00570513  0.01513059  0.00515673 -0.00692301  0.00271768 -0.00114361\n",
      "  0.01487531  0.01822558 -0.00928125  0.00547042 -0.0346218   0.00148536\n",
      " -0.04687399 -0.02670471 -0.01491508 -0.00572652 -0.03383571 -0.01626782\n",
      "  0.02368425 -0.04148972 -0.03784103 -0.00569437  0.01484312  0.00446271\n",
      "  0.00763787  0.00995787  0.00015733  0.01756613  0.04744081 -0.0192919\n",
      "  0.02236026  0.02083952 -0.01284119  0.02750629 -0.02906125 -0.01068517]\n",
      "652850\n",
      "training: 34 testing: 23\n",
      "missed           21\n",
      "correctsource    13\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           15\n",
      "correctsource     8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6  0.8  0.6  0.6  0.6  0.4  0.75]\n",
      "Accuracy: 0.6176470588235294\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.38      0.43        13\n",
      "       missed       0.67      0.76      0.71        21\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        34\n",
      "    macro avg       0.58      0.57      0.57        34\n",
      " weighted avg       0.60      0.62      0.61        34\n",
      "\n",
      "accuracy = 0.4782608695652174\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.30      0.38      0.33         8\n",
      "       missed       0.62      0.53      0.57        15\n",
      "\n",
      "    micro avg       0.48      0.48      0.48        23\n",
      "    macro avg       0.46      0.45      0.45        23\n",
      " weighted avg       0.51      0.48      0.49        23\n",
      "\n",
      "(444,)\n",
      "[ 0.03493687  0.00881773  0.00890781  0.0202373   0.00621661 -0.00630472\n",
      "  0.01047286 -0.00394694 -0.01246728 -0.05050967 -0.01898597 -0.01136588\n",
      "  0.0118971  -0.002891    0.01130101  0.00479631  0.01928023  0.03729231\n",
      " -0.01072246 -0.03052098 -0.00817791 -0.00916321 -0.03552569 -0.02779735\n",
      "  0.02196493 -0.005068    0.01162004 -0.03227096  0.02824669  0.0191782\n",
      "  0.00946359  0.01922766 -0.02123119  0.00917522  0.01591162 -0.00856792\n",
      "  0.00563881 -0.00952732  0.01741745 -0.00503863  0.00618496 -0.00051918\n",
      "  0.01105694  0.00560454 -0.02626889  0.00789282  0.00469212 -0.00905904\n",
      " -0.0220183   0.00506181 -0.00099187 -0.0468694  -0.05483193 -0.01151878\n",
      " -0.00661265 -0.00428631  0.0114317  -0.02529275 -0.00132546 -0.00907265\n",
      "  0.01743086 -0.00780339 -0.01219954  0.01382873  0.00115692  0.00056645\n",
      "  0.00208826  0.00636335 -0.04341546 -0.01368474  0.01894891 -0.03484407\n",
      " -0.01239026  0.02743033  0.0308996   0.00187587 -0.02385386 -0.009919\n",
      "  0.00410168 -0.00078649  0.02071941  0.01643973 -0.00037387  0.00477466\n",
      "  0.01441038 -0.02222433  0.00461531 -0.00672289  0.01871323  0.02130269\n",
      "  0.01730218 -0.0038256  -0.00971347 -0.0065359   0.00749445 -0.00462717\n",
      "  0.00041701  0.01691842 -0.01437594  0.00085063 -0.00931196  0.01433879\n",
      "  0.00668718  0.00254724  0.02083894  0.03889894  0.02516153 -0.00095077\n",
      "  0.01806639 -0.02048185  0.00097297 -0.01039359  0.0229205   0.00322662\n",
      "  0.00699175  0.0174177   0.01514811 -0.01070694  0.00198295 -0.00790204\n",
      " -0.0238334   0.01654591  0.01466134  0.03785113 -0.04018853 -0.00989249\n",
      "  0.02679253  0.00180028  0.01456732 -0.02562484 -0.01877387  0.01543102\n",
      " -0.02414699  0.01008842  0.00536956 -0.02979245 -0.01056325 -0.00927659\n",
      " -0.05053858  0.01954246  0.00987522  0.02081571  0.00175048 -0.03595217\n",
      "  0.01830611 -0.0045975   0.00134415 -0.01563866  0.00422353  0.02656233\n",
      "  0.02238775  0.00724745  0.01901074 -0.01122636  0.02565905 -0.04707602\n",
      "  0.02949746  0.01627566 -0.00414572  0.01840943 -0.01369233 -0.01568533\n",
      "  0.01220933 -0.02792735  0.00671888  0.00612674 -0.0207589  -0.01259744\n",
      " -0.01893484  0.02528632 -0.00925031  0.00533729  0.0139826  -0.01387456\n",
      "  0.01456306  0.0129304  -0.01020253 -0.01742288  0.02395612  0.0178091\n",
      "  0.0197124   0.03226596  0.00622302 -0.01750527  0.00065947 -0.03461853\n",
      "  0.0134952  -0.00982073 -0.01185621 -0.00588231  0.01477425  0.01618596\n",
      " -0.01483871 -0.02224491 -0.00441281  0.00057924 -0.008143   -0.01492354\n",
      " -0.03413623 -0.00394366 -0.01944189 -0.01358181  0.00269758 -0.01125603\n",
      " -0.02373575 -0.03057646  0.01402362  0.01421678 -0.03917825  0.00550481\n",
      " -0.02947255  0.01622921  0.00081171  0.0053078   0.02569124  0.00024316\n",
      "  0.00086745  0.01715797  0.00048084 -0.00387242 -0.02036157  0.01356878\n",
      "  0.03308568  0.01486363  0.0185402   0.02513147 -0.00804407  0.00051249\n",
      "  0.0119903  -0.00750095  0.00303894  0.02721285  0.01024859 -0.0095606\n",
      " -0.0148618   0.03826213 -0.00418701  0.00024635 -0.00423307  0.01772079\n",
      "  0.00409794 -0.0312421   0.02681151  0.0017967  -0.00511324  0.00843678\n",
      "  0.0191092   0.00037426  0.00619456  0.00994623  0.00430312  0.01386056\n",
      "  0.01684351 -0.00451599 -0.00260181 -0.01182956 -0.00351597 -0.00013341\n",
      "  0.02696121  0.00579619 -0.02246054 -0.02166965  0.00634117 -0.01523063\n",
      " -0.01596973 -0.03543459 -0.05294756 -0.04208729 -0.0023642  -0.06290971\n",
      " -0.02436266  0.00937892 -0.01024168  0.02775161 -0.00658415 -0.02112513\n",
      " -0.01761759  0.01167285 -0.01225141 -0.02631909 -0.01383535 -0.02189365\n",
      " -0.03720708  0.00836663  0.00822193  0.03441257  0.02215294  0.00860851\n",
      " -0.00207139 -0.00847537 -0.01030683  0.00995285 -0.01273507  0.02765667\n",
      " -0.0334433   0.02914957  0.00807706 -0.01008148 -0.00835855  0.00450487\n",
      " -0.00314006 -0.02230789 -0.00366999  0.01991853  0.00357089  0.03039006\n",
      "  0.00020199 -0.01363248 -0.00188106 -0.00090405 -0.02465861  0.02433977\n",
      "  0.00813726  0.02073975 -0.01598292 -0.01957489  0.00553846  0.02670542\n",
      " -0.0074119   0.02208263 -0.00981776  0.01941775  0.02963955  0.01377405\n",
      " -0.00614355  0.00930667 -0.00346629  0.00243624  0.02044443  0.01098787\n",
      "  0.01466075 -0.0022823  -0.00511749  0.01976639  0.0144314   0.03656996\n",
      " -0.01045291  0.00299374 -0.01907009  0.00887146 -0.02783316  0.01731346\n",
      " -0.05715646 -0.04545485 -0.00446341  0.00592229  0.03663007  0.00564169\n",
      "  0.0138807  -0.00405211  0.02881711  0.01761806 -0.02389654 -0.01775623\n",
      "  0.0274667  -0.001304   -0.01341326 -0.02411812  0.00232467 -0.01234953\n",
      "  0.01426814  0.02883466 -0.00747011  0.0189302   0.02627617 -0.01262296\n",
      "  0.00934939  0.02758606 -0.02228979 -0.0127504  -0.0006259  -0.00711684\n",
      "  0.00022925  0.00762727 -0.00068411  0.00414366  0.01111925  0.00222731\n",
      "  0.01238801 -0.00166271  0.01059467 -0.04606374  0.00084201 -0.00352578\n",
      " -0.00218462 -0.00061586 -0.02636946  0.00273487 -0.00468855  0.03309109\n",
      " -0.04111161 -0.02416579 -0.00682201 -0.03209258  0.0061173   0.02620811\n",
      "  0.02080562 -0.00427563  0.03244676 -0.00668123 -0.00061819  0.01077299\n",
      "  0.00494486  0.00633297 -0.00194529 -0.00572909  0.04565481  0.00269913\n",
      " -0.02201211  0.00393    -0.00536187 -0.00515551 -0.00059934  0.02562125\n",
      " -0.001155    0.00504806  0.01729297  0.00911732 -0.0085253   0.00953375\n",
      "  0.01792767  0.00028426  0.02846139 -0.0194969  -0.00640931 -0.01823762\n",
      " -0.01457447 -0.01957687 -0.00404567  0.018479   -0.01553464  0.01245751\n",
      "  0.00402841 -0.01150635 -0.02051674  0.00172349  0.00843454  0.00019721\n",
      "  0.01898827  0.00558728  0.0188193   0.00419718  0.0059452   0.01402025]\n",
      "677561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 32 testing: 22\n",
      "correctsource    20\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.4        0.6        0.8        0.6        0.8        0.25\n",
      " 0.66666667]\n",
      "Accuracy: 0.59375\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.64      0.80      0.71        20\n",
      "       missed       0.43      0.25      0.32        12\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        32\n",
      "    macro avg       0.53      0.53      0.51        32\n",
      " weighted avg       0.56      0.59      0.56        32\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.59      0.71      0.65        14\n",
      "       missed       0.20      0.12      0.15         8\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        22\n",
      "    macro avg       0.39      0.42      0.40        22\n",
      " weighted avg       0.45      0.50      0.47        22\n",
      "\n",
      "(444,)\n",
      "[ 0.01033037 -0.01233449 -0.01056056  0.04030922  0.01488293 -0.01566279\n",
      "  0.0322483   0.00717951 -0.00769463 -0.00114474  0.0029741   0.00065865\n",
      "  0.0181677  -0.01880389 -0.02051223  0.00100904 -0.02018253  0.01298053\n",
      " -0.01624281 -0.01778988  0.01615383  0.00028216 -0.00451844 -0.00179818\n",
      "  0.01651326  0.0208328  -0.03435601  0.00444164 -0.00920349  0.00243494\n",
      "  0.0048954  -0.01872831  0.00721369  0.01544496  0.0167491  -0.02878563\n",
      "  0.00292491  0.01652949  0.00700799 -0.0100723  -0.00729434  0.01424039\n",
      " -0.011031    0.01544672 -0.01950684  0.00258146 -0.00649224  0.00164491\n",
      "  0.05228144 -0.00369718 -0.01393657  0.00662767  0.01233336  0.00043785\n",
      "  0.01429011 -0.03686844 -0.01142028  0.02146383 -0.03073616 -0.01860732\n",
      " -0.01086042 -0.00309968  0.03719891  0.00226693  0.00267568 -0.00345109\n",
      " -0.01690992 -0.00058036  0.01131776 -0.03558423 -0.0170942  -0.01360726\n",
      "  0.0011764   0.01221264 -0.01174055  0.03801371 -0.01125315 -0.03701692\n",
      " -0.01619459  0.00246813 -0.00246897 -0.01090689  0.00206027 -0.02779292\n",
      "  0.00238067 -0.03480178 -0.01533915  0.0074596  -0.03079676 -0.00707594\n",
      " -0.00802137 -0.01217385 -0.01349374 -0.01632782 -0.00279955 -0.00471602\n",
      " -0.03101693 -0.00958049 -0.0288156  -0.0062155  -0.02436976  0.02063037\n",
      "  0.01696278 -0.02403824  0.00510911 -0.00501024 -0.0276165  -0.0258655\n",
      "  0.02996427 -0.00152927 -0.00267779 -0.0270293  -0.00177429  0.0199351\n",
      "  0.01723068  0.00613369 -0.00549472 -0.00587601  0.05800965 -0.01513845\n",
      " -0.01232733  0.04546257  0.00590343  0.03881682  0.00846286  0.00200939\n",
      " -0.01889261 -0.0017557  -0.03066535 -0.0172402  -0.00152621  0.0050688\n",
      " -0.0211604   0.00451072 -0.0118116  -0.01042628 -0.00106156 -0.00270245\n",
      " -0.00333096 -0.00797568 -0.04501916  0.00416928  0.01582071 -0.00049382\n",
      "  0.00969805  0.05453158  0.00252853 -0.00311461 -0.02105246  0.01120649\n",
      " -0.04536958 -0.02155858  0.0354108   0.01077241 -0.00978288 -0.01204764\n",
      "  0.00582189 -0.03440579  0.02912566  0.00979362 -0.02169299  0.0104567\n",
      " -0.01135112 -0.03155671 -0.0026777   0.01175294 -0.05092034 -0.03076676\n",
      "  0.00903856  0.01865196  0.00491381  0.00514204 -0.01047928 -0.0373866\n",
      "  0.01420667  0.00334925  0.0124831   0.01510052  0.03751844  0.00616718\n",
      "  0.00938562 -0.0113117  -0.00795298 -0.00667093 -0.01079081  0.0205987\n",
      "  0.00545154  0.02889527 -0.01452525  0.00862081 -0.0225651  -0.00406055\n",
      "  0.00297134  0.01601933 -0.01819226 -0.03220237  0.00241628  0.00814527\n",
      "  0.00123262 -0.0060974  -0.01291831 -0.01016324 -0.01208147 -0.02499233\n",
      " -0.00535799 -0.01918545  0.02357266 -0.03387468  0.00043298 -0.00924338\n",
      "  0.02169687 -0.01316371  0.00637074 -0.00025256 -0.00709357  0.01983391\n",
      "  0.01801991 -0.00031971 -0.00112621 -0.03509186 -0.00016257 -0.01528783\n",
      "  0.04946429  0.02214549  0.00909627  0.01235936  0.01536253  0.0171165\n",
      " -0.0131265   0.02924708 -0.00928464 -0.00014243 -0.0024691   0.00250317\n",
      "  0.00839018  0.01826809  0.00031181 -0.0368232  -0.03039135 -0.00111243\n",
      " -0.02906841  0.00660609  0.01449474  0.03788916  0.01175887 -0.03004908\n",
      "  0.0156515  -0.01643817 -0.00204709 -0.01293053 -0.01354616 -0.00959285\n",
      "  0.00377391 -0.01574945 -0.00840078  0.01650936  0.0265851  -0.01088685\n",
      "  0.01932042 -0.03066182 -0.00028356 -0.01887486 -0.00630652  0.00589539\n",
      "  0.03378012 -0.03544928 -0.00669453  0.01019411  0.0004991  -0.01248235\n",
      " -0.01767442  0.00784817 -0.03895617 -0.00669093 -0.03324746 -0.00102246\n",
      "  0.01566627  0.01980354  0.00675999  0.01915332 -0.00754191  0.00565441\n",
      " -0.02017488 -0.03928835  0.00237331  0.01095109  0.01773255  0.00144717\n",
      " -0.02149769  0.01648412 -0.00592698 -0.015139    0.00416957  0.00800745\n",
      " -0.01029869 -0.0257667   0.00901155  0.03099518 -0.00904868  0.03029248\n",
      "  0.02357079 -0.00869759 -0.00552325  0.01636648 -0.04469617  0.0072459\n",
      " -0.01278404  0.00268554 -0.01031347 -0.01331926  0.04425596 -0.00756654\n",
      "  0.01667124  0.01172787  0.00315972 -0.03024544  0.03613403  0.00614431\n",
      "  0.02550435  0.00212708 -0.00989793  0.00575765  0.02208089  0.01925756\n",
      " -0.00863239  0.024669   -0.0049841  -0.02058287 -0.00116152  0.00341875\n",
      "  0.00186873 -0.01708014  0.02669886  0.01394999  0.02340281 -0.01025858\n",
      "  0.02629603 -0.01922323 -0.01806917 -0.00237326  0.00591925  0.02191674\n",
      " -0.01338384 -0.02370628  0.05443584 -0.00115773  0.019396   -0.01877121\n",
      "  0.00757447  0.02335516  0.00625185 -0.00143653 -0.02095667  0.04476544\n",
      " -0.01110586 -0.00646141  0.00985723 -0.00682671 -0.02882663 -0.00425603\n",
      "  0.00767862  0.02157589  0.01262267 -0.01374849 -0.02279423  0.01632741\n",
      "  0.00564391 -0.02566829 -0.04379594  0.05289587  0.00966344  0.02671143\n",
      "  0.0065158   0.01317962  0.015614    0.0508435  -0.00521331  0.01159967\n",
      " -0.00463181 -0.02611848  0.01294814 -0.00691737  0.01242438 -0.00040494\n",
      " -0.02840011  0.00394588 -0.02001486 -0.00368115 -0.00607637 -0.03436943\n",
      " -0.01543788  0.03010555  0.01188376 -0.00332662 -0.02522922  0.01123619\n",
      " -0.01232584 -0.00305258 -0.0152886  -0.02131815 -0.02266048  0.01010047\n",
      "  0.03082247  0.03243284  0.00076549  0.0157395  -0.00563818  0.0123152\n",
      " -0.00858674  0.00517835  0.01238894  0.01795997 -0.01006993  0.01577228\n",
      "  0.00521293 -0.01195038  0.01087726 -0.01114421  0.00362703 -0.0117403\n",
      "  0.00616084 -0.01634014 -0.0162097   0.0150238   0.00440307 -0.00402476\n",
      " -0.03338129 -0.00890839 -0.01083713 -0.02564377  0.021885   -0.00484562\n",
      "  0.00733725  0.0052331  -0.01154617 -0.03310242  0.01656066 -0.01733557\n",
      " -0.00407677  0.0070266  -0.03168647  0.01681407 -0.00831313 -0.00552918]\n",
      "711830\n",
      "training: 36 testing: 25\n",
      "missed           21\n",
      "correctsource    15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           14\n",
      "correctsource    11\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.6        0.2        0.         0.6        0.4\n",
      " 0.4       ]\n",
      "Accuracy: 0.3611111111111111\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.21      0.20      0.21        15\n",
      "       missed       0.45      0.48      0.47        21\n",
      "\n",
      "    micro avg       0.36      0.36      0.36        36\n",
      "    macro avg       0.33      0.34      0.34        36\n",
      " weighted avg       0.35      0.36      0.36        36\n",
      "\n",
      "accuracy = 0.48\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.40      0.36      0.38        11\n",
      "       missed       0.53      0.57      0.55        14\n",
      "\n",
      "    micro avg       0.48      0.48      0.48        25\n",
      "    macro avg       0.47      0.47      0.47        25\n",
      " weighted avg       0.47      0.48      0.48        25\n",
      "\n",
      "(444,)\n",
      "[-2.41439937e-02  3.14416014e-02 -2.84236501e-03 -1.02776514e-02\n",
      " -1.86047172e-02  2.65199735e-02  5.32099821e-02  2.10939246e-02\n",
      " -4.91757256e-03  3.75492245e-02 -4.01815825e-02  2.53980434e-02\n",
      " -2.15000708e-02  3.44577381e-02 -2.60629043e-02 -1.85172059e-02\n",
      " -2.52574234e-02 -4.80082374e-02 -3.19922763e-02  4.65980029e-02\n",
      "  1.91906543e-02  2.16203570e-02 -1.58519073e-02  1.56800167e-02\n",
      " -2.44283423e-02 -3.09464270e-02 -5.25629293e-02  2.54839115e-02\n",
      " -5.66939959e-02 -2.56388685e-04 -1.35837250e-02  9.38592685e-03\n",
      "  2.95742674e-02 -1.04465232e-02  2.18539972e-02 -1.13031994e-02\n",
      "  1.03374618e-02 -1.67642341e-02 -2.21903010e-02 -1.32366552e-02\n",
      " -1.33109867e-02 -3.79201393e-02  1.33953978e-03  6.56142144e-02\n",
      "  2.26389190e-02 -1.19947774e-02  5.43364183e-02 -5.17346605e-02\n",
      " -2.05681711e-02  8.82264904e-03 -2.99430408e-02  3.59278041e-02\n",
      " -1.84726168e-02  1.40988564e-02 -2.91920307e-02 -3.59887134e-03\n",
      " -1.37801023e-02  2.31902454e-02 -4.93669503e-03 -5.87082190e-02\n",
      "  3.37418067e-02 -9.77050298e-04 -4.33480526e-02  2.46962361e-03\n",
      " -4.07873003e-02 -1.32015084e-02 -4.53906161e-02  3.08470925e-02\n",
      "  3.10622612e-02 -1.69767252e-03 -3.11368531e-02 -8.36222304e-03\n",
      " -7.38125395e-03  8.77809622e-04  9.16257564e-03 -1.48586734e-03\n",
      "  1.45545215e-03  1.55604711e-02  7.10054045e-03  1.00704306e-02\n",
      " -8.12903664e-03 -1.95207406e-02  1.52842916e-02  3.78822047e-03\n",
      " -1.51629696e-02  4.38789274e-02  1.15741619e-03  1.93086289e-02\n",
      " -3.09874432e-02 -3.28034660e-02  2.32755645e-03 -8.09824254e-03\n",
      "  9.11879697e-03 -4.02968850e-03  1.89186261e-03  1.51684004e-02\n",
      " -1.41853722e-02 -4.40610032e-02  4.18355923e-02 -3.87659153e-02\n",
      " -1.55791641e-02 -7.24957742e-02  3.46759681e-02  2.51748970e-03\n",
      " -2.00560972e-02 -9.52716564e-04  3.04259828e-02  1.58067484e-02\n",
      "  2.27534528e-02  1.64012487e-02 -3.36014051e-02  1.90676227e-04\n",
      " -4.85477330e-04 -3.53023376e-02 -3.67025349e-02 -6.76602180e-03\n",
      "  2.83947160e-02 -5.73150119e-02 -1.10066100e-02  1.01737363e-02\n",
      "  2.67086380e-02 -5.46542356e-02 -8.45914211e-03  3.64578985e-02\n",
      "  2.07343682e-02 -4.17036882e-02  1.32542845e-02 -6.40252479e-02\n",
      "  2.53123515e-02  2.49128487e-03  1.85946845e-02 -1.62192853e-02\n",
      "  1.79040950e-02 -5.46337020e-03 -2.90166950e-02 -1.04491063e-02\n",
      " -1.75452780e-02 -3.25101770e-02  5.28408480e-02  3.86102999e-03\n",
      "  5.67022056e-03 -2.69725147e-02 -5.29205814e-02  3.51724135e-02\n",
      " -7.65217770e-04 -1.93333606e-02  4.17496520e-03 -2.72894878e-02\n",
      "  5.57440936e-02  8.74144507e-03 -5.25535245e-03 -1.35970071e-02\n",
      "  3.12326340e-02 -1.08243241e-02  1.21019899e-02 -2.86711443e-03\n",
      " -2.10360573e-02  8.71483092e-03  3.53943539e-03 -8.30730963e-03\n",
      "  3.69032172e-02  6.90855933e-03  2.02705794e-02  2.05573951e-02\n",
      "  3.65828973e-02 -2.64038486e-03 -2.02557137e-02 -1.04351879e-02\n",
      " -1.91493717e-02 -3.95417272e-02  4.35884069e-02 -8.08922784e-03\n",
      "  1.27247324e-02 -1.91610550e-02  4.03951295e-02 -3.83867711e-02\n",
      " -2.46512368e-02  7.93082401e-03  1.19605836e-02 -3.62507346e-02\n",
      "  4.03981400e-02  2.05145496e-02 -4.38312827e-03 -4.98921783e-02\n",
      " -7.55983353e-03 -2.16888196e-02  4.62150971e-03  1.31629040e-02\n",
      "  8.87280418e-03  3.41252328e-02  2.29092989e-02  1.55142826e-02\n",
      "  2.07773750e-02 -2.67133310e-03  2.04969222e-02 -1.92044848e-02\n",
      " -1.40996791e-02  9.76079803e-03 -1.39976710e-02 -1.38853799e-02\n",
      "  4.85208590e-02  1.28876242e-02 -6.69888319e-03  9.02056636e-03\n",
      " -9.91589657e-03  5.27218478e-03 -2.19542791e-02 -3.37519702e-03\n",
      " -5.60852386e-02 -6.47812989e-03  1.69686878e-02  1.61929893e-02\n",
      " -1.85901557e-02  1.54237090e-02  1.67550633e-02  2.65003056e-02\n",
      " -1.32829308e-03  2.07191279e-02  7.77304075e-03  4.90471653e-02\n",
      "  3.29739031e-02 -1.87655378e-03 -2.28786616e-02  9.52046751e-03\n",
      " -2.12135364e-02  1.08104031e-02 -3.23670846e-02  1.98304358e-02\n",
      " -1.80037190e-02  3.86807953e-03  1.88058625e-02 -1.79069737e-02\n",
      "  5.46032799e-02  1.35384906e-02  6.96392269e-03 -3.54514355e-02\n",
      "  1.13418868e-02  1.42394579e-02  1.93747207e-02  7.11907087e-03\n",
      " -6.77298141e-03  3.40922700e-02  8.64308278e-03 -2.39064631e-02\n",
      " -1.98864401e-02 -8.90530535e-03  5.33710459e-02 -1.85718304e-02\n",
      " -8.81691338e-03  1.06575954e-03 -3.51722577e-02  1.37501785e-02\n",
      " -2.66477035e-02 -1.58731682e-02 -8.76212241e-03 -1.37062368e-02\n",
      "  1.59627357e-02  1.16167826e-03  2.01044584e-02 -1.52754390e-02\n",
      "  2.18970112e-03  1.91470037e-02 -3.81677626e-03  8.05993278e-03\n",
      "  6.58140157e-03 -2.55117377e-02 -1.56273863e-02  6.26290075e-02\n",
      " -1.26770387e-02  2.49791897e-02  3.29666896e-03  1.06145153e-02\n",
      "  1.04474538e-02  1.23481141e-02  3.49833833e-02  1.29151843e-02\n",
      "  1.53724073e-02 -6.16038540e-02  1.09851909e-02  1.08187091e-02\n",
      " -1.13702361e-02  3.85463864e-03 -4.32700731e-02 -1.29728236e-02\n",
      "  1.44301967e-03 -2.34664633e-02  4.93485509e-02 -3.55679574e-02\n",
      " -2.56212316e-02  3.77700346e-02 -4.21174808e-02 -2.14988172e-02\n",
      "  2.86274613e-02  3.67223915e-02  5.23247387e-04 -4.61014783e-02\n",
      " -1.45018869e-02  3.35114392e-02 -1.29714781e-02  2.43411445e-02\n",
      " -5.48693272e-02  1.27902006e-02  6.29739468e-03 -3.66749963e-02\n",
      " -2.63620963e-02 -8.29951858e-03 -1.89025718e-02 -5.07255247e-02\n",
      "  1.53069260e-02  3.64968027e-02 -2.64159160e-02  4.24460192e-03\n",
      " -3.88281785e-02 -7.67854733e-03 -3.45038868e-03  2.80254691e-02\n",
      " -2.25747249e-02  3.36656787e-02  1.73406352e-02  3.61876051e-02\n",
      "  3.51762346e-02 -3.53291964e-02 -2.25060281e-03  1.70786841e-02\n",
      "  1.16835761e-02 -5.26846394e-02 -8.87264270e-03 -1.29494943e-02\n",
      " -2.94125696e-02 -6.53424563e-02  7.98754775e-03 -1.28155262e-02\n",
      "  3.36210695e-02  5.67699923e-03 -1.43547380e-02  3.99483326e-04\n",
      "  2.20400368e-02 -2.80630207e-02  2.25901545e-02 -5.72439002e-03\n",
      " -5.39804202e-02 -4.67666597e-03 -1.86660214e-02  1.95385783e-02\n",
      " -1.63368286e-02 -2.33781967e-02 -2.55298452e-02 -5.39391384e-03\n",
      " -2.66996633e-02  2.28954476e-02  3.46671881e-02 -2.22972332e-03\n",
      " -2.88570978e-03 -2.37092204e-02 -1.67298185e-02  5.26017452e-02\n",
      "  6.67332614e-03  3.87574437e-03 -7.04384586e-03  4.84269821e-02\n",
      " -1.95844513e-02  1.98706279e-02 -4.14305932e-02 -5.54909178e-03\n",
      "  5.46445541e-02 -4.72792111e-03  7.74639001e-03  2.04278109e-02\n",
      " -1.39988574e-02 -4.25555962e-02  5.64691833e-03  4.65378977e-02\n",
      " -1.54624517e-02 -7.17809066e-04 -2.15441835e-02  1.26019304e-02\n",
      " -5.09232081e-02  1.26766860e-02 -3.94351216e-02  6.59469667e-05\n",
      "  1.86094094e-02  4.13680996e-02 -5.57214204e-03 -7.13363104e-03\n",
      " -2.35959191e-02 -2.80663730e-02  1.05253420e-02  4.37767660e-03\n",
      "  4.68722107e-02  1.09402151e-02 -5.93010389e-02  2.64954559e-02\n",
      "  2.32768655e-02 -2.63559491e-02  6.21236499e-03  5.50774609e-02\n",
      " -4.11838721e-02  3.59245095e-02  4.88951929e-03  1.83764177e-02\n",
      " -3.76740009e-03  5.64061368e-03  4.52853944e-02 -2.96555534e-02\n",
      "  1.37771117e-02  2.22003219e-05 -6.55176019e-03  5.34318522e-03\n",
      "  1.85001512e-03 -1.25394702e-02  2.88363850e-03 -2.29880296e-02\n",
      " -1.62045275e-02  2.29962962e-02  2.38942539e-02  3.65879716e-03\n",
      "  1.52201778e-02 -4.10292466e-03 -9.26753775e-03 -3.35387216e-02\n",
      "  5.34134553e-04  1.27609884e-02  2.08062398e-02  5.48837451e-02\n",
      "  4.12116961e-02 -2.16554378e-02  2.91610810e-02 -2.30571638e-02\n",
      " -2.64503967e-02 -2.94926237e-02 -3.20941610e-03  1.80829027e-02\n",
      " -3.28948060e-02 -1.79630664e-02  5.30892344e-03 -1.59986253e-02\n",
      "  1.02832910e-02 -1.35037444e-02  5.89049880e-02  3.86712961e-03\n",
      " -1.83872811e-02 -3.44979860e-03 -2.79959907e-02 -2.12944456e-02]\n",
      "729722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 40 testing: 28\n",
      "correctsource    26\n",
      "missed           14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    18\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.33333333 0.83333333 0.5        0.83333333 0.6\n",
      " 0.6       ]\n",
      "Accuracy: 0.575\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.69      0.68        26\n",
      "       missed       0.38      0.36      0.37        14\n",
      "\n",
      "    micro avg       0.57      0.57      0.57        40\n",
      "    macro avg       0.53      0.52      0.52        40\n",
      " weighted avg       0.57      0.57      0.57        40\n",
      "\n",
      "accuracy = 0.6785714285714286\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.71      0.83      0.77        18\n",
      "       missed       0.57      0.40      0.47        10\n",
      "\n",
      "    micro avg       0.68      0.68      0.68        28\n",
      "    macro avg       0.64      0.62      0.62        28\n",
      " weighted avg       0.66      0.68      0.66        28\n",
      "\n",
      "(444,)\n",
      "[ 0.01069384  0.00059326 -0.00169663  0.0025676  -0.00082564 -0.02334886\n",
      " -0.02922482 -0.04250642  0.01547817  0.01479566 -0.00760437 -0.00975825\n",
      " -0.0033668  -0.00920963 -0.00609082  0.00755182 -0.00180797  0.01692575\n",
      "  0.01991054 -0.0052721  -0.03764331 -0.0133547   0.00464343 -0.02858093\n",
      " -0.01198972  0.01266714  0.00243991 -0.0055644   0.02028881 -0.02220365\n",
      "  0.01463825  0.02445582  0.00655386 -0.0378799   0.02695242 -0.01911545\n",
      " -0.03154722 -0.00796987 -0.03463176 -0.01221211  0.01726382  0.02756553\n",
      "  0.00188983  0.02937342 -0.03518886 -0.02406827  0.01656295 -0.01063171\n",
      "  0.00308821 -0.00585478 -0.02592516  0.00524295  0.00440799 -0.01047307\n",
      " -0.00161184  0.0092384   0.00355253  0.03664346 -0.00011337  0.01937928\n",
      "  0.01397142  0.02534098 -0.01572705  0.0025315  -0.00122825  0.00671834\n",
      "  0.01266513 -0.01292473  0.00281808 -0.01425171  0.00162182  0.0150796\n",
      "  0.00061643  0.0196495  -0.03442741 -0.00573163 -0.04036812 -0.04380357\n",
      " -0.02686856  0.00101258 -0.01940912 -0.02831112 -0.01103403  0.01316231\n",
      "  0.02408545  0.00724313  0.00058664  0.00294264  0.01429552 -0.02641718\n",
      " -0.01695388 -0.01704824  0.00646551 -0.02084133 -0.01775489 -0.00125712\n",
      " -0.01864335  0.01620577  0.01136414  0.02335464  0.02580943 -0.03271205\n",
      " -0.03323784  0.00411037 -0.03418815  0.01788028  0.02651997 -0.00538711\n",
      " -0.00496577 -0.00889482  0.01341034 -0.00640115 -0.0164576   0.02663386\n",
      " -0.00743874  0.01415872  0.00643629 -0.00187216 -0.01632649  0.02089636\n",
      " -0.01077077  0.04860538 -0.02570557 -0.03800489  0.00793787  0.00363393\n",
      "  0.0176408  -0.01073083 -0.02654911  0.01338428  0.02398868  0.04937617\n",
      "  0.00535647  0.01666088  0.0189208  -0.00867589  0.02770346  0.01269032\n",
      "  0.01569707  0.01199095  0.0172844   0.03183642  0.03388498  0.00281482\n",
      "  0.00222007 -0.01310299  0.05729344 -0.002916   -0.0187112   0.01672362\n",
      "  0.02048096 -0.04868919 -0.01294376 -0.01282284  0.09368824 -0.0308321\n",
      "  0.01616022 -0.00420684 -0.02242816 -0.01628865  0.02557689 -0.00104789\n",
      " -0.00382685 -0.0143108   0.00841807 -0.01706878 -0.0040163  -0.03198863\n",
      "  0.0286742   0.01283196 -0.03992128 -0.02310512 -0.03076602  0.00551825\n",
      "  0.01324158  0.01701967 -0.01071777 -0.00894328  0.01481843 -0.02223362\n",
      "  0.02467072  0.00756596 -0.02346139  0.0380445   0.01210617  0.00616791\n",
      " -0.02590566 -0.03532953 -0.01935125 -0.03116695  0.01697516 -0.0051763\n",
      " -0.02527081 -0.00621595  0.02514978 -0.0059228  -0.0067342  -0.0035703\n",
      " -0.00497155 -0.00652936  0.03523912  0.02822425  0.0298991   0.05873202\n",
      "  0.01541502  0.02742602 -0.03555364  0.00668413 -0.00202337 -0.03993613\n",
      " -0.05848835  0.00032962 -0.00637366 -0.03551349 -0.00174134  0.04087623\n",
      " -0.05844099 -0.03231674 -0.02507281  0.02253633 -0.00024929 -0.00891048\n",
      " -0.02251506 -0.0031226   0.02086917  0.0012282   0.02111688 -0.02977032\n",
      "  0.04091397 -0.0060952   0.01190386 -0.02576014  0.03717615  0.0373968\n",
      " -0.01686735 -0.0288267   0.02384259 -0.01578275  0.04586599 -0.00994549\n",
      " -0.00828155 -0.01487093  0.00029527  0.00835381 -0.01315928  0.01980338\n",
      "  0.02578441  0.03086984  0.01116876  0.0333655  -0.01774303  0.00460588\n",
      " -0.00620951  0.02381988 -0.00710569  0.0020318   0.02588673 -0.02755818\n",
      " -0.00124516  0.00141252 -0.01530196  0.00676034  0.00479266  0.02546349\n",
      " -0.00911036  0.00572256 -0.03114699 -0.009278   -0.029738   -0.02828703\n",
      "  0.0032144  -0.00592711  0.00339474  0.03019883 -0.00314233 -0.03669245\n",
      "  0.010329   -0.00474611 -0.02930012 -0.00106161 -0.00771179 -0.017161\n",
      " -0.01695427  0.00159586 -0.01905594 -0.02477096  0.00217983 -0.02679579\n",
      " -0.00823702 -0.01255918 -0.02874205  0.0214738  -0.00884129  0.02716255\n",
      " -0.02900691  0.0256535  -0.01248665 -0.02102334 -0.02439412 -0.02185925\n",
      "  0.05725888  0.00960765  0.00375407  0.01810819 -0.00551609  0.01132872\n",
      "  0.03567579  0.03128342 -0.0400838   0.02885635 -0.00417705 -0.0238897\n",
      " -0.00507534 -0.00876935 -0.0297661   0.00818865 -0.03330272 -0.00099041\n",
      "  0.02430591  0.03164647 -0.01222342 -0.01290473 -0.00785509 -0.00265645\n",
      " -0.02739113  0.00115665  0.01651607  0.00089292 -0.01651783  0.00933586\n",
      "  0.03707248  0.02635017  0.00167663 -0.02184354 -0.02546524 -0.05496904\n",
      " -0.01949051  0.00869391 -0.02072069 -0.00023282 -0.01478639 -0.00200985\n",
      "  0.02153707  0.02189433  0.04063491 -0.02930558 -0.02198362  0.0041134\n",
      " -0.01855289 -0.04631268 -0.01194987 -0.008706   -0.00452815  0.00928763\n",
      " -0.03709879 -0.01122214 -0.00906226 -0.01674397  0.01056932 -0.03618969\n",
      "  0.00446635 -0.00974909 -0.03208217  0.00408856 -0.01692567  0.00820663\n",
      "  0.01964978  0.01492054 -0.00240419  0.00430356  0.00321342 -0.02131861\n",
      " -0.03123255  0.02461215 -0.01211258 -0.01288661  0.00940627 -0.02384593\n",
      "  0.00512101  0.01155049  0.05513155  0.0003601  -0.00324202 -0.00351227\n",
      " -0.01720526 -0.03872028  0.0089465  -0.03358121 -0.00627681  0.01629888\n",
      " -0.00944955 -0.01244326  0.00163009  0.0034818  -0.00939309  0.01316362\n",
      "  0.04367613 -0.00518441 -0.02429145 -0.00913681  0.00137435 -0.01856862\n",
      "  0.03176215  0.00717637  0.00768675 -0.03659466  0.00928827 -0.01721272\n",
      "  0.00920093 -0.02324644 -0.03810172  0.02062942  0.04006016 -0.01179122\n",
      " -0.00677362  0.0066306   0.00109819 -0.00822366 -0.02956992  0.00795122\n",
      " -0.00176261 -0.01567133  0.00122396 -0.0062441  -0.02030516 -0.00304468\n",
      "  0.01764224 -0.00543145  0.01714555  0.02923123  0.01708582  0.01309927\n",
      "  0.01609881 -0.01305565  0.00453066 -0.02332185 -0.0168146   0.00806719\n",
      "  0.06022201 -0.01167346  0.01634584 -0.01161579  0.0093976   0.00466811]\n",
      "739694\n",
      "training: 32 testing: 22\n",
      "correctsource    20\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6  0.6  0.6  0.6  0.6  0.25 1.  ]\n",
      "Accuracy: 0.59375\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.67      0.70      0.68        20\n",
      "       missed       0.45      0.42      0.43        12\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        32\n",
      "    macro avg       0.56      0.56      0.56        32\n",
      " weighted avg       0.59      0.59      0.59        32\n",
      "\n",
      "accuracy = 0.45454545454545453\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.36      0.45        14\n",
      "       missed       0.36      0.62      0.45         8\n",
      "\n",
      "    micro avg       0.45      0.45      0.45        22\n",
      "    macro avg       0.49      0.49      0.45        22\n",
      " weighted avg       0.53      0.45      0.45        22\n",
      "\n",
      "(444,)\n",
      "[ 3.99444433e-02 -8.38017154e-03 -1.08432628e-02 -6.80127070e-03\n",
      " -5.26303801e-03  4.63892425e-03  7.14574864e-03  2.22586979e-02\n",
      "  8.83508534e-03 -2.84792828e-02  3.06017316e-02  1.39634360e-02\n",
      " -1.54627200e-02 -5.23740722e-03  7.00564253e-03  7.19328865e-03\n",
      " -1.65270382e-02  2.38416062e-02  3.15131170e-02 -1.21276532e-02\n",
      "  1.07427431e-02  1.34991759e-02 -7.43903655e-03  4.55287405e-03\n",
      " -1.98526584e-02 -8.28242652e-03 -4.33167746e-03 -1.93520058e-02\n",
      "  2.38992148e-02 -1.24014692e-02  8.06048485e-03  1.28267901e-02\n",
      "  2.67228154e-02  1.69319504e-02 -1.05790118e-02 -1.15880873e-02\n",
      " -5.72723537e-03 -5.11933510e-03  1.48660431e-02  2.82893794e-02\n",
      "  0.00000000e+00 -5.71909919e-03 -4.92636344e-03  2.37406180e-03\n",
      " -2.77838853e-03 -2.41652732e-02 -7.88224734e-03 -3.13387513e-02\n",
      " -1.23945176e-02 -3.88615167e-03  1.19191353e-04 -7.87892550e-03\n",
      " -1.63218821e-03 -1.53097084e-02  3.81043619e-03 -4.05268487e-02\n",
      " -9.37716943e-03  5.87540291e-03  3.25188822e-02  3.32375325e-03\n",
      "  2.30016926e-02  1.10149052e-02  1.68442082e-02  2.93621272e-02\n",
      " -1.88506165e-02 -1.24515393e-02  1.92774717e-02  4.51287001e-03\n",
      " -1.29524901e-02  4.27837550e-02 -4.43914202e-02  3.41081277e-02\n",
      "  3.42253956e-03 -3.33382153e-02 -2.20159449e-02 -9.59149004e-03\n",
      "  9.16744650e-03 -1.91101078e-02 -5.90984665e-03 -6.84447518e-03\n",
      " -2.99723653e-03  2.38672374e-02  1.79655312e-02  1.44536953e-02\n",
      "  7.83675695e-04 -2.44527151e-03 -2.06356968e-03 -7.73693905e-02\n",
      " -1.15458213e-02 -1.12029337e-03 -7.32876740e-03  7.48331741e-03\n",
      "  1.67404817e-02  4.22918441e-02 -5.27748115e-03 -6.45523225e-03\n",
      "  3.61306725e-03  1.20531285e-02  1.44081826e-02 -2.23667504e-03\n",
      " -9.59919570e-03 -3.35796550e-02  2.90830171e-02  3.50248568e-02\n",
      "  4.54672352e-03  4.52346968e-03 -3.09289907e-02 -3.67296847e-02\n",
      "  2.08427468e-02  2.02478023e-02 -4.00978101e-03  6.25515993e-03\n",
      "  5.70286893e-04  1.60448328e-02 -3.21298574e-02 -1.96341702e-02\n",
      "  1.86448135e-02 -1.08035295e-02  1.29932754e-02 -2.53306127e-02\n",
      " -5.11151008e-03  1.75329569e-02  1.42088169e-02  3.60535088e-02\n",
      "  1.31451690e-02 -3.52322294e-03  1.44525383e-02  2.82495048e-02\n",
      " -4.43630571e-02 -3.77489539e-04 -1.88456431e-02 -2.12238275e-02\n",
      " -2.12978214e-03 -1.09517972e-02 -3.05683832e-02  1.26813086e-03\n",
      " -4.22971156e-02 -7.08994871e-04 -2.57400820e-02  3.07807682e-02\n",
      " -9.45953722e-03 -3.00956381e-02  3.41104381e-02  2.86268584e-02\n",
      " -1.21039580e-03 -4.82072296e-03  1.99149599e-02  1.55541182e-02\n",
      " -2.61443297e-02 -6.71996206e-03  1.72033517e-02 -3.32882169e-02\n",
      " -1.43514291e-02 -2.60977109e-02 -2.82131403e-03 -7.50570249e-03\n",
      "  8.58894248e-03  3.11277613e-03  2.75768900e-03 -1.03224436e-02\n",
      "  1.13832888e-02 -2.89329717e-03  3.36499655e-02 -2.90275681e-02\n",
      "  9.82485976e-03 -5.54730201e-03  8.55946229e-03 -1.32988411e-02\n",
      "  1.36664313e-02 -1.61949255e-02 -3.06360347e-02 -2.09642240e-02\n",
      " -2.26913597e-02  2.88826972e-02  9.06565456e-03  1.20345382e-03\n",
      " -2.09046527e-02 -2.12461898e-02 -1.26686897e-02 -1.23776102e-02\n",
      "  8.83048037e-04 -3.27995885e-02  1.86124468e-02 -1.65443744e-02\n",
      "  2.74903054e-02  2.28644021e-02  7.95184045e-03  6.42224466e-04\n",
      "  1.93551020e-03 -3.02887537e-02 -2.51153773e-02 -4.38342964e-02\n",
      "  1.25374385e-02  6.72736993e-03 -1.75330072e-02 -1.08143574e-02\n",
      "  4.16033844e-03  9.60788826e-03 -8.09521334e-04 -2.02658590e-02\n",
      "  9.09379782e-03 -2.06132118e-03 -1.23765240e-02  4.38876874e-03\n",
      " -2.22197175e-02  9.88740932e-04  2.09976615e-02 -2.32749363e-02\n",
      " -2.08495539e-02  3.15238080e-04  1.13726502e-03 -3.72272362e-02\n",
      " -1.85613887e-03  1.08710851e-02  1.53901638e-02 -2.61433968e-02\n",
      " -3.82031516e-02 -3.30044345e-02  2.17460498e-02 -2.07450492e-02\n",
      " -1.34755910e-02  1.01585052e-02 -1.54439925e-03 -6.33589873e-03\n",
      "  9.98725983e-03  1.48767285e-02  2.22144679e-02  2.27896072e-02\n",
      "  1.51307270e-02  6.79779591e-03 -1.53457943e-02 -2.73234462e-03\n",
      "  2.14490400e-03 -6.42591445e-03  2.40543439e-02  3.85955911e-02\n",
      " -3.82436079e-02  2.98659308e-02 -3.49719421e-02 -7.89504367e-03\n",
      " -1.08413587e-02 -2.55006543e-02 -1.40595499e-03  1.79298521e-03\n",
      " -1.42886115e-02  1.37602732e-02 -3.45624691e-03 -5.27592708e-03\n",
      " -8.77361144e-03 -3.67146664e-02  3.41814036e-03 -8.76349976e-03\n",
      "  1.45896078e-02  2.88440784e-03 -3.01417589e-02  3.35879848e-02\n",
      "  3.04288268e-02 -2.67058156e-02  9.08755075e-03  3.45298058e-02\n",
      "  4.08103634e-03  2.21553640e-02 -1.41010507e-02 -4.80569336e-03\n",
      " -7.19123186e-03  2.74709910e-02  9.46043175e-03  1.02426845e-02\n",
      " -2.83125047e-02  2.13074856e-02 -2.40586030e-02 -1.37461167e-02\n",
      " -9.37637162e-03  3.03601292e-02 -2.19025383e-02  4.27496534e-02\n",
      " -7.93554926e-03  1.42115710e-02  1.11641534e-03  1.95825491e-02\n",
      " -1.20413755e-02  1.47862282e-02  1.04044004e-02  9.34506421e-03\n",
      "  5.57972949e-03  5.52725564e-03  2.38003457e-02 -9.23712123e-04\n",
      " -3.43861079e-02 -8.91125316e-03  7.73692984e-03  8.43858902e-03\n",
      " -3.66909710e-03  1.24755581e-02  3.12089927e-02 -1.48312449e-02\n",
      " -2.42012729e-03  1.80658178e-02 -6.49769839e-03  9.10943332e-03\n",
      " -9.02884045e-03 -1.84326747e-02 -2.16288386e-02 -1.00269743e-02\n",
      " -1.82709126e-03 -1.63181488e-02  2.00544003e-02  3.78058635e-03\n",
      "  3.73459331e-02 -1.54221427e-02  2.23409439e-02 -2.16911170e-02\n",
      "  1.53326318e-03  2.05323471e-02 -7.01642760e-03 -3.68940722e-02\n",
      " -1.02437311e-02  2.12980099e-03  8.81876318e-03 -3.37353502e-03\n",
      " -2.20937668e-03  7.28829666e-03 -2.11635777e-02 -1.65479294e-02\n",
      "  2.48652948e-02 -1.21256293e-02  2.04613100e-02 -1.95978970e-02\n",
      " -3.75036532e-04  2.00960849e-02  1.90471093e-03  1.63932113e-02\n",
      "  3.57599235e-02  1.07551854e-02  8.13314274e-03 -2.06744733e-02\n",
      "  1.13287898e-02 -5.63716734e-02 -1.13979479e-02  4.94432885e-03\n",
      "  2.14882710e-03 -1.68438155e-02  1.83650466e-02  1.90121596e-04\n",
      "  2.67955286e-02 -8.35083593e-03  1.12842184e-02  1.58187841e-02\n",
      "  5.28671542e-03 -3.92136408e-02 -1.57042741e-02  2.51938951e-02\n",
      "  3.82053814e-03  1.04350653e-02  2.86944934e-02  2.81450448e-02\n",
      "  4.35716617e-03 -9.09098504e-03  1.68158702e-02  1.37588847e-02\n",
      "  7.73315705e-03 -9.25278545e-03 -5.18061445e-02  5.87344404e-03\n",
      "  9.85120170e-03 -1.46067124e-02  6.58781527e-03 -3.59030683e-02\n",
      " -1.46913357e-02  5.23737678e-03  3.26340390e-04  1.22313085e-02\n",
      " -2.28907672e-02 -9.12517253e-03 -1.23390754e-02 -6.84448143e-03\n",
      " -9.37574294e-04 -2.66833028e-02  7.65463505e-03  3.25999205e-02\n",
      "  1.12250943e-02 -2.24779562e-02 -1.64476510e-02  1.60318402e-02\n",
      "  5.89783220e-03 -3.27721691e-03 -2.02312199e-02 -2.38357776e-03\n",
      "  1.46401393e-02 -1.27950044e-02 -4.62508313e-03  4.18352565e-03\n",
      " -1.39718265e-02  1.85860579e-02 -1.06483025e-02  7.19399491e-03\n",
      "  3.72733118e-04 -3.57593400e-02  4.64568903e-03 -2.50622390e-03\n",
      " -1.23525431e-02 -4.80745364e-04  1.23077528e-02  5.27433638e-03\n",
      "  1.61485780e-02  1.34042815e-02 -1.64190561e-02  1.13115800e-02\n",
      " -3.24365545e-02 -9.19938313e-03 -1.24136569e-02  7.08247868e-03\n",
      " -9.13114193e-03 -1.12407264e-02  2.27863575e-02  4.10899280e-02\n",
      "  2.08130632e-02 -1.42681870e-02  5.41408796e-05 -2.87204493e-03\n",
      " -2.34160659e-02  2.34051174e-02  3.51462146e-03  1.36690355e-02\n",
      "  1.63593493e-02  2.21388662e-02 -3.95085164e-02 -5.04237601e-03\n",
      " -1.43102644e-02 -2.48253933e-02 -1.15433544e-02 -1.84077521e-02\n",
      " -1.03927288e-02  3.54190161e-02 -9.81988938e-03 -2.14830967e-02\n",
      " -3.25754580e-02  1.01987201e-02  3.47920149e-03  9.69428817e-03\n",
      " -9.12537025e-03  1.27225680e-02 -6.46210923e-03  1.31187912e-02]\n",
      "748676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 testing: 24\n",
      "missed           26\n",
      "correctsource    10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           18\n",
      "correctsource     6\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.66666667 0.66666667 0.6        0.4        0.5\n",
      " 0.75      ]\n",
      "Accuracy: 0.6111111111111112\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.30      0.30      0.30        10\n",
      "       missed       0.73      0.73      0.73        26\n",
      "\n",
      "    micro avg       0.61      0.61      0.61        36\n",
      "    macro avg       0.52      0.52      0.52        36\n",
      " weighted avg       0.61      0.61      0.61        36\n",
      "\n",
      "accuracy = 0.5416666666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.27      0.50      0.35         6\n",
      "       missed       0.77      0.56      0.65        18\n",
      "\n",
      "    micro avg       0.54      0.54      0.54        24\n",
      "    macro avg       0.52      0.53      0.50        24\n",
      " weighted avg       0.65      0.54      0.57        24\n",
      "\n",
      "(444,)\n",
      "[-0.00872425 -0.01346329 -0.00795562 -0.01445018 -0.003659   -0.0302707\n",
      " -0.02131802 -0.0173863  -0.01606291 -0.00200201 -0.00985295  0.00940759\n",
      " -0.02520029 -0.06470571 -0.00584836  0.00752883 -0.02286779 -0.0450395\n",
      "  0.01858604  0.04112772 -0.00450568 -0.03961754 -0.02617399  0.01436739\n",
      " -0.01478467  0.00526942  0.00370321  0.04169733  0.01771439  0.00347222\n",
      "  0.00649178  0.0030373  -0.01886308 -0.03157369  0.00103904 -0.00564574\n",
      "  0.02318913  0.00335763  0.01238545  0.02302459  0.0113244   0.01279832\n",
      " -0.0052803   0.04329112  0.01591189 -0.05372862  0.00728997  0.0452449\n",
      " -0.00528345 -0.01664872 -0.01875974  0.00994423 -0.00455474 -0.00676036\n",
      " -0.01551903 -0.00849336 -0.01097651 -0.0402443   0.00495041 -0.02585579\n",
      " -0.0380338  -0.02987744  0.03024977  0.03507875 -0.01865132 -0.03918824\n",
      " -0.02780674 -0.0101207   0.01102765 -0.0128317  -0.02962839  0.0066897\n",
      " -0.00735384 -0.00771227  0.01059651  0.01655664 -0.02047536  0.00451017\n",
      "  0.00830242  0.00872657  0.02435768  0.01254078  0.01743599  0.02809794\n",
      "  0.01086623  0.041326    0.0135751  -0.02386723 -0.02237954  0.02630967\n",
      " -0.0303932   0.01025614 -0.0232263  -0.01893191 -0.00724193  0.02591756\n",
      " -0.011021   -0.04592219  0.01851075 -0.0179633  -0.01743245 -0.02986262\n",
      " -0.0159646  -0.00013412 -0.00065604  0.02060283 -0.02326346 -0.02628828\n",
      "  0.02829143  0.02034212  0.01962447  0.01338654 -0.00390129  0.02620303\n",
      " -0.01330242 -0.01831555  0.01726182 -0.0082865  -0.01215624  0.01373522\n",
      "  0.0358854  -0.00992894 -0.02398386 -0.01273481 -0.00375059  0.00671829\n",
      "  0.00491706  0.00320442 -0.01072816  0.01554589 -0.02717231 -0.02150257\n",
      "  0.007507    0.02687171 -0.00898089  0.00334964 -0.03205434 -0.02176971\n",
      " -0.00585019 -0.0100016   0.01874786  0.01794967  0.02699598  0.01740729\n",
      "  0.01425779  0.01846918 -0.01873664  0.03424461  0.00408421 -0.00672474\n",
      "  0.03404048  0.0131258  -0.00543277 -0.02263499  0.015435   -0.00831962\n",
      " -0.0067633   0.02271651  0.01195606 -0.01112164 -0.03539357 -0.01421119\n",
      " -0.02352883  0.00077823 -0.01072008 -0.01294429  0.02437718  0.00238246\n",
      " -0.0193445   0.01998073 -0.02288616 -0.00750029 -0.02011099  0.00571909\n",
      "  0.00038972 -0.00416544 -0.00103103 -0.00908645 -0.00931783 -0.03108583\n",
      " -0.00802637  0.02883309  0.02319114  0.01655566  0.00375714 -0.01454801\n",
      "  0.00368735  0.00197519  0.01648356  0.03083884 -0.03728222 -0.01109596\n",
      " -0.00471668  0.00477602  0.03484973 -0.02204286 -0.01314277  0.0195961\n",
      "  0.00319345 -0.0015571   0.02399437 -0.0263836   0.00422514 -0.03903381\n",
      " -0.01136389  0.02361271  0.01179912 -0.00148776  0.0236344  -0.00647885\n",
      " -0.02684773 -0.00863331  0.         -0.00359738 -0.00829057 -0.0132956\n",
      "  0.0348953  -0.00363306  0.03106168 -0.0137194   0.00854406  0.00746057\n",
      " -0.00916091 -0.01027181 -0.00965736 -0.01199129 -0.00516618 -0.01491176\n",
      " -0.01601552 -0.03844803  0.03038946 -0.00513583 -0.02669978  0.01434585\n",
      " -0.02354876  0.01944181 -0.03933202 -0.00398962 -0.00841239  0.00700676\n",
      " -0.01436931 -0.00715379  0.01049254 -0.06064682  0.01393429 -0.0413372\n",
      " -0.0232674  -0.01461004  0.00782679 -0.01428126  0.0133804   0.00587495\n",
      "  0.03151197  0.00221369  0.005854    0.01551635  0.03609276 -0.02413152\n",
      " -0.02497419 -0.00573893 -0.00082418  0.01630843 -0.01682179 -0.01350364\n",
      " -0.01628158  0.0117404   0.00265897  0.01920906 -0.0198795  -0.00624041\n",
      " -0.02835844 -0.00409994  0.00948635  0.05078014 -0.00634063  0.03878601\n",
      "  0.03673395  0.00024845  0.02000152  0.03018437  0.04346036 -0.02616226\n",
      "  0.01542408  0.01244723 -0.00085407 -0.03543483  0.01414712  0.03467782\n",
      "  0.0272336   0.00995109  0.01892615  0.00192055  0.02188266 -0.03232242\n",
      " -0.00994309  0.0164574  -0.00553799 -0.00805987 -0.01267639 -0.02930944\n",
      "  0.00426972  0.00029977  0.01193614 -0.0166306  -0.00072646 -0.03034365\n",
      " -0.01864227 -0.00405378 -0.01766314 -0.00502516 -0.00915937 -0.01854509\n",
      " -0.01995611 -0.02136774 -0.01393529  0.02062391 -0.01049787  0.00834123\n",
      "  0.02129848 -0.03733977 -0.04220682 -0.00356572  0.04371397 -0.06333846\n",
      "  0.03177199 -0.00804998 -0.00308624  0.00606996 -0.00284237  0.0046392\n",
      " -0.00122896 -0.01225544  0.03218554  0.02138304 -0.02069598  0.01984463\n",
      "  0.00581544  0.00818834  0.01871556  0.01484819  0.02924055 -0.02922612\n",
      "  0.01823234  0.04389792 -0.03288993 -0.00626985  0.0125465  -0.02745129\n",
      "  0.01556903 -0.00623713 -0.00213622 -0.03770001  0.03834723 -0.05380489\n",
      " -0.0190808  -0.01043271  0.04155534 -0.0129493   0.00683517  0.00524707\n",
      "  0.00059396 -0.01685901 -0.0194282  -0.03691728 -0.01586802  0.02302098\n",
      " -0.01745995  0.00056439  0.02562032 -0.00803209  0.00201449  0.0407984\n",
      "  0.01694077 -0.00104591 -0.00889379 -0.01446446  0.00545465  0.00023849\n",
      "  0.0065046  -0.06224944  0.03354661  0.01930425  0.01401675 -0.00151862\n",
      " -0.00945056  0.01727816  0.02238324  0.01127182 -0.02160519 -0.00083695\n",
      "  0.01705323 -0.02446843 -0.00800074  0.04556528 -0.02501396  0.00811469\n",
      " -0.01427503 -0.01079922  0.01157584 -0.04523061  0.04162063  0.02792323\n",
      "  0.0116086  -0.01179399 -0.01392651  0.00074034  0.0001438   0.03671776\n",
      " -0.02849751 -0.01600528 -0.00273649  0.01960109 -0.00376772 -0.00756646\n",
      " -0.02214956  0.00045074 -0.00415183  0.03823689 -0.03293989 -0.02161878\n",
      " -0.03072994 -0.01647445 -0.01358842 -0.0208866  -0.00398877 -0.02072052\n",
      " -0.04863924 -0.02329419  0.00858265 -0.02209173 -0.00353072  0.0161524\n",
      " -0.01036628 -0.00772104  0.02368888 -0.02149117  0.02151386  0.00452493\n",
      "  0.01023457 -0.01781772 -0.0055539   0.03617326  0.01011289 -0.0243531 ]\n",
      "778749\n",
      "training: 33 testing: 22\n",
      "missed           19\n",
      "correctsource    14\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           13\n",
      "correctsource     9\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.6  0.4  0.4  0.6  0.6  0.75 0.5 ]\n",
      "Accuracy: 0.5454545454545454\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.47      0.50      0.48        14\n",
      "       missed       0.61      0.58      0.59        19\n",
      "\n",
      "    micro avg       0.55      0.55      0.55        33\n",
      "    macro avg       0.54      0.54      0.54        33\n",
      " weighted avg       0.55      0.55      0.55        33\n",
      "\n",
      "accuracy = 0.5909090909090909\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.56      0.53         9\n",
      "       missed       0.67      0.62      0.64        13\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        22\n",
      "    macro avg       0.58      0.59      0.58        22\n",
      " weighted avg       0.60      0.59      0.59        22\n",
      "\n",
      "(444,)\n",
      "[ 7.88881940e-03  1.07735336e-02  1.94586554e-02 -4.71597734e-03\n",
      " -2.69272901e-03 -7.69871459e-03 -6.48386660e-03 -4.76568720e-02\n",
      " -2.86160554e-02 -1.87657641e-02 -1.63576442e-03 -2.17048896e-03\n",
      " -8.98706396e-03  2.37153588e-02  2.65344138e-02  1.21743592e-02\n",
      "  1.71206840e-03  6.08107625e-03  8.10079620e-04 -9.79604580e-03\n",
      " -1.61934576e-02 -7.55178659e-03  1.18587487e-02  1.43836615e-02\n",
      "  3.00055281e-03  7.94412615e-03  2.18126043e-04  1.66869227e-03\n",
      " -1.51516710e-03 -1.29154191e-02 -6.66363608e-03  2.33469287e-02\n",
      " -3.90058599e-02 -7.92080889e-03 -1.04239924e-02  1.52673669e-02\n",
      "  2.07149674e-02  7.58114870e-04  6.30157190e-03 -8.47435363e-03\n",
      "  8.89561233e-03 -1.03213695e-02  3.36098991e-03  1.26997929e-02\n",
      "  1.20236627e-02 -1.29494095e-02 -1.93823227e-02 -1.13462342e-02\n",
      "  4.13460403e-02  3.98016262e-03  1.19560931e-02 -1.47499141e-02\n",
      "  8.26418719e-03 -1.19349292e-02 -1.06041249e-02 -1.19212234e-02\n",
      "  1.70934209e-03 -4.28662090e-04  5.30164485e-03 -7.60342574e-03\n",
      " -3.19224503e-02 -1.18200109e-02  2.95058835e-02  1.08677724e-02\n",
      " -9.20902264e-03  2.81102259e-02 -2.66876492e-02 -9.39459026e-03\n",
      " -2.12767314e-02 -5.12681588e-03  7.91807260e-04  1.53918952e-02\n",
      " -5.23074859e-03 -6.65128031e-03 -2.87443708e-02 -6.00809574e-03\n",
      "  1.53990215e-03 -8.61776407e-04 -3.53710723e-03 -3.23085022e-02\n",
      " -3.49264197e-02  1.77142083e-02 -2.51139137e-02 -2.20467115e-03\n",
      " -8.36390283e-03 -1.16759497e-02  1.36881331e-02  1.03058062e-02\n",
      " -5.17696970e-03  2.02426717e-02 -3.47397716e-02 -1.80199915e-02\n",
      " -2.87387547e-02 -1.57230350e-02 -1.78229940e-02 -1.62724584e-02\n",
      "  1.41450554e-02 -2.22446425e-03  4.30708261e-02  2.35511290e-02\n",
      "  2.65049237e-02  1.73695892e-02 -5.18375895e-02 -1.46918274e-02\n",
      "  6.67334468e-03  6.65780179e-03 -2.66949625e-02 -1.34228028e-02\n",
      "  2.53292928e-02  2.97421100e-02 -2.38982332e-02  7.07804292e-03\n",
      " -2.31399025e-02 -6.56643537e-03 -4.46069556e-03 -5.35108106e-03\n",
      "  6.46339648e-03  4.23570131e-02 -1.69892548e-03 -1.13656731e-02\n",
      "  4.08180393e-02 -6.72487137e-02 -4.37320244e-03  2.37552606e-02\n",
      " -1.25286539e-02  1.93897090e-02 -1.94683970e-03  3.88954153e-03\n",
      " -1.38532714e-03  1.76851061e-02 -1.27400447e-02  7.84511782e-04\n",
      " -8.95180139e-03  3.34221858e-03  1.01980322e-02  1.60946565e-05\n",
      "  1.30761698e-02 -9.86028676e-03  9.18945949e-04 -3.72228980e-02\n",
      "  2.84867113e-02  6.26560713e-03  6.71791226e-03 -1.39767544e-02\n",
      "  2.93732016e-03 -1.65923994e-02 -1.31781678e-02  3.13954229e-03\n",
      " -1.11160013e-02 -1.73684429e-02  1.76638752e-02  9.24358991e-03\n",
      " -6.69287553e-03 -7.72711766e-03  1.34297284e-02  9.12550738e-03\n",
      "  2.95793066e-02 -9.01787800e-03  1.33364659e-02 -4.10971279e-03\n",
      "  1.78165480e-02 -1.69819122e-02 -1.23192771e-03 -2.40999288e-02\n",
      " -1.17532908e-03 -1.14411335e-02  2.30733696e-02  9.40173235e-03\n",
      "  1.35996016e-03 -1.43661685e-02  3.69638671e-02  1.93181184e-02\n",
      " -3.41215033e-03  3.87599621e-03 -2.11468985e-02 -2.17117335e-03\n",
      " -3.58117408e-02  1.49026633e-02 -1.11800822e-02  3.25813178e-02\n",
      "  1.26935277e-02  4.89884811e-04 -5.70709020e-03 -1.77156657e-02\n",
      "  1.36738737e-02  3.01904828e-02  1.74976747e-02  1.81453533e-02\n",
      " -5.97357563e-03  1.47173612e-02 -1.59973313e-02 -6.38796010e-04\n",
      " -6.64083784e-03 -2.09804314e-02  1.33270263e-02  1.61392840e-02\n",
      "  1.14999126e-02 -1.93593931e-02  5.04823965e-03 -1.05424266e-02\n",
      " -3.27695019e-02 -6.92160259e-04  2.10957980e-02 -3.01761757e-03\n",
      "  1.66908365e-02 -2.62526591e-02  3.85760823e-02 -5.58510958e-03\n",
      " -1.30702023e-02  4.60357651e-02  3.44819217e-03 -1.85334943e-03\n",
      " -2.45086513e-02  3.72666127e-02  1.10578698e-02 -8.35441706e-03\n",
      " -2.25048571e-02  1.16077524e-02  7.45069190e-04  2.82823606e-02\n",
      "  2.60518192e-02  1.02461191e-02  2.38167319e-02 -9.07288052e-03\n",
      " -9.78288534e-03 -3.03862629e-02  1.31686233e-02 -1.60115079e-02\n",
      " -1.79421207e-03  6.62432114e-03  2.76919339e-02 -1.24495912e-02\n",
      "  1.36872622e-02 -2.01183206e-02  2.43800816e-03 -1.20614036e-03\n",
      " -3.08851355e-02 -2.47638581e-02  2.03324618e-02 -2.57384894e-03\n",
      "  3.17563630e-02  2.63341700e-02  1.13074886e-03 -3.32065002e-02\n",
      " -6.42935344e-03 -2.68967856e-03 -3.95831270e-03  6.47225408e-03\n",
      " -1.66760408e-02 -2.60840036e-03  9.74164315e-03 -2.97677662e-02\n",
      " -6.40974723e-03  2.33215285e-02  2.60554051e-02  1.05433349e-02\n",
      " -1.21173088e-02 -1.91042746e-02  3.01495198e-02 -1.29639480e-02\n",
      " -2.77649563e-02  1.02683483e-02  1.28473818e-02  6.37201529e-03\n",
      " -2.18922532e-02  3.03848803e-03  2.30267805e-02 -2.06626837e-02\n",
      " -9.92558481e-03 -9.64427015e-04 -1.87145115e-02 -3.63780203e-03\n",
      "  8.66409133e-03 -4.05115597e-02 -2.71031232e-03 -2.57083282e-02\n",
      " -3.71711180e-02  3.43563420e-03 -6.77320895e-03 -1.92920675e-02\n",
      "  9.77170633e-03 -1.27243135e-03  1.83293125e-02  1.42717504e-03\n",
      "  6.32667744e-03 -2.58708167e-03 -2.06594368e-02 -1.58731883e-02\n",
      " -2.72265074e-02  4.67562978e-02  6.49171004e-03  2.29382135e-02\n",
      " -3.60471707e-03 -2.45818628e-02  5.03848447e-04  2.35425493e-02\n",
      " -9.13569332e-03 -2.54851505e-02  8.69706709e-04  2.51911902e-02\n",
      "  8.12498758e-03  6.51181978e-03  2.15540198e-02 -3.49835303e-03\n",
      "  1.43351636e-02 -2.18814196e-02  2.84295367e-02  5.42198760e-03\n",
      " -1.72814158e-02 -3.52745333e-03 -3.09395803e-02  5.22940557e-03\n",
      " -4.85159979e-03  3.01983141e-02  1.46631847e-02  1.60249506e-02\n",
      "  2.79425691e-02 -3.90542176e-03  3.11888602e-02  2.26908150e-02\n",
      " -1.63636283e-02  1.45777193e-02  1.28165779e-03 -3.52678855e-03\n",
      " -2.25019722e-02  1.02080853e-02 -4.98082899e-03  1.68367750e-02\n",
      "  2.86399167e-02 -2.96878785e-02 -1.60746512e-02  1.90356073e-02\n",
      " -7.21937349e-04  2.18935967e-02  2.41321970e-02 -2.25697685e-02\n",
      "  2.25149159e-02 -1.27758693e-03  9.22066319e-03  3.64713746e-02\n",
      " -2.18695307e-03 -4.48190475e-03 -2.49971608e-03  8.18101971e-03\n",
      " -3.42210254e-02 -7.32590702e-03  2.42462078e-02  2.31085953e-02\n",
      " -4.09412092e-03  4.23190638e-02  4.48797065e-04  4.01456610e-03\n",
      "  3.21679254e-03 -2.34867245e-02  2.19469613e-02 -8.25519624e-03\n",
      "  3.53355535e-03  2.25873890e-03 -1.55513425e-02  2.74362760e-02\n",
      "  7.53301824e-03 -2.20527521e-02  2.59097254e-02  8.40266449e-03\n",
      "  2.24630695e-02 -1.89169338e-02 -6.57606398e-03  1.99620272e-03\n",
      "  2.22529849e-02  1.74816620e-02 -1.88565217e-02 -5.96023441e-03\n",
      "  2.00936190e-02 -3.26454832e-03 -9.56431167e-03 -3.22070024e-03\n",
      "  4.75376576e-04 -7.56040584e-04 -6.80421384e-04  1.43101263e-02\n",
      " -1.07904036e-02 -1.12173808e-02  4.58963410e-03  8.60727039e-03\n",
      " -1.85432026e-02 -6.51092738e-03  2.01261548e-02  1.52546044e-02\n",
      " -2.48534954e-02  1.74264081e-03 -2.55583003e-02 -1.49402783e-02\n",
      "  1.49836105e-02  1.59649321e-03  2.26696849e-03 -7.84007791e-03\n",
      " -5.24205407e-03  1.80327803e-03 -1.77531591e-02  2.68091434e-02\n",
      " -3.31920409e-02  8.22542955e-03  1.06397709e-02  2.18927415e-02\n",
      "  2.23770274e-02 -1.75594200e-02  2.48935387e-02  5.76683245e-03\n",
      " -8.41707393e-03  9.88767062e-03 -8.30823984e-03 -8.13987244e-03\n",
      "  1.21700529e-02 -9.76448965e-03  8.20094903e-04  2.58726620e-02\n",
      "  2.92174399e-02 -1.89295379e-02 -1.18427292e-02 -1.18453762e-03\n",
      " -4.36106850e-03  9.58233530e-04 -8.43928935e-03 -1.69160222e-02\n",
      "  2.81370513e-02 -4.91625433e-02 -1.32490392e-02 -9.91918769e-03\n",
      " -1.84392953e-02 -3.06371931e-03 -4.54519170e-03  1.41940020e-02\n",
      "  2.15778208e-02  3.18348778e-02  1.05041630e-02  4.46106386e-03\n",
      " -3.05856450e-03 -6.67702491e-03  2.50825501e-03  3.04645142e-03\n",
      " -2.34180088e-02 -2.68373405e-02 -1.04664411e-02  3.54600540e-02]\n",
      "783781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 34 testing: 23\n",
      "correctsource    24\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    16\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.5        0.5        0.5        0.5        0.5\n",
      " 0.5       ]\n",
      "Accuracy: 0.5294117647058824\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.65      0.71      0.68        24\n",
      "       missed       0.12      0.10      0.11        10\n",
      "\n",
      "    micro avg       0.53      0.53      0.53        34\n",
      "    macro avg       0.39      0.40      0.40        34\n",
      " weighted avg       0.50      0.53      0.51        34\n",
      "\n",
      "accuracy = 0.6956521739130435\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.74      0.88      0.80        16\n",
      "       missed       0.50      0.29      0.36         7\n",
      "\n",
      "    micro avg       0.70      0.70      0.70        23\n",
      "    macro avg       0.62      0.58      0.58        23\n",
      " weighted avg       0.66      0.70      0.67        23\n",
      "\n",
      "(444,)\n",
      "[ 0.02339025  0.02252627 -0.02279545 -0.02270783  0.03879704 -0.05216075\n",
      "  0.00171111 -0.01594455  0.00116715  0.00487292 -0.00145414  0.01224236\n",
      "  0.01129104 -0.03655845 -0.02187476  0.01488312  0.021666    0.03169296\n",
      "  0.03171701 -0.00760897 -0.03166834 -0.01307042  0.01347578  0.01055468\n",
      "  0.00536131 -0.02765506  0.0293677  -0.01770055 -0.00096549 -0.03914913\n",
      " -0.00611415  0.00615633  0.03491728  0.00700602 -0.01326594  0.00299846\n",
      "  0.0548175   0.0131073   0.02663589 -0.00378291 -0.00748389 -0.00180387\n",
      "  0.03987232  0.004477    0.01212072  0.00141842  0.00732366 -0.01241842\n",
      " -0.00215171  0.00632588  0.03232999 -0.01835928  0.02481605  0.01276024\n",
      "  0.02467744 -0.02869992  0.01374157  0.00514194  0.03026068  0.01377511\n",
      "  0.01826132  0.01395456  0.01781112  0.02213349  0.02107129 -0.01155557\n",
      "  0.00269778 -0.02856302 -0.01943035  0.03104289  0.07027502 -0.03320259\n",
      " -0.04410647  0.01079282 -0.01799163  0.01048369 -0.03434583 -0.01547037\n",
      "  0.02005213  0.02887157  0.01533209  0.01155617 -0.01402996  0.00296556\n",
      "  0.00672555  0.01544912 -0.0364428  -0.02130038  0.01518357 -0.00486865\n",
      "  0.05684926 -0.0315566  -0.00837304  0.00349513 -0.05773233  0.01190614\n",
      "  0.00200723  0.0034621  -0.01391241  0.01160509  0.03242519  0.03025965\n",
      "  0.02953197  0.01031434  0.02558031 -0.00679143 -0.0038332   0.01523306\n",
      " -0.00509514  0.00137574 -0.04860694 -0.02297098 -0.01461671  0.04980224\n",
      "  0.02959117 -0.00863998 -0.01543868  0.01213506 -0.01480919 -0.01365769\n",
      "  0.05914727  0.04491182 -0.005614    0.00539459 -0.04247512  0.01341488\n",
      "  0.02950679  0.03964128 -0.00434658 -0.03564047 -0.01303725  0.03040571\n",
      "  0.00695191 -0.00056162  0.01112173 -0.00657819 -0.02198709  0.00803134\n",
      " -0.02311664  0.01558848 -0.01053072  0.00073308  0.00142978 -0.02446618\n",
      "  0.02363357 -0.01026413  0.02513824  0.04322464  0.0144246   0.02241107\n",
      " -0.02419826  0.01019143 -0.0298821   0.01222991 -0.01948926  0.03257202\n",
      " -0.00065048 -0.02746575  0.00624072  0.00166937 -0.01853752 -0.0349662\n",
      "  0.01022164  0.02313385  0.01261289 -0.02227465 -0.00259868  0.02749974\n",
      " -0.02009932  0.03035986 -0.05825314  0.03203916  0.04296197  0.0099651\n",
      "  0.00892818  0.03034389 -0.00553032  0.00048365  0.01599233  0.03251341\n",
      "  0.01628928  0.00653275  0.00067888 -0.0399371   0.00623166  0.01226794\n",
      " -0.02184765  0.00120911  0.02737019 -0.00439812  0.06838552  0.01064902\n",
      "  0.0089721   0.00784575 -0.01317791  0.02376767 -0.01306682 -0.00983302\n",
      "  0.00591763 -0.00967501 -0.02613219  0.03107556  0.01460637 -0.03845416\n",
      "  0.01395407  0.01747935  0.04542345  0.0725479  -0.03977957 -0.01930532\n",
      " -0.00436554 -0.01365019  0.          0.00451005  0.01611443  0.01041082\n",
      " -0.01255405  0.031224   -0.00711455 -0.02446289  0.00818466  0.02151726\n",
      " -0.02148422 -0.00857     0.03019296  0.00451745  0.05690673 -0.03254186\n",
      "  0.03937309 -0.00208864  0.02482568 -0.00775649  0.02214236 -0.0287054\n",
      " -0.00433235  0.01805744 -0.00262286  0.0077843   0.02482694 -0.01169611\n",
      "  0.0199228  -0.00617369 -0.01850866  0.00585184  0.03428347 -0.02710978\n",
      "  0.02968299 -0.01939968  0.03745676  0.02026343  0.02888787 -0.03124189\n",
      "  0.04799354 -0.01200135 -0.03227399 -0.00241057  0.01593047  0.0458762\n",
      "  0.00811724 -0.00454245  0.0143511   0.00636961 -0.03565775  0.02942345\n",
      "  0.00037988 -0.04036394 -0.00068844  0.00647485  0.02359278  0.00077569\n",
      " -0.00550035  0.00207522 -0.01740947 -0.02463462 -0.0185044  -0.02132953\n",
      " -0.01456156  0.0047336   0.02332281 -0.03935639 -0.01568704 -0.01531576\n",
      " -0.00501734  0.02768216 -0.00354929  0.03324675 -0.01279874  0.02740413\n",
      "  0.00211448 -0.01332038  0.00272748  0.02177116 -0.00115451 -0.00485223\n",
      " -0.03720026 -0.00495931 -0.04481815  0.01463725  0.01138075 -0.01836285\n",
      "  0.00679006  0.01469007  0.01145444 -0.03202697  0.01361802 -0.00044077\n",
      " -0.02689014  0.01595385 -0.05102261  0.03494857  0.04297986  0.03026902\n",
      "  0.01480246 -0.00289783 -0.0136033   0.00555337  0.03691969  0.00985822\n",
      "  0.00380535  0.01783951 -0.01337025 -0.02978511  0.01678585 -0.00821153\n",
      "  0.00118176 -0.01758744  0.0164631   0.01395339 -0.01128948  0.02389854\n",
      "  0.01431935  0.00118315  0.01095207 -0.0070497   0.03176466 -0.04267111\n",
      " -0.02086241 -0.05608035  0.01384775 -0.01977805 -0.00803867 -0.00969763\n",
      " -0.02639375 -0.02879738  0.02694    -0.01488431  0.01645765 -0.03232792\n",
      "  0.01619978 -0.03223453  0.03324606 -0.01448648  0.02595718 -0.00787335\n",
      "  0.00486181 -0.0116523  -0.00668223  0.00979512  0.00281406  0.02210841\n",
      " -0.00418261  0.01033796  0.01883696 -0.01671153  0.04137242 -0.02473493\n",
      "  0.00586021  0.01495718 -0.00085799 -0.01645622 -0.01443002 -0.01676156\n",
      "  0.0034539   0.0084571  -0.01747461 -0.02413781 -0.01252381  0.01797758\n",
      " -0.00595134 -0.01157537  0.00965352 -0.02636248 -0.0174454   0.03954463\n",
      "  0.0049059  -0.00285229  0.01097415 -0.02248256  0.0585457  -0.00382673\n",
      " -0.02535033  0.00572175 -0.01106383 -0.00465453 -0.00037276  0.02382804\n",
      " -0.04366964  0.02259127 -0.01119311  0.01063762  0.02906142  0.01338407\n",
      "  0.00777185  0.01065483  0.01044633  0.00690304 -0.01713361 -0.00262027\n",
      "  0.02506657  0.01720135  0.00796818 -0.01740699 -0.01805835 -0.03326597\n",
      "  0.00578586 -0.01119297 -0.00547829 -0.02573546 -0.04725761  0.00225008\n",
      "  0.01914615 -0.01156264 -0.01486775 -0.03733566 -0.00060778  0.00814649\n",
      "  0.02874241 -0.01020365 -0.0200428  -0.01023328 -0.00664485 -0.036776\n",
      "  0.01407905 -0.01392606 -0.02447691  0.00994478 -0.03320895  0.04166083\n",
      " -0.04564956 -0.02093331 -0.00499393 -0.02184197 -0.01541049  0.00208785]\n",
      "884343\n",
      "training: 32 testing: 22\n",
      "correctsource    17\n",
      "missed           15\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    12\n",
      "missed           10\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.66666667 0.8        0.6        0.5        0.25       0.75\n",
      " 0.5       ]\n",
      "Accuracy: 0.59375\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.61      0.65      0.63        17\n",
      "       missed       0.57      0.53      0.55        15\n",
      "\n",
      "    micro avg       0.59      0.59      0.59        32\n",
      "    macro avg       0.59      0.59      0.59        32\n",
      " weighted avg       0.59      0.59      0.59        32\n",
      "\n",
      "accuracy = 0.4090909090909091\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.46      0.50      0.48        12\n",
      "       missed       0.33      0.30      0.32        10\n",
      "\n",
      "    micro avg       0.41      0.41      0.41        22\n",
      "    macro avg       0.40      0.40      0.40        22\n",
      " weighted avg       0.40      0.41      0.41        22\n",
      "\n",
      "(444,)\n",
      "[ 3.32379448e-02  2.10585147e-02 -3.10641018e-02  5.27016065e-03\n",
      "  4.50514207e-03 -1.81845202e-02  4.02396082e-02  1.91354692e-02\n",
      "  1.26317839e-02  1.18603514e-02 -7.62631770e-05  4.20209807e-03\n",
      " -2.56874441e-03  2.85232863e-02  1.35812114e-02 -3.77671402e-03\n",
      "  2.01577856e-02  1.43574740e-02 -2.22913515e-02 -2.83045377e-02\n",
      "  3.34071348e-02 -5.24444331e-02 -2.17881019e-02  1.78697725e-03\n",
      "  1.54983439e-02  3.18724057e-02 -1.19470156e-02 -1.23081589e-02\n",
      " -7.00256364e-03 -1.75220630e-02 -2.72153894e-02 -6.01788892e-03\n",
      " -1.95224869e-02 -3.30221801e-02 -2.29201399e-02  3.09102000e-02\n",
      "  3.88906151e-02  8.54138516e-03 -4.74602873e-03  3.32644432e-02\n",
      "  2.71051547e-02  5.37283116e-03  3.31263968e-02  1.42529346e-02\n",
      "  7.35923782e-03 -5.49952965e-04  2.69683064e-02  1.96138483e-03\n",
      "  2.91045370e-02 -1.04789625e-02  5.40434973e-02  2.81599895e-02\n",
      " -2.52284859e-02 -1.32896649e-02 -5.44013494e-03 -2.39342336e-02\n",
      "  1.41740973e-02  8.48834627e-03  3.05106494e-02  1.44296453e-03\n",
      "  1.52590202e-02 -8.27939530e-03 -8.95317667e-03  3.43935296e-02\n",
      " -6.05090659e-03  4.67189486e-03  1.29902321e-02 -1.15427313e-02\n",
      "  1.80079772e-02  4.87616098e-02  1.13959493e-03 -2.58438069e-02\n",
      "  9.52054403e-03 -1.73372927e-02 -1.25507288e-03 -1.02883522e-02\n",
      " -8.69475990e-03  1.69583625e-02  3.16465731e-03  2.45989927e-03\n",
      "  1.00349608e-02 -4.66827216e-03 -7.50443448e-03 -3.65925277e-03\n",
      "  2.23714757e-02 -3.70515694e-02  1.24114401e-02  2.63091775e-03\n",
      " -9.09513672e-03  1.51521440e-02 -2.02595280e-02 -5.75962129e-03\n",
      "  1.91511881e-02  1.87429986e-04  2.15866109e-03  8.73359385e-04\n",
      "  1.30818954e-02 -1.07572547e-02  2.14276011e-02  6.06638526e-03\n",
      " -6.21629261e-03  3.46983257e-02 -1.90333654e-02  3.35034269e-02\n",
      " -8.96546836e-03 -4.38847857e-02 -1.68840511e-02 -1.25544871e-02\n",
      " -3.85417677e-03 -1.62655171e-02 -3.82700506e-02 -1.31776219e-02\n",
      "  2.00611281e-02  1.83826263e-02  2.31128467e-02  1.88607988e-02\n",
      " -1.94599881e-02  1.40729871e-02  7.27330824e-03  2.78391705e-02\n",
      " -2.20004739e-02  1.49074921e-02 -1.36111483e-02  3.20097765e-02\n",
      " -6.52657848e-03 -1.12301000e-02 -2.55738236e-02 -1.10361641e-02\n",
      " -2.12919584e-02 -5.94378945e-03  2.40410099e-02 -1.94827910e-02\n",
      "  7.81100794e-03 -7.06717042e-04  1.30398606e-02 -7.13039826e-03\n",
      "  2.75854485e-03  1.95298242e-03  7.26657164e-03  1.48864807e-02\n",
      "  1.47323707e-02 -1.67590547e-02  5.07090043e-02  1.04195071e-02\n",
      " -3.31573447e-02  4.50639118e-03  3.44564624e-02 -1.16211218e-02\n",
      "  8.03038312e-03  4.35005412e-02  4.16924590e-02 -6.42666624e-03\n",
      "  2.61735370e-02  1.41526345e-02 -3.26089605e-02 -1.88268353e-02\n",
      "  1.18614102e-02 -2.66953615e-02  1.22654080e-03  5.11960413e-03\n",
      "  1.19965654e-02 -2.79520202e-02 -2.93668195e-02  1.54526065e-03\n",
      "  2.34746305e-02 -2.80420800e-02  6.99226893e-03 -2.78772177e-03\n",
      "  1.71819099e-02  3.63449110e-03  2.68956754e-02  3.77211019e-02\n",
      "  1.99575398e-03  1.96440468e-03  8.23433374e-03  1.13570948e-02\n",
      " -6.35073062e-03  1.04552739e-02  1.73398231e-02  4.08618608e-03\n",
      "  4.01774079e-03  3.00502950e-02 -1.98249676e-03 -1.91205353e-02\n",
      " -6.45022113e-03  3.24189714e-03 -1.56158518e-03 -5.27807242e-04\n",
      " -1.48748412e-02 -1.07052248e-02  3.60533747e-03 -3.19083042e-02\n",
      "  1.21394387e-02  3.30444906e-02  9.76137560e-03  3.84178656e-02\n",
      "  1.53884250e-02 -5.46155655e-03  8.22623251e-03 -2.39449870e-02\n",
      "  6.88584707e-03  2.75073888e-02 -2.38498014e-02  1.81410994e-03\n",
      "  1.93161412e-02 -2.20257250e-02 -8.36404770e-03  3.97791710e-02\n",
      "  7.22551573e-03  2.24976407e-02 -2.77535935e-02  2.11095109e-02\n",
      "  0.00000000e+00  3.78826499e-03  1.59205768e-02  3.08555933e-02\n",
      " -2.40515608e-02 -1.99747909e-02  8.53148836e-04  1.36729497e-02\n",
      "  1.64163122e-02 -1.20956832e-02  1.88935235e-02  1.75085180e-02\n",
      " -2.68422097e-02  1.23089512e-03  1.10202205e-03  2.66622502e-03\n",
      " -8.98200183e-03  1.69507196e-02 -3.13989371e-03  3.51884776e-02\n",
      "  5.31411425e-04 -1.70520217e-02  1.72302968e-02  2.91067224e-02\n",
      " -4.87924380e-02  1.70603463e-02 -9.04113277e-03  2.37398740e-02\n",
      "  2.16094451e-02 -2.03105846e-02 -1.26783538e-02 -1.77382040e-02\n",
      "  6.93142038e-03  1.21071088e-02  3.11284512e-02  1.11409388e-04\n",
      "  2.35138005e-02 -6.45737712e-03 -1.35776106e-02 -2.52813578e-02\n",
      "  2.75030078e-02 -3.90943375e-03  1.82037969e-02  2.47192063e-02\n",
      " -3.16709103e-03  1.12752929e-02  1.19773216e-02  1.51106504e-02\n",
      "  2.96699954e-03 -7.04511775e-03 -1.14759613e-02  4.28217076e-03\n",
      "  1.54544604e-02 -3.10773159e-03 -1.53856129e-03 -2.92484359e-02\n",
      " -1.45617209e-02 -1.94917484e-02 -2.11595517e-02 -4.39400212e-03\n",
      "  1.00607933e-03  3.59521601e-02 -9.71309930e-03 -7.10765915e-03\n",
      "  3.40277511e-03 -1.57342808e-02  1.14436150e-02  3.43157182e-03\n",
      " -4.50561857e-02 -3.06803372e-02 -2.35855318e-02  2.96331255e-02\n",
      " -4.75890820e-02  1.33034212e-02  1.34322540e-02  3.79864036e-03\n",
      " -7.09114850e-02  2.57080284e-03  2.15807920e-02  2.63322225e-03\n",
      "  1.62843409e-02 -8.13169648e-04 -4.97883243e-03 -1.63655191e-02\n",
      "  1.57087290e-02  3.48429236e-04 -9.90766428e-03  3.45039425e-02\n",
      " -3.49618624e-02  2.31910490e-03 -2.67484175e-02 -1.50167075e-02\n",
      "  9.72913870e-03 -5.99741130e-04  3.44793970e-02 -1.40963411e-02\n",
      " -4.23021232e-02  1.05715209e-02  2.39484434e-02  1.31067227e-02\n",
      " -1.00062580e-02  2.58930359e-02  4.38736332e-03  1.15239665e-03\n",
      " -1.67778662e-02 -1.35326615e-02 -1.61671209e-02  1.27169474e-02\n",
      " -1.57373709e-02 -4.39587508e-02 -2.35788033e-02 -1.52438857e-02\n",
      " -9.20549790e-03  2.14457802e-02  5.99607949e-03  3.54753832e-02\n",
      "  7.38367584e-03  3.53231266e-02  2.19563086e-03 -1.76644049e-02\n",
      " -2.34358976e-02 -3.19539432e-02 -3.16727748e-03 -2.60666730e-02\n",
      "  1.04187896e-02  6.54316686e-03 -2.15806935e-02  1.70786995e-02\n",
      "  6.40929876e-03 -1.71191969e-02 -1.80286005e-02 -1.17673996e-02\n",
      "  3.58349366e-03 -1.97848480e-02  7.75766425e-03  6.28296394e-04\n",
      " -5.01651097e-03  1.04896566e-02 -2.28705817e-02 -3.18158926e-02\n",
      " -4.04285821e-02  1.78805346e-02  1.37781407e-02 -2.17163650e-03\n",
      "  1.99427664e-03  4.61516411e-03 -4.24545411e-03  7.40212594e-03\n",
      " -2.79354096e-02  2.23954184e-02 -2.88935959e-02  2.73162619e-03\n",
      "  2.44406233e-03  5.82564366e-03  2.16077352e-02  3.65773635e-02\n",
      " -3.06458748e-02  1.39038584e-02  5.02042473e-02 -1.24600531e-02\n",
      " -2.52875023e-03  5.74353029e-03 -1.17333783e-02 -2.34567253e-02\n",
      "  2.76384274e-02  1.62165904e-02  1.22596632e-02 -2.15056853e-02\n",
      " -3.38408358e-02 -2.21820114e-03 -4.65268073e-03 -9.27578653e-03\n",
      " -4.91129572e-03  1.75276458e-02 -2.47485517e-02  1.32770669e-02\n",
      " -1.59000701e-02  9.50867849e-03  2.58549235e-02  1.29044581e-02\n",
      " -1.11025644e-02 -6.98149504e-03  1.13667270e-02 -7.56893389e-03\n",
      "  1.79443190e-02  3.12732448e-02 -6.80119999e-04  6.77341000e-03\n",
      " -2.71561885e-02  1.31659828e-02  2.28361158e-02 -1.69033301e-02\n",
      "  2.86381639e-03  2.01565548e-02  1.42715663e-02  4.16018025e-02\n",
      "  2.56995723e-02 -4.09661526e-02 -2.22742513e-02  2.41487846e-02\n",
      " -1.73015739e-02 -7.26080641e-03  7.60874691e-03  1.09345702e-03\n",
      " -4.96280493e-03 -2.43308854e-02 -1.77367688e-02  1.00482513e-02\n",
      " -4.79735221e-03  3.01880655e-02 -2.80352818e-02  6.32712961e-03\n",
      "  3.07818125e-02  2.04316024e-02  3.75300787e-02 -8.57007268e-03\n",
      " -3.01497325e-03  3.07437990e-02 -1.53794654e-02 -1.60783530e-02\n",
      "  3.39335847e-03  2.44069678e-02 -2.04558755e-02 -2.48576292e-03\n",
      " -5.11115019e-03  1.43851797e-02  2.36492225e-02 -8.55784393e-04\n",
      "  5.92224002e-02  1.99015037e-02  7.20439444e-03  2.40600427e-02]\n",
      "886007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 39 testing: 26\n",
      "correctsource    27\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    18\n",
      "missed            8\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.5        0.66666667 0.83333333 0.83333333 0.6\n",
      " 0.25      ]\n",
      "Accuracy: 0.6153846153846154\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.75      0.67      0.71        27\n",
      "       missed       0.40      0.50      0.44        12\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        39\n",
      "    macro avg       0.57      0.58      0.58        39\n",
      " weighted avg       0.64      0.62      0.63        39\n",
      "\n",
      "accuracy = 0.5769230769230769\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.71      0.67      0.69        18\n",
      "       missed       0.33      0.38      0.35         8\n",
      "\n",
      "    micro avg       0.58      0.58      0.58        26\n",
      "    macro avg       0.52      0.52      0.52        26\n",
      " weighted avg       0.59      0.58      0.58        26\n",
      "\n",
      "(444,)\n",
      "[-0.00663641  0.01983101  0.00136091  0.03558362  0.00447592 -0.02898282\n",
      " -0.02375654  0.02095591 -0.01711063  0.00085946 -0.00125246  0.01556339\n",
      " -0.0040879  -0.04038578  0.01078185  0.01573313 -0.01075099  0.026988\n",
      " -0.04554034 -0.00199561 -0.01230574  0.02956228 -0.01065983 -0.00281271\n",
      " -0.02901016  0.01323619 -0.02042378 -0.00633044  0.02952684  0.01920787\n",
      " -0.03670525 -0.02721722 -0.01902993 -0.05630486  0.00290245  0.01081415\n",
      "  0.03706353  0.0370515   0.         -0.01135189  0.02002693  0.01777668\n",
      " -0.03706572  0.05939503 -0.00230747  0.02559253  0.02298206  0.03298114\n",
      "  0.05087677  0.01576536  0.00345692 -0.00689627  0.01043986  0.01781995\n",
      " -0.01565215  0.00389742 -0.05552629  0.00957414 -0.04093782  0.01002014\n",
      " -0.00785641 -0.0059448   0.01134279  0.02250463 -0.01783043  0.0209704\n",
      " -0.01968792 -0.00185803  0.00953065 -0.00377588 -0.00348734  0.00010234\n",
      "  0.00550503  0.01783405  0.01530404 -0.02823028 -0.01985522  0.00568248\n",
      "  0.02464561  0.04949619  0.00724144 -0.00180174  0.02112805 -0.02827399\n",
      " -0.0555581  -0.01477243  0.00093807 -0.03044791 -0.01840952  0.00419413\n",
      "  0.00422749 -0.03618252 -0.03028089 -0.00647911  0.02586607  0.00641441\n",
      " -0.0131807  -0.04168517  0.03736863  0.01047923 -0.00294896 -0.01253276\n",
      "  0.01848112  0.01862936  0.         -0.03620286  0.03457251  0.00822793\n",
      " -0.02541272  0.01525466  0.00224181  0.02394985 -0.00551212 -0.03073712\n",
      " -0.01704922 -0.0519848  -0.00264226  0.02242977  0.00556007  0.01457236\n",
      " -0.01492808  0.00442363  0.01247634 -0.0075986   0.02569821 -0.0171169\n",
      " -0.04613812 -0.004231    0.01532641  0.00761665 -0.00080564  0.00589451\n",
      " -0.01360341 -0.00410333 -0.02573892 -0.0157063  -0.00837339  0.01869626\n",
      "  0.00080292  0.00556164  0.01987791  0.02545183  0.00090514 -0.0067848\n",
      "  0.00290631  0.03624668 -0.00103298 -0.01733364  0.01842794  0.00285055\n",
      "  0.01160202  0.00913723 -0.05816884  0.00851753  0.02407685  0.03878248\n",
      " -0.00075005  0.03002031  0.01175388 -0.0002759  -0.03272817 -0.02949201\n",
      "  0.00490229  0.01220091 -0.01710315  0.01494594 -0.01986605 -0.01948406\n",
      "  0.00384078  0.02802293 -0.00351564  0.01022604 -0.00882627  0.0221721\n",
      "  0.04369495  0.00111393  0.00426642  0.02961183 -0.01952726  0.01906242\n",
      " -0.04412778  0.03884379 -0.01074408 -0.01469734  0.01755465  0.01716926\n",
      "  0.03431885 -0.00293325  0.01342037 -0.01991643  0.0093072  -0.00842417\n",
      " -0.01401369  0.01903657  0.04110446 -0.02682581  0.00018878 -0.02985138\n",
      " -0.0189012   0.01030872  0.02865257 -0.06007943 -0.01550678 -0.00051748\n",
      "  0.02184523  0.00586099 -0.00434633 -0.02298387  0.02961684  0.02593987\n",
      " -0.0055808   0.06751421  0.00886425  0.03571104 -0.03364011 -0.00115979\n",
      "  0.04604758  0.02758201 -0.01306155  0.00154451 -0.01778891 -0.0032673\n",
      "  0.          0.02279861  0.01357378 -0.03261585  0.01792221  0.0465633\n",
      " -0.0224895  -0.00418556 -0.0055892   0.01863474  0.01621014 -0.00563602\n",
      "  0.01410055 -0.00067248 -0.01220689  0.00709326 -0.0080816  -0.00901531\n",
      " -0.05197352  0.01177095  0.01261668 -0.01830091 -0.01989198 -0.01681368\n",
      "  0.0042434  -0.00025791 -0.01860391  0.01206335  0.00693787  0.01871156\n",
      "  0.01884167  0.00990634 -0.02105906 -0.04216125 -0.00680074 -0.01592029\n",
      " -0.05698573  0.03741117 -0.01923928  0.00770205  0.01816561 -0.03446414\n",
      "  0.04958888 -0.01282124 -0.01426896 -0.00544696  0.05113897 -0.00469276\n",
      " -0.01074788  0.00189919  0.02072167 -0.01212683 -0.01568323  0.01661252\n",
      "  0.00868463  0.00681507  0.01006424  0.01622917  0.00274599 -0.01467026\n",
      "  0.0220949  -0.00510592 -0.00139695  0.          0.00541417  0.01465389\n",
      "  0.01287606 -0.01607636  0.01182277 -0.01737996 -0.02094623 -0.04570834\n",
      " -0.01560954  0.00192708 -0.03605402  0.01373731  0.00959075 -0.00324477\n",
      "  0.00674436 -0.00869277  0.02882667  0.01497268 -0.00941646 -0.02837924\n",
      " -0.0193238  -0.00900104 -0.01186148  0.00917657  0.0415451   0.02326382\n",
      "  0.02350079 -0.00949261 -0.01541369 -0.01256372  0.01281741 -0.02914693\n",
      " -0.01386984  0.01453343  0.00573459  0.02324576 -0.01230661 -0.02465554\n",
      "  0.02414323  0.01068856 -0.04185428 -0.02210945 -0.00885605  0.05570012\n",
      "  0.02260652 -0.01625138 -0.03263625  0.02956367  0.00631751 -0.00149056\n",
      " -0.01171141  0.01203081  0.00335877 -0.02796544  0.01123144  0.01996794\n",
      "  0.02627662  0.02727999  0.03450431  0.02671469  0.01340841  0.01017599\n",
      " -0.02129219  0.05561839  0.00663984 -0.00325621  0.00423189 -0.00780407\n",
      " -0.01821653  0.00480589 -0.06376586  0.01333143  0.01174517 -0.04019652\n",
      " -0.00609443 -0.01682957  0.01682055  0.01059837 -0.00544774  0.01988542\n",
      "  0.0156665  -0.00492492  0.03063956  0.         -0.0186536   0.02715419\n",
      " -0.00027201 -0.01570349 -0.00356697  0.03821909 -0.03675001  0.04224982\n",
      " -0.00664744 -0.00869758 -0.00720827 -0.02287959  0.00573694  0.02676308\n",
      " -0.01324257 -0.00950746  0.02506852 -0.00466627 -0.01255292 -0.01241274\n",
      "  0.00973794  0.00018267 -0.00992783 -0.00605656  0.01026582 -0.02900847\n",
      "  0.00067928 -0.00863096  0.01047058 -0.06422615 -0.01183817  0.00052842\n",
      "  0.00378397  0.03445307  0.00525409  0.00871563 -0.00704453  0.01760394\n",
      "  0.01876198 -0.0272137   0.03055366 -0.00091262 -0.02780012  0.01241794\n",
      "  0.00582996  0.00768153  0.02572461  0.02187225  0.0087838  -0.02199911\n",
      " -0.00582959  0.01716772 -0.01387516 -0.01955836  0.01146098 -0.027606\n",
      "  0.0282972   0.00796497  0.02014434  0.01184246  0.0247574  -0.02812715\n",
      "  0.00053925 -0.00336272  0.03260092 -0.04953067 -0.01761993 -0.0033544\n",
      " -0.03623584  0.04442061  0.01661545 -0.05774593  0.01054636 -0.02308672]\n",
      "936730\n",
      "training: 36 testing: 24\n",
      "missed           18\n",
      "correctsource    18\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           12\n",
      "correctsource    12\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.5        0.33333333 0.5        0.16666667 0.5        0.75\n",
      " 0.25      ]\n",
      "Accuracy: 0.4166666666666667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.41      0.39      0.40        18\n",
      "       missed       0.42      0.44      0.43        18\n",
      "\n",
      "    micro avg       0.42      0.42      0.42        36\n",
      "    macro avg       0.42      0.42      0.42        36\n",
      " weighted avg       0.42      0.42      0.42        36\n",
      "\n",
      "accuracy = 0.625\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.67      0.64        12\n",
      "       missed       0.64      0.58      0.61        12\n",
      "\n",
      "    micro avg       0.62      0.62      0.62        24\n",
      "    macro avg       0.63      0.62      0.62        24\n",
      " weighted avg       0.63      0.62      0.62        24\n",
      "\n",
      "(444,)\n",
      "[-1.38494879e-01 -2.12575637e-02 -1.75003100e-02 -9.83273277e-02\n",
      " -1.43432648e-01 -2.65645717e-02 -1.63685179e-01  1.05839751e-01\n",
      " -6.16038023e-02  3.36653296e-02  1.35930196e-02 -9.06034431e-02\n",
      " -1.63213656e-02 -1.82010798e-02  2.42908322e-01  7.47636687e-02\n",
      "  6.24713942e-03 -8.33181536e-03  5.21038066e-03  4.87283132e-02\n",
      " -5.22809532e-02 -3.80760573e-02 -8.25787916e-02  4.51164830e-02\n",
      "  6.92993818e-02 -1.14201665e+00 -3.22223173e-01  4.05745249e-02\n",
      " -4.36024206e-03 -2.81107668e-03  1.04192195e-02  1.17347136e-03\n",
      " -5.48591550e-02 -2.02802015e-02  3.20611639e-02  7.94880023e-02\n",
      "  2.70472037e-02 -1.11246375e-01 -4.14260490e-03  1.78549665e-02\n",
      "  0.00000000e+00 -1.37221247e-02  1.30673513e-02 -3.10979382e-02\n",
      "  4.85721482e-03 -3.29233431e-02  2.79378084e-02 -1.06336036e-01\n",
      "  3.65188832e-02 -5.00113601e-03 -1.11338889e-01 -2.58028625e-01\n",
      "  3.78787809e-02 -6.78280549e-03  1.69939584e-02 -4.53327773e-01\n",
      "  1.08576961e-01 -6.22313124e-02 -2.53146805e-02 -7.27034554e-02\n",
      "  2.92041945e-02 -1.52805619e-01  3.07530493e-01  2.56043676e-02\n",
      "  2.80943485e-01 -1.38378903e-02  5.36497091e-01  1.62157560e-02\n",
      " -2.41982475e-02 -4.19141088e-04  3.98805572e-02  5.60113131e-01\n",
      " -3.28516305e-02  4.87875809e-02 -3.05405238e-02 -2.07681995e-02\n",
      " -2.72388711e-01 -1.00045037e-02  2.23264110e-03  8.01644755e-03\n",
      "  4.66399734e-01  6.04880252e-03 -1.56983752e-02 -3.97137001e-01\n",
      " -5.19568486e-03  4.31118925e-02 -3.57401237e-01 -5.74271343e-02\n",
      "  7.08318279e-02  6.00970681e-02 -3.40948147e-01 -1.50290801e-01\n",
      " -3.59655436e-03  4.03126368e-04  5.31867625e-02 -9.44737537e-02\n",
      "  4.25785942e-04 -3.53074195e-04  4.26076468e-01  4.49491429e-02\n",
      " -1.96381415e-02 -1.33912190e-01 -2.29843918e-02  2.02421898e-02\n",
      "  6.23207613e-02 -7.24967979e-02  2.40168223e-01  1.16274159e-02\n",
      " -2.90524361e-02 -4.24874821e-02  2.35714650e-01  1.78507815e-02\n",
      " -8.30880693e-03 -1.01218164e-01  2.08539241e-01 -1.40672297e-02\n",
      "  4.92166367e-02  4.28392933e-04 -2.70174773e-01 -1.68361252e-02\n",
      " -6.39895000e-03  1.33533442e-01 -1.04671917e-01 -3.29514230e-02\n",
      " -9.04117023e-03 -1.84992565e-01 -6.94858183e-03 -3.37351107e-01\n",
      "  3.64435986e-02 -6.80713813e-03 -8.24950389e-01  1.51268342e-02\n",
      "  2.80799691e-03 -4.89265381e-02 -2.95965005e-02 -7.38570269e-03\n",
      "  1.33152567e-01  3.42320985e-02 -1.25579368e-03 -1.17611138e-02\n",
      " -2.03419882e-02  4.83891443e-02  1.02389013e-02 -1.60890690e-02\n",
      "  2.02916420e-01 -7.90506563e-02  3.00334615e-02  3.61730185e-01\n",
      " -8.67735865e-03 -9.94169481e-03  1.80624919e-02  1.12307425e-01\n",
      " -1.69835332e-01 -3.31277661e-02  2.34892898e-02 -2.84834634e-02\n",
      "  1.60319341e-02 -2.69873390e-02  1.88369566e-01 -1.57289563e-01\n",
      " -2.96101828e-02 -6.58719350e-02 -8.96371299e-02  5.03396437e-02\n",
      "  3.41992105e-02 -9.64373789e-02 -1.15827728e-01  7.45662566e-03\n",
      "  2.67316997e-02 -1.10838888e-02 -5.67863553e-03 -3.51634013e-01\n",
      "  2.11649820e-02  2.55514932e-04 -3.95708691e-02  6.18174944e-02\n",
      "  1.41492274e-02  2.00604464e-01  2.29702618e-01 -4.22578677e-02\n",
      "  2.31144117e-02 -4.46117554e-02 -3.91470978e-02 -8.75869041e-02\n",
      " -7.24710037e-03 -1.31143678e-01  4.47226792e-01 -5.48631256e-02\n",
      "  4.05926313e-02 -1.78463977e-02  5.89627560e-02 -8.11300803e-03\n",
      "  5.49123312e-02 -4.48446988e-02 -1.92691040e-01 -3.05992069e-02\n",
      "  6.35445962e-03 -1.40587730e-02 -3.29694261e-02 -2.53920206e-02\n",
      "  8.26144585e-02  9.42654655e-03  4.32769890e-02  5.04062957e-02\n",
      "  2.18619317e-02 -4.52588516e-01 -5.26719665e-02  1.49765088e-01\n",
      "  2.89810682e-02  1.19862209e-02 -3.28550719e-02 -6.19618178e-02\n",
      " -3.88102153e-02  4.40150855e-02 -2.54628567e-03  5.87802000e-02\n",
      " -3.46361956e-02 -8.36148330e-01  2.06112972e-01  1.48825364e-01\n",
      " -2.87963610e-02 -8.35268089e-03  1.24739629e-02  5.75704250e-02\n",
      " -3.37172892e-03 -8.90763695e-01 -8.23900353e-03 -2.12640811e-02\n",
      " -7.03953844e-03  9.13338767e-02 -2.72549675e-02  9.79103638e-02\n",
      " -1.38995523e-02 -5.83824836e-02  5.06304018e-02  5.05318420e-03\n",
      "  2.22129716e-01  5.55763126e-03 -7.93471775e-02  1.70791237e-02\n",
      " -5.07761815e-02 -2.24410372e-01  3.55514560e-02 -9.40374641e-03\n",
      "  2.15160621e-01 -1.35894017e-02  3.43228614e-03  9.05157083e-03\n",
      "  4.11003702e-03  4.77785820e-02  4.50043275e-01  8.87216935e-02\n",
      " -4.97754953e-02  1.81075136e-02  5.61024007e-01  4.85377663e-03\n",
      "  4.08159837e-02 -9.98341590e-02  3.54455980e-02 -9.69033506e-02\n",
      "  1.40571645e-01  2.35789786e-02 -2.80685019e-03  2.34840681e-01\n",
      " -2.39621329e-02 -1.07236019e-01  9.48095520e-02 -1.85256622e-02\n",
      " -1.12235830e-01  4.86557077e-02  1.68801414e-01  2.98937428e-01\n",
      "  4.34790603e-02 -1.48464443e-02 -1.23966721e-02 -1.27533761e-01\n",
      "  2.37345156e-02  6.94818395e-02 -7.29063731e-02  4.21861812e-03\n",
      "  1.02833534e-01  4.45138711e-02 -3.07624954e-02  1.60045089e-01\n",
      "  1.07421319e-02  4.91485279e-02 -8.09987732e-03 -8.99107215e-02\n",
      "  1.81297949e-02  1.83050099e-02  5.45119531e-02  1.18588348e-02\n",
      " -3.44932715e-02 -3.51558489e-02 -5.22933323e-02  2.03671271e-01\n",
      " -8.68225448e-02  3.53075523e-03 -7.27667460e-03 -2.50890925e-02\n",
      " -4.95937448e-02  4.27544229e-02 -4.26199900e-02  5.91834578e-02\n",
      " -1.74186925e-02 -2.65496333e-02 -5.09966538e-02 -1.05806721e-01\n",
      " -1.15484070e-02  9.89040382e-03  3.16203220e-02 -7.34682143e-02\n",
      " -8.91167303e-01 -1.99204440e-03  1.38022510e-02  1.00375083e+00\n",
      "  2.79738105e-03 -2.33643399e-03  7.36350765e-02  4.50095250e-01\n",
      " -7.67840124e-02  1.55870515e-02 -6.59728397e-02 -2.00892895e-02\n",
      "  2.25129977e-02  6.98205258e-02  4.98695455e-01  6.06504450e-03\n",
      " -3.29214336e-03  7.50026779e-02 -2.24581289e-02  2.85133631e-02\n",
      " -1.26959723e-01  1.05615848e-02 -7.38060130e-02 -4.23042290e-02\n",
      "  2.23766804e-02  1.33157573e-01 -6.05115239e-02 -2.64888297e-02\n",
      "  1.07005259e-02  3.03640993e-02  3.42799173e-03  7.62741556e-02\n",
      " -1.11071269e-02 -3.26130329e-02  3.56251255e-02 -5.99713965e-02\n",
      " -5.36093799e-02 -1.10202059e-02 -1.17179330e-01 -7.70275152e-02\n",
      " -3.27288610e-01 -5.38474711e-02  1.89008300e-02 -3.37854902e-02\n",
      " -3.52623949e-02  1.24650668e-02 -8.57292796e-02  2.07293017e-02\n",
      "  5.18541568e-02  1.05689260e-02  1.52238947e-02 -3.11704166e-02\n",
      "  3.90204131e-01 -4.99172331e-03  1.90589410e-02  1.65254864e-01\n",
      " -4.26894052e-01 -5.66433035e-02 -1.12425525e-01  1.21559580e-01\n",
      "  1.16809726e-01  9.60318935e-01 -1.71004427e-02 -2.99305434e-02\n",
      "  2.64970505e-02  2.92398320e-01 -1.16162652e-02  2.60531550e-02\n",
      " -2.39624154e-02  1.43670261e-01  1.27026419e-01  5.77947771e-02\n",
      "  2.63093944e-02  2.79665811e-01 -2.63211501e-02 -3.44200773e-02\n",
      " -3.16091247e-01 -4.92279667e-02  5.48333313e-02  1.21291538e-01\n",
      " -1.27816063e-02  1.91113634e-01 -1.59712512e-02 -2.30591364e-03\n",
      "  5.40054571e-02 -7.46623697e-03 -7.99872891e-02 -5.12804302e-02\n",
      "  6.29447415e-02  3.29298599e-01 -1.17595938e-01 -1.02600060e-02\n",
      " -3.38131855e-02  2.73508476e-01 -1.69241324e-01 -2.55678984e-02\n",
      "  2.29220992e-01 -6.98887347e-02  3.16657285e-03  7.32207533e-03\n",
      "  5.15863808e-02  2.36767715e-02  5.49479452e-04  1.45404206e-02\n",
      "  1.25387232e-02 -2.92515408e-03 -2.94071178e-02  1.99435783e-02\n",
      " -3.87957772e-02 -3.01995216e-02 -1.83333637e-02  8.71802469e-01\n",
      "  1.35528782e-01 -2.55571109e-02  3.67383808e-01  3.93011008e-02\n",
      " -2.61405759e-01  5.54747305e-02  6.47446990e-04  4.81980950e-03\n",
      " -1.21946443e-03 -1.20830166e-02 -3.76582826e-01 -1.19115958e-01\n",
      "  7.56725196e-01  1.31521296e-03  1.80608698e-02 -2.77913521e-02\n",
      " -1.44342832e-03 -1.22624148e-02  1.40435682e-02  3.00781519e-04]\n",
      "956130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 38 testing: 26\n",
      "missed           20\n",
      "correctsource    18\n",
      "Name: ctl_miss_ws_cs, dtype: int64 missed           13\n",
      "correctsource    13\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.33333333 0.66666667 0.66666667 0.5        0.8        0.6\n",
      " 1.        ]\n",
      "Accuracy: 0.631578947368421\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.62      0.56      0.59        18\n",
      "       missed       0.64      0.70      0.67        20\n",
      "\n",
      "    micro avg       0.63      0.63      0.63        38\n",
      "    macro avg       0.63      0.63      0.63        38\n",
      " weighted avg       0.63      0.63      0.63        38\n",
      "\n",
      "accuracy = 0.5\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.23      0.32        13\n",
      "       missed       0.50      0.77      0.61        13\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        26\n",
      "    macro avg       0.50      0.50      0.46        26\n",
      " weighted avg       0.50      0.50      0.46        26\n",
      "\n",
      "(444,)\n",
      "[-1.29707726e-02  5.26494937e-03  3.56427047e-02 -2.40418179e-02\n",
      "  2.58689092e-02  1.99396417e-02 -9.72042170e-03 -5.74195450e-02\n",
      " -2.57377129e-03 -9.59676889e-04  2.36348035e-02 -8.40600365e-03\n",
      " -9.08157661e-04 -2.36740223e-02  1.92864661e-02  1.46559076e-02\n",
      " -1.34989639e-02  3.72185260e-03  2.49567683e-02  2.34981212e-02\n",
      " -4.66147477e-03 -2.27303834e-03 -1.19285318e-02 -9.03996351e-03\n",
      " -8.54971469e-03  1.67876558e-02  8.84065570e-03  2.76297473e-02\n",
      " -2.67773456e-02 -3.67290636e-02  9.78663932e-03 -2.58456827e-02\n",
      " -2.97902031e-02 -9.09002079e-04 -2.11155015e-02 -1.75006063e-02\n",
      " -2.56257878e-02 -1.74837602e-02 -2.28014695e-02 -2.31123047e-02\n",
      " -6.61176002e-03 -1.15647667e-02 -3.63588365e-03  1.67329794e-03\n",
      " -3.12895281e-02 -1.65225725e-02 -2.59394833e-02 -2.09953156e-02\n",
      "  2.76554965e-02  1.53342229e-03 -3.36951192e-02 -1.56076858e-02\n",
      "  2.86301255e-02 -1.40532924e-02  1.55981879e-02  2.01207137e-03\n",
      "  1.29642237e-02  5.33304606e-03 -3.05585625e-02 -2.47715951e-03\n",
      " -2.07576521e-02 -2.33430003e-02  4.27708339e-03  1.01204039e-02\n",
      " -3.15581585e-02  8.85767618e-03 -2.61908535e-02 -4.85211169e-02\n",
      "  2.02100480e-02 -6.10592232e-02 -2.73600987e-02  1.39112085e-02\n",
      "  1.97947694e-02  4.51443403e-02 -3.47196478e-02  2.54413768e-02\n",
      "  1.41255657e-02 -2.43155422e-02  9.68597501e-03  8.33307126e-03\n",
      " -1.30960634e-02  1.54612108e-02 -2.97279736e-02  1.34015395e-02\n",
      " -3.00728607e-02  2.40980845e-02 -7.20424698e-02  9.51253879e-03\n",
      " -2.68038520e-03  3.22525794e-02  3.26338215e-02  2.33415495e-02\n",
      "  8.34723883e-04 -2.14520466e-02  2.81086975e-03  1.07121854e-02\n",
      "  1.53707154e-02  1.63379822e-02 -1.10050397e-02 -3.36349228e-02\n",
      " -1.97415369e-02 -1.67638982e-02 -2.58030855e-02  7.02714712e-03\n",
      "  2.32692823e-02 -1.98263237e-02 -4.03702734e-02  1.03151867e-03\n",
      " -1.73467823e-02  1.23310499e-03 -1.08889887e-02 -5.88121521e-02\n",
      "  3.47331752e-03  3.29733227e-02  1.93126701e-02  1.29126540e-02\n",
      " -5.05684315e-03  1.28215589e-02 -2.70971659e-02 -3.27331497e-02\n",
      " -4.71879459e-03  1.51390776e-02 -1.18804679e-02  3.11438998e-02\n",
      "  8.86336400e-03 -2.01910874e-02 -2.06343985e-02  1.84915562e-02\n",
      "  4.05703543e-02 -1.79957510e-02  1.46547984e-02 -1.90843215e-02\n",
      " -1.07025655e-02  6.38009274e-03 -2.96390224e-02 -4.71656090e-02\n",
      "  5.68938672e-03 -5.71558098e-03  3.37889240e-02  8.57715011e-03\n",
      "  2.93348156e-03 -3.60222390e-02 -5.46729713e-03  2.44209151e-02\n",
      " -7.19350000e-03 -9.64031466e-03 -2.86294099e-02  1.68608617e-02\n",
      "  1.88219132e-02 -5.93333077e-03 -2.04736184e-02  3.85286101e-03\n",
      " -3.07995930e-02  4.45697337e-02  1.74261436e-02  3.50355442e-02\n",
      "  1.15842227e-02  1.10211969e-02  3.58092230e-02  5.82359366e-04\n",
      "  1.14832348e-02 -1.52268220e-02 -2.40216815e-02  2.85265218e-03\n",
      "  3.00278680e-02  2.51509124e-02 -1.62044236e-02 -1.80890967e-02\n",
      " -1.72731859e-02  1.35889743e-02  1.40141170e-02 -5.52452649e-03\n",
      " -5.54421952e-03 -8.35908844e-03  3.02333925e-02  2.38550518e-03\n",
      "  5.98622351e-03  2.41092497e-02 -1.98090956e-02 -1.64254217e-02\n",
      "  1.50043027e-02 -4.90504969e-03  4.75671163e-03 -2.21254299e-02\n",
      "  1.86129287e-02  5.63276597e-03  3.31118075e-02  1.43078254e-03\n",
      "  1.00824025e-02 -8.71393203e-03  1.68861542e-02 -8.87029068e-03\n",
      "  4.42753264e-02  2.40927459e-03  6.90962222e-03  5.39996227e-03\n",
      " -2.45974635e-02  1.41147859e-02  3.78613740e-02  1.85952498e-02\n",
      "  3.85210706e-02  5.05372026e-03 -3.09226468e-03  1.45849159e-02\n",
      " -7.16667505e-03 -1.78427895e-02 -2.10035678e-02 -3.03990605e-02\n",
      "  4.76140200e-02  1.76575512e-02  1.74917472e-03  8.04699820e-03\n",
      "  1.91646705e-02  4.62691902e-02  4.83170864e-02 -6.47999447e-02\n",
      "  4.58855155e-03 -1.34021266e-03 -1.59911646e-02  1.39544499e-02\n",
      " -3.37310651e-02 -1.52394044e-02 -1.15814595e-02  1.82318021e-02\n",
      "  9.66494266e-03 -8.00334210e-03 -2.59783203e-04 -5.30134554e-03\n",
      " -9.24741422e-03 -1.19593813e-02 -1.89968892e-02  2.06068059e-02\n",
      "  1.93806678e-02  4.76133917e-03 -4.84467329e-02  2.18143134e-02\n",
      "  6.00880015e-03 -1.72913770e-02  1.21066354e-02  3.85718393e-04\n",
      " -1.70406456e-02 -8.47287236e-03 -1.73649239e-02  1.53506969e-02\n",
      "  8.15549735e-03 -1.13495336e-02 -5.78401666e-02  2.47861099e-02\n",
      "  1.06138615e-02 -2.06048400e-02  9.35896914e-03 -6.85450736e-03\n",
      " -7.68249452e-03 -3.05354105e-03 -6.08524066e-03 -1.27151545e-02\n",
      "  1.63051116e-02  1.68145859e-04  2.77470575e-02  7.83580028e-03\n",
      "  4.99754536e-03  9.83187097e-03  7.80353807e-03  5.56123581e-03\n",
      " -4.12422370e-03  1.59979601e-02 -4.52186176e-03 -7.14624857e-03\n",
      "  1.31091306e-02  8.79451098e-03 -5.34724532e-03 -2.85524210e-02\n",
      " -1.48516931e-02  1.05489826e-02  5.69323347e-03 -1.00728365e-02\n",
      "  1.67971207e-02 -2.73607860e-04  2.31753097e-02 -2.82019269e-02\n",
      " -3.82550319e-04  7.53544020e-03 -1.18242887e-02 -2.28608321e-02\n",
      "  4.95794369e-03 -1.29242480e-03  2.84277807e-02 -2.01976038e-03\n",
      "  2.09642616e-02 -3.65649912e-03  5.47104718e-03 -4.89242539e-05\n",
      "  1.17755882e-02 -2.89757394e-02  4.66459923e-03 -2.68006782e-03\n",
      "  1.30900676e-02 -2.69312094e-02  2.35497062e-02 -4.56810808e-02\n",
      "  2.10508910e-03 -2.31815127e-02  8.44844745e-03  4.34716689e-03\n",
      "  1.71644160e-02  1.90947614e-02  1.03614048e-02  3.54430599e-02\n",
      "  6.76645212e-03 -1.90193984e-03 -1.61191841e-02 -1.65695898e-02\n",
      "  1.42461411e-02  1.98270139e-02 -1.57888600e-02  1.78167258e-02\n",
      " -7.28800515e-03  8.32489001e-03  2.29506169e-03 -2.57151660e-02\n",
      " -1.35552912e-02 -2.27295283e-03 -7.61450885e-03  1.56212062e-02\n",
      "  6.33643637e-03 -1.33831416e-02 -9.74961628e-03  1.33310223e-02\n",
      " -1.45246934e-02  4.01746100e-02 -6.65365812e-03 -2.82083740e-02\n",
      "  9.58949864e-03  2.65408706e-03 -2.26154444e-02 -2.12521304e-02\n",
      " -5.33531901e-02  1.11623926e-02  2.54808380e-02 -2.33163200e-02\n",
      " -1.10452323e-02  3.87771160e-03 -5.55836541e-02  1.73155154e-02\n",
      "  1.57172182e-02  1.03987747e-03 -2.02214544e-03 -1.23297020e-02\n",
      " -8.33517241e-03 -3.25859628e-02  1.84994135e-02  2.08557995e-03\n",
      "  2.70917974e-02 -1.33132046e-02  2.40853272e-02 -1.39708351e-02\n",
      "  2.91444211e-03 -6.79422934e-03 -3.61703132e-02  3.77528330e-03\n",
      " -3.01311795e-02  2.54635112e-02  3.60356428e-02  1.59300494e-02\n",
      "  4.46540866e-02  4.33502329e-03  3.39906769e-02  2.44400789e-02\n",
      " -1.57026121e-02 -1.83476264e-02 -1.40511661e-02  1.49673715e-02\n",
      " -1.59291031e-03 -2.83483957e-02 -1.59003373e-02  2.87088875e-02\n",
      "  3.23233369e-02  4.48735016e-02  1.74557706e-02  1.95478752e-02\n",
      "  3.96290526e-03 -3.61183703e-02  9.63210451e-03  5.30433602e-03\n",
      " -5.44921090e-03  2.67750688e-02  2.61626839e-02 -1.58608967e-02\n",
      " -3.46612930e-02  6.38316211e-03  1.85413554e-02 -6.73897581e-03\n",
      "  9.42922249e-03  1.95759981e-02  1.07961261e-02  1.74159973e-02\n",
      " -3.50648011e-02 -4.89089013e-02  5.66536738e-02  1.38878415e-02\n",
      "  2.50169879e-02 -2.48195600e-02 -1.85397586e-02 -2.38134144e-02\n",
      " -8.25685754e-03 -1.49960506e-02 -8.18992043e-04  1.33226791e-02\n",
      "  2.51663436e-02  1.96796901e-02 -1.12836912e-02 -6.74906034e-04\n",
      "  2.45311846e-02 -2.89043033e-02  1.10704948e-03 -2.00385637e-03\n",
      " -1.16797407e-03 -2.02713858e-02 -7.69619228e-03 -2.25282173e-03\n",
      " -2.48288149e-02 -5.05222745e-02  1.11371548e-02 -4.75789631e-02\n",
      "  2.30291469e-02  2.05707327e-02 -1.46008179e-02 -2.06523679e-02\n",
      "  8.00795211e-03  1.28931827e-02  2.39643105e-02 -3.11899507e-02\n",
      "  2.93697483e-02 -1.72503818e-02  2.69836544e-02  8.67199728e-03\n",
      " -1.77419738e-02  1.51493780e-03  1.41942216e-02  2.57347267e-02\n",
      " -9.80505425e-03  2.65988663e-02 -3.40260876e-02 -3.10508302e-02]\n",
      "983291\n",
      "training: 40 testing: 28\n",
      "correctsource    29\n",
      "missed           11\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    21\n",
      "missed            7\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.71428571 0.66666667 0.5        0.33333333 0.8        0.6\n",
      " 0.6       ]\n",
      "Accuracy: 0.6\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.74      0.69      0.71        29\n",
      "       missed       0.31      0.36      0.33        11\n",
      "\n",
      "    micro avg       0.60      0.60      0.60        40\n",
      "    macro avg       0.52      0.53      0.52        40\n",
      " weighted avg       0.62      0.60      0.61        40\n",
      "\n",
      "accuracy = 0.6071428571428571\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.75      0.71      0.73        21\n",
      "       missed       0.25      0.29      0.27         7\n",
      "\n",
      "    micro avg       0.61      0.61      0.61        28\n",
      "    macro avg       0.50      0.50      0.50        28\n",
      " weighted avg       0.62      0.61      0.62        28\n",
      "\n",
      "(444,)\n",
      "[-2.40949425e-02  1.35969387e-02  1.03056334e-02  3.59882440e-02\n",
      "  1.82608964e-02 -9.48567359e-03  4.82395001e-03  3.74370499e-02\n",
      "  2.37716892e-02  2.32835329e-02 -3.67991164e-03 -1.63722690e-02\n",
      "  1.91328288e-02  1.85483118e-03  1.93800310e-02  8.02151156e-03\n",
      " -2.21722838e-03 -5.78758984e-03 -1.34235242e-02 -1.50440631e-02\n",
      "  1.87862408e-02 -1.35808471e-02 -3.76930035e-02  3.56178894e-02\n",
      "  1.62465096e-02  5.85840971e-03 -3.68526149e-02 -9.50776209e-03\n",
      "  5.12376068e-02  9.73424350e-03 -1.47041369e-02 -4.78387770e-02\n",
      " -7.35065248e-03  1.55037778e-03 -2.79675065e-02 -5.46884253e-03\n",
      " -1.81354193e-02 -2.90281488e-02 -3.78350556e-02  1.12592098e-02\n",
      "  0.00000000e+00 -7.63104713e-03 -1.35799639e-02  2.15169493e-02\n",
      "  1.22780152e-02  1.15316116e-05 -3.24368510e-03  2.40299827e-02\n",
      "  1.53773501e-02 -9.73093908e-03 -2.24639197e-02 -5.84201989e-04\n",
      " -1.52492535e-02 -3.94059730e-02 -5.78188392e-03 -4.27868502e-03\n",
      "  1.49226919e-03 -1.77476431e-02 -4.07179423e-03 -2.43431461e-02\n",
      "  7.21447040e-03 -9.25357122e-03  2.41433097e-02 -1.44908535e-02\n",
      " -3.47853100e-04  1.98472133e-02 -1.99872618e-03 -2.67714771e-02\n",
      "  1.64853706e-03 -1.90116403e-02 -8.60108468e-03 -6.54188441e-03\n",
      "  1.49317904e-02  7.56724398e-03  9.86954157e-03 -1.12604082e-02\n",
      "  1.60044591e-04  1.71920161e-02  1.30078618e-02 -5.26164347e-03\n",
      "  8.98172391e-03 -1.16291802e-02  2.28474423e-02 -1.80882036e-02\n",
      " -2.86186799e-02 -1.78202298e-02 -4.47865031e-02  2.15755635e-02\n",
      " -1.81496883e-02  6.11735997e-03 -9.22935824e-03 -7.62411209e-03\n",
      "  1.61761302e-02  1.31201615e-02 -3.53121264e-03 -5.43057550e-03\n",
      "  2.73260948e-02 -3.06972156e-02  1.11816822e-02 -1.57808595e-02\n",
      " -8.15662239e-03 -5.88531194e-03  1.08922425e-02 -1.97668219e-02\n",
      "  1.84157399e-02  1.86548284e-02  1.10183930e-02  8.29669886e-04\n",
      " -2.02771178e-02 -5.82858695e-02 -1.06946757e-02 -7.17639474e-03\n",
      "  1.66255705e-02 -1.16657312e-02  3.13772319e-03 -3.06164411e-04\n",
      "  7.07295571e-03 -3.76952715e-03 -8.03154387e-03 -1.32879247e-02\n",
      " -2.82212515e-02 -5.91868527e-03 -1.95506376e-02 -3.04930829e-02\n",
      "  9.47209569e-03 -2.04753200e-02 -6.44838926e-03  8.10952415e-03\n",
      "  2.38015191e-02  1.62438426e-03  1.98050116e-02  8.05388257e-03\n",
      "  1.78286279e-02 -6.41752238e-03 -2.37701923e-02 -2.57666262e-02\n",
      " -8.71739315e-03  1.54521739e-02 -3.74057283e-03  2.06112078e-02\n",
      " -2.98416923e-02 -1.65146795e-02  2.51257720e-04  6.89323669e-03\n",
      " -2.72825721e-03  2.81089142e-02 -1.06272733e-02  9.23104128e-03\n",
      " -9.87182958e-03  1.65517354e-02 -1.71837762e-02 -2.13984007e-02\n",
      "  7.48183466e-03 -1.91811224e-02 -3.38011800e-03  3.45943366e-03\n",
      " -1.53685841e-02  8.23081251e-03 -1.77393157e-03 -4.91621493e-03\n",
      "  3.87318428e-03  9.81680931e-03 -4.29624247e-02  1.94650926e-02\n",
      " -2.74742701e-02 -9.30952340e-03 -4.13291283e-02 -6.03188322e-02\n",
      " -9.81990768e-03  9.39139216e-03 -3.39548164e-02 -2.66302640e-03\n",
      " -1.89259594e-02 -5.71852903e-04  4.29394675e-02  1.60797564e-02\n",
      " -3.59809044e-03  9.84716975e-03  6.32174584e-03 -2.89870819e-04\n",
      "  1.19400071e-02  1.50761459e-02 -1.92333486e-02 -2.68292323e-03\n",
      "  2.00774457e-02 -3.27663039e-02  3.39404728e-03  1.00985371e-02\n",
      " -1.15015732e-02 -4.45317883e-03  1.06832555e-02  1.93610356e-02\n",
      "  2.85907105e-02 -1.12567480e-02  1.27118752e-02 -1.57343746e-02\n",
      "  1.54155824e-02  1.30071649e-03  4.05300283e-03 -3.51774235e-02\n",
      "  4.20801834e-03  8.09359502e-03  6.77779560e-03  4.62445867e-03\n",
      " -1.25116110e-02 -2.19784841e-02  2.48679646e-02 -3.32905400e-02\n",
      "  8.34802730e-03 -1.52180924e-02  2.23130581e-02  1.34387293e-02\n",
      " -2.84384873e-02 -1.96921103e-02 -3.87794165e-03  1.22040964e-02\n",
      "  1.00212172e-02 -2.35621287e-02  2.47310934e-02  1.04311048e-02\n",
      "  5.43507992e-03 -4.91334251e-03  6.01815938e-03  1.45134751e-02\n",
      "  1.37176747e-02  8.93702991e-03 -1.65080582e-02  5.59662743e-03\n",
      "  1.82913761e-02 -3.41328422e-02 -8.55940722e-03  1.05405023e-02\n",
      " -6.49834291e-03  2.21817077e-02  2.59510470e-02  3.72374617e-02\n",
      " -3.96446165e-03 -2.92789715e-02  2.35284669e-03 -4.18386130e-02\n",
      " -4.20489256e-02 -1.13935751e-02 -1.25533421e-02  8.90082643e-03\n",
      "  1.01063818e-02 -2.39039185e-02 -4.77759303e-03 -8.13868552e-03\n",
      "  1.00684658e-02 -2.04110472e-02  1.85553293e-02  2.38246649e-02\n",
      "  6.33906473e-03 -1.75929366e-02 -2.76930095e-03  1.67511981e-02\n",
      "  1.85189218e-03 -1.54346097e-02 -2.51937533e-02  1.89590905e-02\n",
      "  1.46625024e-02  2.73792597e-02 -3.86548680e-03  1.71491261e-03\n",
      "  1.54575190e-02 -8.17355187e-03 -1.38512716e-02  4.31537037e-02\n",
      " -1.15303169e-02  2.73522618e-02  5.77253296e-03  1.40375490e-02\n",
      " -7.42568026e-03  7.53662838e-03  9.60333366e-03  2.66843827e-03\n",
      " -1.13574393e-02 -1.86499187e-02 -7.20255675e-03  9.41543217e-03\n",
      " -1.17174648e-02  1.08216211e-03 -6.70172077e-03  2.59810300e-02\n",
      "  2.28017701e-02 -1.19423448e-02 -1.31529733e-02  3.19034990e-02\n",
      " -1.56760178e-02  8.16723883e-03  1.24729661e-02 -1.14682507e-02\n",
      " -3.75150202e-02  1.89173418e-02 -4.31314620e-04  1.01997439e-03\n",
      "  5.78525714e-03 -1.31028736e-02  1.42241059e-03  1.11693164e-03\n",
      " -2.19619617e-02  1.51498694e-03  2.20139804e-03  1.04121513e-02\n",
      "  1.03330696e-02 -2.67943538e-02  1.91824672e-02 -5.10578940e-03\n",
      "  2.14562219e-02 -8.30576210e-03 -5.79482434e-03  1.34423607e-02\n",
      "  1.95816881e-02  2.19763898e-02 -2.28248757e-02 -4.17558532e-02\n",
      " -5.65321966e-03 -3.11380164e-02  2.37190425e-03  8.04754525e-04\n",
      "  9.21368313e-04  1.17250615e-02 -2.92652130e-03 -9.37671380e-03\n",
      " -2.07475115e-02  2.68047625e-02  3.06052408e-03 -1.82444289e-02\n",
      " -2.21205418e-02  1.96411005e-02  6.26705475e-03  6.71020341e-03\n",
      " -6.72574307e-02  2.49990753e-02  2.86978611e-02  1.08712856e-02\n",
      "  2.13636102e-02 -3.08719152e-02 -4.50907416e-02  8.52892180e-03\n",
      " -4.70192967e-03  1.22162439e-02  3.17062395e-03  9.03500961e-03\n",
      "  1.22756173e-02  3.51120424e-03 -4.99512069e-03 -9.98009631e-03\n",
      "  2.17460119e-02 -3.35515723e-03 -6.52662039e-03  2.35663473e-02\n",
      " -6.03083286e-02  1.22130404e-03 -1.35275855e-02 -4.20247904e-03\n",
      " -2.09890045e-02  3.33289908e-03  9.16321555e-03  1.74126816e-02\n",
      "  2.60874454e-02  1.06816968e-02  3.09816622e-03  2.07784625e-02\n",
      " -1.61882906e-02 -3.89142217e-03  6.55971147e-02  7.09215995e-03\n",
      " -1.56892893e-02 -1.84388860e-02  3.78233014e-03 -2.42568764e-02\n",
      "  3.03831119e-02  4.28330638e-02  6.62294771e-03  1.69933997e-02\n",
      " -3.80401079e-03  1.44558130e-02  3.30631462e-02 -1.57517245e-02\n",
      "  2.61410206e-02  7.31429086e-03 -1.15368309e-02 -1.88454505e-02\n",
      "  2.05761791e-02  1.01613585e-02 -1.37092239e-02  2.11076209e-02\n",
      " -1.33147104e-02  6.70685355e-03  1.59199071e-03  1.09545163e-02\n",
      "  4.97786746e-03  1.30074620e-02 -1.06053998e-03  2.18762850e-02\n",
      "  1.68536972e-02  1.06894248e-02 -7.08218278e-03 -8.28761868e-03\n",
      " -5.40081360e-02  1.40894591e-02 -1.10961631e-02  1.06428925e-02\n",
      "  2.87120522e-02 -2.22037710e-02  1.44263017e-03 -2.43863777e-02\n",
      "  2.00828390e-02  3.81815932e-03  4.98068326e-03 -1.06358506e-02\n",
      "  1.57442645e-02 -4.18100761e-03 -1.71829500e-02  2.46811388e-02\n",
      "  2.52077108e-02 -1.85054037e-02  2.30441927e-02 -4.67723518e-03\n",
      " -2.00523337e-02  8.04234329e-03  8.34585846e-03  1.91042423e-02\n",
      "  3.04651663e-03  1.25779441e-02  2.47120571e-02  3.03423525e-02\n",
      " -3.27370114e-03 -1.45242272e-02  1.74989927e-02 -4.51559034e-02\n",
      " -1.27350726e-02 -2.90580657e-02 -9.29339162e-03  2.22846068e-03\n",
      " -1.17835726e-02 -1.73661208e-02 -3.39641778e-03  2.15082420e-03\n",
      "  1.75282380e-02  1.99432202e-02  3.43220518e-03 -5.73978864e-03]\n",
      "998166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 37 testing: 26\n",
      "correctsource    21\n",
      "missed           16\n",
      "Name: ctl_miss_ws_cs, dtype: int64 correctsource    14\n",
      "missed           12\n",
      "Name: ctl_miss_ws_cs, dtype: int64\n",
      "[0.16666667 0.5        0.4        0.6        0.6        0.8\n",
      " 0.6       ]\n",
      "Accuracy: 0.5135135135135135\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.56      0.67      0.61        21\n",
      "       missed       0.42      0.31      0.36        16\n",
      "\n",
      "    micro avg       0.51      0.51      0.51        37\n",
      "    macro avg       0.49      0.49      0.48        37\n",
      " weighted avg       0.50      0.51      0.50        37\n",
      "\n",
      "accuracy = 0.46153846153846156\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "correctsource       0.50      0.71      0.59        14\n",
      "       missed       0.33      0.17      0.22        12\n",
      "\n",
      "    micro avg       0.46      0.46      0.46        26\n",
      "    macro avg       0.42      0.44      0.41        26\n",
      " weighted avg       0.42      0.46      0.42        26\n",
      "\n",
      "(444,)\n",
      "[ 1.70457095e-02  2.40797394e-02 -1.35530976e-02  1.36384651e-02\n",
      " -8.02356917e-04  9.57397855e-03 -2.14874088e-02 -2.53632802e-02\n",
      "  3.83769948e-02 -2.41104409e-02  5.92396361e-03 -2.94929125e-02\n",
      " -1.06555687e-02 -1.25899263e-03 -3.57018184e-03 -1.14865642e-02\n",
      "  1.06635261e-02 -1.05596866e-02  4.44155603e-02 -2.04269179e-03\n",
      " -3.73625055e-02 -1.10342162e-02 -5.51894137e-03  9.21681352e-03\n",
      " -3.81965814e-02 -1.61956248e-02  1.69199031e-02 -4.14780312e-02\n",
      " -6.17840008e-03  2.28879584e-02  1.31648577e-02 -1.01813956e-02\n",
      " -1.95776677e-03  6.41877700e-03  5.02954992e-02  1.44196001e-02\n",
      " -3.19276881e-02  6.12814145e-03 -4.59975416e-02  4.95804458e-03\n",
      "  0.00000000e+00 -2.34636853e-02  3.14759969e-03  8.25534961e-04\n",
      "  8.75861435e-03  1.39784417e-02  2.75182551e-02 -6.64786183e-03\n",
      " -3.37272827e-02 -2.22375742e-03  3.12196618e-02 -3.15655495e-04\n",
      "  3.01297473e-02  3.00032363e-02 -1.13520801e-02  1.07049206e-02\n",
      "  2.41858183e-02 -8.02342336e-04  1.68133693e-02  2.95406577e-02\n",
      "  5.99676666e-03  2.18377886e-02 -1.00226867e-02  5.70134316e-02\n",
      "  1.09800098e-02 -7.37311035e-03  6.81110900e-03  3.40753653e-02\n",
      " -2.56110394e-03 -2.21011679e-02  1.96284874e-02  1.24524353e-02\n",
      " -2.67683890e-02  2.34638990e-02 -1.95006124e-02  2.52820312e-02\n",
      " -8.01668226e-03  1.25580269e-02  1.85093680e-02  1.76245335e-02\n",
      "  4.41138528e-03  2.89407900e-02 -1.21239246e-02  2.16364409e-02\n",
      "  3.92666712e-04 -4.86394758e-03 -3.65485302e-02  4.13680614e-03\n",
      "  1.10918345e-02 -2.12527849e-02  3.12449880e-02 -1.55547564e-03\n",
      "  1.67276783e-03 -1.34203709e-02  1.26945418e-02  4.11337405e-04\n",
      " -1.32132650e-02 -2.53634514e-03  3.74473445e-02  5.06966927e-02\n",
      "  1.40915350e-02 -2.62726478e-02 -3.47784793e-02  1.47222595e-02\n",
      "  3.15354599e-02 -5.74388325e-03  1.24516180e-02  1.95086257e-02\n",
      "  2.60981584e-02  1.88754636e-02  4.48143264e-02  2.08758373e-02\n",
      " -3.41212856e-02 -2.78329870e-02 -1.20133879e-02  3.19900486e-03\n",
      " -3.21760017e-03 -1.45505145e-03 -7.18590444e-05 -1.02809628e-02\n",
      " -2.28470628e-02 -1.11981266e-02  7.48785569e-03 -3.80641978e-03\n",
      " -2.44144153e-02 -3.06803712e-03  1.58090228e-02 -3.41014360e-02\n",
      "  2.15240811e-02  6.07056344e-04 -2.95875197e-02 -1.25241716e-03\n",
      " -1.29209285e-02  1.35002762e-02 -2.79051222e-02 -8.30983640e-04\n",
      "  1.58724292e-02  2.88060118e-02  4.82458703e-03  2.96823792e-02\n",
      "  2.81752781e-02 -1.24377098e-02  3.81319788e-03  3.43255896e-02\n",
      " -5.19240180e-03  6.59783799e-03 -3.33939390e-02  1.50197301e-03\n",
      "  1.22573779e-02  5.76870517e-03  4.73299805e-03 -2.98876604e-02\n",
      "  1.96616962e-02 -1.42154201e-02 -5.65173038e-03 -1.58689304e-02\n",
      "  1.67832441e-02  5.35002416e-03 -6.77418451e-04 -9.79188273e-04\n",
      "  4.11243718e-02 -4.97344969e-04  2.03100496e-02 -1.85608434e-02\n",
      " -1.68710418e-02  2.75300811e-02  2.11185051e-03 -2.20578346e-03\n",
      " -1.56690073e-02  2.51420614e-02  7.02643209e-03  1.59740373e-02\n",
      " -8.10664434e-03 -1.99385093e-02  5.49053895e-02 -2.85069493e-02\n",
      " -3.53559953e-02  1.60961660e-03 -2.44669941e-02 -1.37444334e-02\n",
      " -4.13570981e-03 -3.26513066e-02 -1.65320049e-02 -2.25661618e-02\n",
      "  2.01163682e-02  1.00596914e-03  1.15085220e-02 -3.32601209e-02\n",
      "  1.27640434e-02  1.73464860e-02 -2.23526798e-02 -2.08197211e-02\n",
      " -1.14017850e-02 -3.81659658e-02 -1.49942435e-02  4.11953284e-03\n",
      " -2.00159520e-03 -7.45101870e-04  9.65921279e-03 -1.79805205e-02\n",
      "  5.09430264e-02 -2.34762913e-02  3.04875458e-03  1.41902057e-02\n",
      "  2.98870738e-02  1.63016891e-02  1.98968001e-03 -1.28076874e-02\n",
      "  1.61432841e-02  2.36668895e-02 -1.68121201e-03 -3.73798378e-02\n",
      "  6.41678847e-03 -5.55449131e-03  6.84105921e-04 -5.20544836e-02\n",
      " -9.09239568e-03 -5.91197337e-03 -2.87969479e-02  6.93769504e-02\n",
      " -2.97783378e-02 -5.05424784e-03  0.00000000e+00 -1.14465217e-02\n",
      "  5.22024961e-03  1.12818644e-02 -6.45771380e-03  1.87147487e-03\n",
      "  3.32322172e-02 -2.92106430e-02 -5.63906076e-03  1.07937540e-02\n",
      "  1.67202722e-02 -6.24350763e-03 -2.48656683e-02 -2.10627279e-02\n",
      "  4.19141486e-02  3.38259722e-02  3.26906775e-03  1.78432453e-02\n",
      "  1.63371859e-03  1.19469697e-03  1.27588935e-02  3.52405782e-02\n",
      "  9.43753102e-04 -1.64526796e-02 -1.31055814e-02 -2.50689627e-02\n",
      " -1.22291651e-02 -2.62688824e-02 -2.13620391e-02  5.91044969e-02\n",
      " -9.30261806e-03  1.18090626e-02  1.30492207e-02  5.34325802e-02\n",
      " -1.17348617e-02 -1.46990472e-02  3.27155972e-03 -1.54155106e-03\n",
      " -1.52989766e-02  5.54485364e-02  9.51408315e-03 -8.29895011e-03\n",
      " -1.57419902e-02  3.05250714e-03  2.63090731e-02  4.70043103e-04\n",
      "  3.13473487e-02 -1.00350448e-02  1.95943360e-02 -1.92032825e-03\n",
      "  3.07318032e-02 -4.28172226e-02  1.65349287e-02 -8.19187622e-03\n",
      " -1.23947897e-02  8.73464297e-04 -4.66461652e-03  1.22433328e-02\n",
      " -3.50204094e-02 -5.86940372e-03 -5.89914718e-03 -1.96771526e-02\n",
      "  2.41364652e-02 -2.64291621e-02  4.68343589e-04 -3.99918331e-03\n",
      "  3.70382490e-02 -2.99655624e-02 -1.74854760e-02  6.02657718e-03\n",
      " -3.08219986e-02  2.87072207e-03  1.20979988e-02  4.21995374e-03\n",
      "  3.53211377e-02 -4.26950469e-02  2.78755774e-02 -1.18168359e-02\n",
      "  2.58642091e-02 -1.60926175e-02 -3.31538347e-02  2.54521934e-02\n",
      "  4.60517790e-03 -5.23439589e-03  5.47360232e-05  2.01775408e-02\n",
      "  1.57380901e-02  2.40705026e-02 -2.46780531e-02  8.26944915e-04\n",
      " -1.07437378e-03  7.28480619e-03  3.52871189e-02  3.52932831e-03\n",
      "  8.65564462e-03  4.24634346e-03  3.08515979e-02 -1.47445003e-02\n",
      "  1.99008679e-02  2.17673845e-02  1.02545950e-02 -6.55454228e-03\n",
      " -2.83018063e-03  2.79213546e-03  1.76638927e-02  2.14412820e-02\n",
      "  5.58757549e-03 -5.31319643e-03  6.00919790e-03  2.16177071e-02\n",
      "  4.78636754e-02  1.52231640e-02 -1.54470012e-02 -8.51238733e-03\n",
      " -8.33231919e-03  2.98372573e-02  3.24347086e-02 -1.69946420e-02\n",
      "  8.02051521e-03 -1.23858701e-02  5.41291531e-03 -4.49670241e-02\n",
      " -2.18077328e-02 -1.15402330e-02  3.93749721e-03  1.16400868e-02\n",
      " -5.84978125e-03 -3.04540408e-03  2.47849671e-02  7.64731371e-03\n",
      "  5.67108993e-02  7.55964767e-03  2.53852840e-02 -1.76943086e-02\n",
      " -7.14360682e-04  8.00193928e-03 -2.18319431e-02 -3.66605711e-03\n",
      "  7.23949612e-03  5.29982027e-03 -2.59829382e-02 -3.65244984e-02\n",
      " -6.21888831e-03 -1.28443105e-02  6.41528655e-02 -4.78705836e-03\n",
      " -3.88273483e-02  5.68162619e-02  2.98044037e-02  2.25647905e-02\n",
      " -1.07319019e-02 -4.16884239e-02  9.51451396e-03 -3.91329993e-02\n",
      " -3.30029919e-02 -1.30560102e-03 -2.62917793e-02  1.67306142e-02\n",
      " -2.23384442e-02  3.01373060e-03  2.18965114e-02  4.10500698e-02\n",
      "  3.32734501e-03 -3.51451188e-02 -3.59939410e-02 -3.50385268e-02\n",
      " -8.15812483e-04  2.17100588e-02 -3.66263604e-02 -2.47139863e-02\n",
      "  2.03194375e-02 -1.83371626e-02 -1.08916663e-02 -1.92295376e-02\n",
      "  2.80244958e-02 -1.57005733e-02  7.05628794e-03  1.81538275e-03\n",
      "  1.20906810e-02 -2.40241655e-02  4.69807252e-02 -8.31826902e-03\n",
      "  1.01721231e-02 -1.56341610e-02 -2.15738155e-02 -4.98836594e-03\n",
      " -3.54669178e-03  5.75134106e-03  8.26815278e-05 -9.71429777e-03\n",
      "  1.71491013e-02  1.45301777e-02 -9.39679926e-03  3.30271553e-02\n",
      "  3.76753420e-02 -1.66309248e-02  3.18052608e-03  2.07738596e-02\n",
      "  3.18258030e-02  9.78116760e-03  1.77663818e-02  4.62276254e-04\n",
      "  4.86935188e-03 -1.43919162e-02 -7.10076256e-03 -3.35242011e-03\n",
      " -8.00024110e-03  1.57862326e-02 -1.09785588e-02  3.45853351e-02\n",
      " -3.79390504e-02  5.50909336e-03 -1.17139198e-02 -8.87076128e-03\n",
      " -1.94415651e-03 -1.34143814e-02  2.99304733e-02 -8.48338762e-03\n",
      " -5.26066686e-03  1.64814989e-02 -2.49195359e-02 -2.84099212e-02]\n"
     ]
    }
   ],
   "source": [
    "# CORRECT SOURCE VERSUS MISSED TRIAL CLASSIFICATION\n",
    "\n",
    "for numnet in numnets:\n",
    "    \n",
    "    basc = image.load_img(os.path.join(basc_dir, 'MIST_'+str(numnet)+'.nii'))\n",
    "    b_labels = '/Users/mombot/Documents/Simexp/CIMAQ/Data/MIST/Release/Parcel_Information/MIST_'+str(numnet)+'.csv'\n",
    "    basc_labels = pd.read_csv(b_labels, sep=';')\n",
    "\n",
    "    # build data structure to store accuracy data and coefficients\n",
    "    cs_miss_data = pd.DataFrame()\n",
    "    cs_miss_data.insert(loc = 0, column = 'dccid', value = 'None', allow_duplicates=True)\n",
    "    # cs_miss_data.insert(loc = 1, column = 'diagnosis', value = 'None', allow_duplicates=True)\n",
    "    for i in range(0, 7):\n",
    "        cs_miss_data.insert(loc = cs_miss_data.shape[1], column = 'CV'+str(i+1)+'_acc', value = NaN, allow_duplicates=True)\n",
    "    cs_miss_data.insert(loc = cs_miss_data.shape[1], column = 'TrainSet_MeanCV_acc', value = 'None', allow_duplicates=True)\n",
    "    cs_miss_data.insert(loc = cs_miss_data.shape[1], column = 'TestSet_acc', value = 'None', allow_duplicates=True)\n",
    "    netnames = basc_labels['name']\n",
    "    for i in range(0, numnet):\n",
    "        cs_miss_data.insert(loc = cs_miss_data.shape[1], column = netnames[i]+'_coef', value = NaN, allow_duplicates=True)\n",
    "\n",
    "    for sub in cmiss_subs:\n",
    "        print(sub)\n",
    "        s_data = [sub]\n",
    "        # load subject's beta maps (one per trial)\n",
    "        betas = image.load_img(img=os.path.join(beta_dir, str(sub), 'TrialContrasts/betas_sub'+str(sub)+'*.nii'),\n",
    "                               wildcards=True)\n",
    "        # initialize NiftiLabelMasker object    \n",
    "        sub_mask = nb.load(os.path.join(mask_dir, 'func_sub'+str(sub)+'_mask_stereonl.nii'))\n",
    "        sub_label_masker = NiftiLabelsMasker(labels_img=basc, standardize=True, mask_img=sub_mask,\n",
    "                                             memory = 'nilearn_cache', verbose=0)\n",
    "\n",
    "        # transform subject's beta maps into vector of network means per trial\n",
    "        X_cs_ws_miss_ctl = sub_label_masker.fit_transform(betas)   \n",
    "\n",
    "        # load subject's trial labels\n",
    "        labels_file = os.path.join(label_dir, 'sub-'+str(sub)+'_ctl_miss_ws_cs.tsv')\n",
    "        y_cs_ws_miss_ctl = pd.read_csv(labels_file, sep='\\t')\n",
    "        y_cs_ws_miss_ctl_labels = y_cs_ws_miss_ctl['ctl_miss_ws_cs']\n",
    "        # mask X and y data to exclude trials of no interest\n",
    "        cs_miss_mask = y_cs_ws_miss_ctl_labels.isin(['correctsource', 'missed'])\n",
    "        y_cs_miss = y_cs_ws_miss_ctl_labels[cs_miss_mask]      \n",
    "        X_cs_miss  = X_cs_ws_miss_ctl[cs_miss_mask]\n",
    "\n",
    "        # Split trials into a training and a test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_cs_miss, # x\n",
    "            y_cs_miss, # y\n",
    "            test_size = 0.4, # 60%/40% split\n",
    "            shuffle = True, # shuffle dataset before splitting\n",
    "            stratify = y_cs_miss, # keep distribution of conditions consistent betw. train & test sets\n",
    "            #random_state = 123  # if set number, same shuffle each time, otherwise randomization algo\n",
    "            ) \n",
    "        print('training:', len(X_train), 'testing:', len(X_test))\n",
    "        print(y_train.value_counts(), y_test.value_counts())\n",
    "\n",
    "        # define the model\n",
    "        sub_svc = SVC(kernel='linear', class_weight='balanced')\n",
    "\n",
    "        # do cross-validation to evaluate model performance\n",
    "        # within 10 folds of training set\n",
    "        # predict\n",
    "        y_pred = cross_val_predict(sub_svc, X_train, y_train,\n",
    "                                   groups=y_train, cv=7)\n",
    "        # scores\n",
    "        cv_acc = cross_val_score(sub_svc, X_train, y_train,\n",
    "                             groups=y_train, cv=7)\n",
    "        print(cv_acc)\n",
    "\n",
    "        for i in range(0, len(cv_acc)):\n",
    "            s_data.append(cv_acc[i])\n",
    "\n",
    "        # evaluate overall model performance on training data\n",
    "        overall_acc = accuracy_score(y_pred = y_pred, y_true = y_train)\n",
    "        overall_cr = classification_report(y_pred = y_pred, y_true = y_train)\n",
    "        print('Accuracy:',overall_acc)\n",
    "        print(overall_cr)\n",
    "\n",
    "        s_data.append(overall_acc)\n",
    "\n",
    "        # Test model on unseen data from the test set\n",
    "        sub_svc.fit(X_train, y_train)\n",
    "        y_pred = sub_svc.predict(X_test) # classify age class using testing data\n",
    "        acc = sub_svc.score(X_test, y_test) # get accuracy\n",
    "\n",
    "        cr = classification_report(y_pred=y_pred, y_true=y_test) # get prec., recall & f1\n",
    "        # print results\n",
    "        print('accuracy =', acc)\n",
    "        print(cr)  \n",
    "\n",
    "        s_data.append(acc)\n",
    "\n",
    "        # get coefficients\n",
    "        coef_ = sub_svc.coef_[0]\n",
    "        print(coef_.shape)\n",
    "        print(coef_)\n",
    "\n",
    "        sub_basc = basc_labels.copy()\n",
    "        sub_basc.insert(loc=3, column='coef', value=coef_, allow_duplicates=True)\n",
    "\n",
    "        coef = sub_basc['coef']\n",
    "        for j in range(0, len(coef)):\n",
    "            s_data.append(coef[j])\n",
    "\n",
    "        #sub_basc.sort_values(by='coef', axis = 0, ascending = False, inplace=True)\n",
    "        #print(sub_basc.iloc[0:12, 2:4])\n",
    "\n",
    "        cs_miss_data = cs_miss_data.append(pd.Series(s_data, index=cs_miss_data.columns), ignore_index=True)\n",
    "\n",
    "    demo_data = cmiss_data.copy()\n",
    "    demo_data.reset_index(level=None, drop=False, inplace=True)\n",
    "\n",
    "    cs_miss_data.insert(loc = 1, column = 'cognitive_status', value = demo_data['cognitive_status'], allow_duplicates=True)\n",
    "    cs_miss_data.insert(loc = 2, column = 'total_scrubbed_frames', value = demo_data['total_scrubbed_frames'], allow_duplicates=True)\n",
    "    cs_miss_data.insert(loc = 3, column = 'mean_FD', value = demo_data['mean_FD'], allow_duplicates=True)\n",
    "    cs_miss_data.insert(loc = 4, column = 'hits', value = demo_data['hits'], allow_duplicates=True)\n",
    "    cs_miss_data.insert(loc = 5, column = 'miss', value = demo_data['miss'], allow_duplicates=True)\n",
    "    cs_miss_data.insert(loc = 6, column = 'correct_source', value = demo_data['correct_source'], allow_duplicates=True)\n",
    "    cs_miss_data.insert(loc = 7, column = 'wrong_source', value = demo_data['wrong_source'], allow_duplicates=True)\n",
    "    cs_miss_data.insert(loc = 8, column = 'dprime', value = demo_data['dprime'], allow_duplicates=True)\n",
    "    cs_miss_data.insert(loc = 9, column = 'associative_memScore', value = demo_data['associative_memScore'], allow_duplicates=True)    \n",
    "\n",
    "    cs_miss_data.to_csv(os.path.join(output_dir, 'SVC_withinSub_cs_miss_'+str(numnet)+'networks.tsv'),\n",
    "        sep='\\t', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
